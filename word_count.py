# -*- coding: utf-8 -*-
"""word_count.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13qMqAT5kf5olip0QArlpzg7pV3RzrvC6
"""

!pip install pyspark

import sys

from pyspark import SparkContext, SparkConf

if __name__ == "__main__":

  # create Spark context with Spark configuration
  conf = SparkConf().setAppName("Spark Count")
  sc = SparkContext(conf=conf)

  # get threshold
  threshold = sys.argv[2]

  # read in text file and split each document into words
  tokenized = sc.textFile(sys.argv[1]).flatMap(lambda line: line.split(" "))

  # count the occurrence of each word
  wordCounts = tokenized.map(lambda word: (word, 1)).reduceByKey(lambda v1,v2:v1 +v2)

  # filter out words with fewer than threshold occurrences
  filtered = wordCounts.filter(lambda pair:pair[1] >= threshold)

  # count characters
  charCounts = filtered.flatMap(lambda pair:pair[0]).map(lambda c: c).map(lambda c: (c, 1)).reduceByKey(lambda v1,v2:v1 +v2)

  list = charCounts.collect()
  print(repr(list)[1:-1])

!wget --no-check-certificate .../inputfile.txt
!hdfs dfs -put inputfile.txt

!spark-submit --master yarn --deploy-mode client --executor-memory 1g \
  --name wordcount --conf "spark.app.id=wordcount" wordcount.py \
  hdfs://namenode_host:8020/path/to/inputfile.txt 2