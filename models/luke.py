# -*- coding: utf-8 -*-
"""LUKE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sWlEhZlFAEElrU8yInE_ye6fQOEjsFAr
"""

!pip install transformers

from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification

model = LukeModel.from_pretrained("studio-ousia/luke-base")
tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-base")
# Example 1: Computing the contextualized entity representation corresponding to the entity mention

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"
inputs = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors="pt")
outputs = model(**inputs)
word_last_hidden_state = outputs.last_hidden_state
entity_last_hidden_state = outputs.entity_last_hidden_state
# Example 2: Inputting Wikipedia entities to obtain enriched contextualized representations

entities = [
    "we",
    "LM",
    "LMS",
    "Calgary"
]  # Wikipedia entity titles corresponding to the entity mentions "we", "LM", "LMS" and "Calgary"
entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"
inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors="pt")
outputs = model(**inputs)
word_last_hidden_state = outputs.last_hidden_state
entity_last_hidden_state = outputs.entity_last_hidden_state
# Example 3: Classifying the relationship between two entities using LukeForEntityPairClassification head model

model = LukeForEntityPairClassification.from_pretrained("studio-ousia/luke-large-finetuned-tacred")
tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-large-finetuned-tacred")
entity_spans = [(25, 26), (47,48)]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"
inputs = tokenizer(text, entity_spans=entity_spans, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = int(logits[0].argmax())
print("Predicted class:", model.config.id2label[predicted_class_idx])

from transformers import LukeTokenizer, LukeModel

tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-base")
model = LukeModel.from_pretrained("studio-ousia/luke-base")
# Compute the contextualized entity representation corresponding to the entity mention "we", "LM", "LMS" and "Calgary"

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"

encoding = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors="pt")
outputs = model(**encoding)
word_last_hidden_state = outputs.last_hidden_state
entity_last_hidden_state = outputs.entity_last_hidden_state
# Input Wikipedia entities to obtain enriched contextualized representations of word tokens

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
entities = [
    "we",
    "LM",
    "LMS",
    "Calgary"
]  # Wikipedia entity titles corresponding to the entity mentions "we", "LM", "LMS" and "Calgary"
entity_spans = [
    (25, 26), 
    (47,48), 
    (96,98), 
    (119,126)
]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"

encoding = tokenizer(
    text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors="pt"
)
outputs = model(**encoding)
word_last_hidden_state = outputs.last_hidden_state
entity_last_hidden_state = outputs.entity_last_hidden_state

from transformers import LukeTokenizer, LukeForEntityClassification

tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-large-finetuned-open-entity")
model = LukeForEntityClassification.from_pretrained("studio-ousia/luke-large-finetuned-open-entity")

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
entity_spans = [(119,126)]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"
inputs = tokenizer(text, entity_spans=entity_spans, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

from transformers import LukeTokenizer, LukeForEntityPairClassification

tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-large-finetuned-tacred")
model = LukeForEntityPairClassification.from_pretrained("studio-ousia/luke-large-finetuned-tacred")

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
entity_spans = [
    (96,98), 
    (119,126)
]  # character-based entity spans corresponding to "we", "LM", "LMS" and "Calgary"
inputs = tokenizer(text, entity_spans=entity_spans, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

from transformers import LukeTokenizer, LukeForEntitySpanClassification

tokenizer = LukeTokenizer.from_pretrained("studio-ousia/luke-large-finetuned-conll-2003")
model = LukeForEntitySpanClassification.from_pretrained("studio-ousia/luke-large-finetuned-conll-2003")

text = "Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one."
import spacy
from spacy.pipeline import merge_entities    
nlp = spacy.load("en_core_web_sm")  # or any other model
nlp.add_pipe(merge_entities)
print([token.text for token in nlp("Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.")])

word_start_positions = [0, 8, 14, 17, 21]  # character-based start positions of word tokens
word_end_positions = [7, 13, 16, 20, 28]  # character-based end positions of word tokens
entity_spans = []
for i, start_pos in enumerate(word_start_positions):
    for end_pos in word_end_positions[i:]:
        entity_spans.append((start_pos, end_pos))

inputs = tokenizer(text, entity_spans=entity_spans, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_indices = logits.argmax(-1).squeeze().tolist()
for span, predicted_class_idx in zip(entity_spans, predicted_class_indices):
    if predicted_class_idx != 0:
        print(text[span[0] : span[1]], model.config.id2label[predicted_class_idx])

