{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_0K1lsW1dj9"
      },
      "outputs": [],
      "source": [
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95FHWXOVpUFp",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! pip install ipywidgets\n",
        "! jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzqD2WDFOIN-"
      },
      "outputs": [],
      "source": [
        "from nemo.collections import nlp as nemo_nlp\n",
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "import os\n",
        "import wget \n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8HZrDmr12_-"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"DATA_DIR\"\n",
        "WORK_DIR = \"WORK_DIR\"\n",
        "os.environ['DATA_DIR'] = DATA_DIR\n",
        "\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "! wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "! unzip -o SST-2.zip -d {DATA_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB0oLE4R9EhJ"
      },
      "outputs": [],
      "source": [
        "! sed 1d {DATA_DIR}/SST-2/train.tsv > {DATA_DIR}/SST-2/train_nemo_format.tsv\n",
        "! sed 1d {DATA_DIR}/SST-2/dev.tsv > {DATA_DIR}/SST-2/dev_nemo_format.tsv\n",
        "! ls -l {DATA_DIR}/SST-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UDPgadLN6SG"
      },
      "outputs": [],
      "source": [
        "# let's take a look at the data \n",
        "print('Contents (first 5 lines) of train.tsv:')\n",
        "! head -n 5 {DATA_DIR}/SST-2/train_nemo_format.tsv\n",
        "\n",
        "print('\\nContents (first 5 lines) of test.tsv:')\n",
        "! head -n 5 {DATA_DIR}/SST-2/test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1gA8PsJ13MJ"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = \"text_classification_config.yaml\"\n",
        "CONFIG_DIR = WORK_DIR + '/configs/'\n",
        "\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "if not os.path.exists(CONFIG_DIR + MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, CONFIG_DIR)\n",
        "    print('Config file downloaded!')\n",
        "else:\n",
        "    print ('config file already exists')\n",
        "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
        "print(config_path)\n",
        "config = OmegaConf.load(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFSMiWtlkaC5"
      },
      "outputs": [],
      "source": [
        "config.model.dataset.num_classes=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQHCJN-ZaoLp"
      },
      "outputs": [],
      "source": [
        "config.model.train_ds.file_path = os.path.join(DATA_DIR, 'SST-2/train_nemo_format.tsv')\n",
        "config.model.validation_ds.file_path = os.path.join(DATA_DIR, 'SST-2/dev_nemo_format.tsv')\n",
        "\n",
        "config.save_to = 'trained-model.nemo'\n",
        "config.export_to = 'trained-model.onnx'\n",
        "\n",
        "print(\"Train dataloader's config: \\n\")\n",
        "print(OmegaConf.to_yaml(config.model.train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tG4FzZ4Ui60"
      },
      "outputs": [],
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knF6QeQQdMrH"
      },
      "outputs": [],
      "source": [
        "config.trainer.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.devices = 1\n",
        "config.trainer.strategy = None\n",
        "config.trainer.max_epochs = 1\n",
        "trainer = pl.Trainer(**config.trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uztqGAmdrYt"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(**config.trainer)\n",
        "print(OmegaConf.to_yaml(config.exp_manager))\n",
        "exp_dir = exp_manager(trainer, config.exp_manager)\n",
        "print(exp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeuc2i7Y_nP5"
      },
      "outputs": [],
      "source": [
        "print(nemo_nlp.modules.get_pretrained_lm_models_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK2xglXyAUOO"
      },
      "outputs": [],
      "source": [
        "config.model.language_model.pretrained_model_name = \"bert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgsGLydWo-6-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = nemo_nlp.models.TextClassificationModel(cfg=config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTJr16_pp0aS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "  COLAB_ENV = False\n",
        "\n",
        "if COLAB_ENV:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir {exp_dir}\n",
        "else:\n",
        "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUvnSpyjp0Dh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.fit(model)\n",
        "model.save_to(config.save_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92PB0iTqNnW-"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
        "eval_model = nemo_nlp.models.TextClassificationModel.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
        "eval_config = OmegaConf.create({'file_path': config.model.validation_ds.file_path, 'batch_size': 64, 'shuffle': False, 'num_samples': -1})\n",
        "eval_model.setup_test_data(test_data_config=eval_config)\n",
        "\n",
        "eval_trainer_cfg = config.trainer.copy()\n",
        "eval_trainer_cfg.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "eval_trainer_cfg.strategy = None \n",
        "eval_trainer = pl.Trainer(**eval_trainer_cfg)\n",
        "\n",
        "eval_trainer.test(model=eval_model, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKe5Jn4u9xng"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
        "infer_model = nemo_nlp.models.TextClassificationModel.load_from_checkpoint(checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQhsamclRtxJ"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    infer_model.to(\"cuda\")\n",
        "else:\n",
        "    infer_model.to(\"cpu\")\n",
        "\n",
        "queries = [\"We are seeing deaths. So we have had deaths from people who sat in the waiting room and have passed away in the waiting room\",\n",
        "           \"We need to email to our local political representatives and tell them that ER services are failing and we need to fix it\",\n",
        "           \"It could be you, me or anyone else, who will get to be the first one's to suffer\",\n",
        "           \"It could be a baby, it could be an infant, it could be a mother, it could be a father who will loose their life because of ER failure\", \n",
        "           \"It could be a newly wed, or too be wed, it could be a student, a teacher, a farmer, an engineer, doctor or Professor\",\n",
        "           \"No matter who you are, what you do, where do you live, what your race is, what your background is. If ER fails we all would be probably victims\",\n",
        "           \"Speak up let them know the real issues. Let them know we want change. Let them know we want actions\",\n",
        "           \"It seems to me that modern primary, secondary or even tertiary education system is just mass producing conformist enablers who are incapable of looking farther than their immediate sphere of life\",\n",
        "           \"They become like horses who are given side blinds to let them focus on a certain path\",\n",
        "           \"Why is everyone so conforming/accepting to everything as if whatever is happening is the most ideal/efficient/best/just/moral way to do things?\",\n",
        "           \"I am not asking anyone to break the law. I am not advocating anarchy. I am just questioning the attitude of not questioning enough.\",\n",
        "           \"I see consumers not knowing their rights, their powers, their unused influence. We are so socially conditioned to accept whatever is going on is good. I see voters oblivious to what they should ask for from candidates.\",\n",
        "           \"May be this is so gradual that people don't notice. People are made conformist and then burdened with debt, with responsibilities shoved down upon them by societal expectations. This creates a nation of followers. A follower nation cannot take care of itself. It depends on an active force to be there to guide them to what they think is the best. And this can go wrong too.\",\n",
        "           \"We need disruptive mindset. We need to ask questions. Questions such as, do I deserve to be treated this way?\"]\n",
        "           \n",
        "results = infer_model.classifytext(queries=queries, batch_size=3, max_seq_length=512)\n",
        "\n",
        "print('The prediction results of some sample queries with the trained model:')\n",
        "for query, result in zip(queries, results):\n",
        "    print(f'Query : {query}')\n",
        "    print(f'Predicted label: {result}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ref1qSonGNhP"
      },
      "source": [
        "## Training Script\n",
        "\n",
        "If you have NeMo installed locally (eg. cloned from the Github), you can also train the model with `examples/nlp/text_classification/text_classifciation_with_bert.py`. This script contains an example on how to train, evaluate and perform inference with the TextClassificationModel.\n",
        "\n",
        "For example the following would train a model for 50 epochs in 2 GPUs on a classification task with 2 classes:\n",
        "\n",
        "```\n",
        "# python text_classification_with_bert.py \n",
        "        model.dataset.num_classes=2\n",
        "        model.train_ds=PATH_TO_TRAIN_FILE \n",
        "        model.validation_ds=PATH_TO_VAL_FILE  \n",
        "        trainer.max_epochs=50\n",
        "        trainer.devices=2\n",
        "        trainer.accelerator='gpu'\n",
        "```\n",
        "\n",
        "This script would also reload the best checkpoint after the training is done and does evaluation on the dev set. Then perform inference on some sample queries. \n",
        "\n",
        "\n",
        "By default, this script uses `examples/nlp/text_classification/conf/text_classifciation_config.py` config file, and you may update all the params in the config file from the command line. You may also use another config file like this:\n",
        "\n",
        "```\n",
        "# python text_classification_with_bert.py --config-name==PATH_TO_CONFIG_FILE\n",
        "        model.dataset.num_classes=2\n",
        "        model.train_ds=PATH_TO_TRAIN_FILE \n",
        "        model.validation_ds=PATH_TO_VAL_FILE  \n",
        "        trainer.max_epochs=50\n",
        "        trainer.devices=2\n",
        "        trainer.accelerator='gpu'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrtO4jSs50Hz"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "You can also deploy a model to an inference engine (like TensorRT or ONNXRuntime) using ONNX exporter.\n",
        "If you don't have one, let's install it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPTGzXGg50Hz"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade onnxruntime # for gpu, use onnxruntime-gpu\n",
        "# !mkdir -p ort\n",
        "# %cd ort\n",
        "# !git clean -xfd\n",
        "# !git clone --depth 1 --branch v1.8.0 https://github.com/microsoft/onnxruntime.git .\n",
        "# !./build.sh --skip_tests --config Release --build_shared_lib --parallel --use_cuda --cuda_home /usr/local/cuda --cudnn_home /usr/lib/x86_64-linux-gnu --build_wheel\n",
        "# !pip uninstall -y onnxruntime\n",
        "# !pip uninstall -y onnxruntime-gpu\n",
        "# !pip install  --upgrade --force-reinstall ./build/Linux/Release/dist/onnxruntime*.whl\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoGXU7MP50Hz"
      },
      "source": [
        "Then export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbxNK9BK50Hz"
      },
      "outputs": [],
      "source": [
        "model.export(config.export_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dphT2_gX50Hz"
      },
      "source": [
        "And run some queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMs2stRe50Hz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nemo.utils import logging\n",
        "from nemo.collections.nlp.parts.utils_funcs import tensor2list\n",
        "from nemo.collections.nlp.models.text_classification import TextClassificationModel\n",
        "from nemo.collections.nlp.data.text_classification import TextClassificationDataset\n",
        "\n",
        "import onnxruntime\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "def postprocessing(results, labels):\n",
        "    return [labels[str(r)] for r in results]\n",
        "\n",
        "def create_infer_dataloader(model, queries):\n",
        "    batch_size = len(queries)\n",
        "    dataset = TextClassificationDataset(tokenizer=model.tokenizer, queries=queries, max_seq_length=512)\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        collate_fn=dataset.collate_fn,\n",
        "    )\n",
        "\n",
        "queries = [\"We are seeing deaths. So we have had deaths from people who sat in the waiting room and have passed away in the waiting room\",\n",
        "           \"We need to email to our local political representatives and tell them that ER services are failing and we need to fix it\",\n",
        "           \"It could be you, me or anyone else, who will get to be the first one's to suffer\",\n",
        "           \"It could be a baby, it could be an infant, it could be a mother, it could be a father who will loose their life because of ER failure\", \n",
        "           \"It could be a newly wed, or too be wed, it could be a student, a teacher, a farmer, an engineer, doctor or Professor\",\n",
        "           \"No matter who you are, what you do, where do you live, what your race is, what your background is. If ER fails we all would be probably victims\",\n",
        "           \"Speak up let them know the real issues. Let them know we want change. Let them know we want actions\",\n",
        "           \"It seems to me that modern primary, secondary or even tertiary education system is just mass producing conformist enablers who are incapable of looking farther than their immediate sphere of life\",\n",
        "           \"They become like horses who are given side blinds to let them focus on a certain path\",\n",
        "           \"Why is everyone so conforming/accepting to everything as if whatever is happening is the most ideal/efficient/best/just/moral way to do things?\",\n",
        "           \"I am not asking anyone to break the law. I am not advocating anarchy. I am just questioning the attitude of not questioning enough.\",\n",
        "           \"I see consumers not knowing their rights, their powers, their unused influence. We are so socially conditioned to accept whatever is going on is good. I see voters oblivious to what they should ask for from candidates.\",\n",
        "           \"May be this is so gradual that people don't notice. People are made conformist and then burdened with debt, with responsibilities shoved down upon them by societal expectations. This creates a nation of followers. A follower nation cannot take care of itself. It depends on an active force to be there to guide them to what they think is the best. And this can go wrong too.\",\n",
        "           \"We need disruptive mindset. We need to ask questions. Questions such as, do I deserve to be treated this way?\"]\n",
        "\n",
        "model.eval()\n",
        "infer_datalayer = create_infer_dataloader(model, queries)\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(config.export_to)\n",
        "\n",
        "for batch in infer_datalayer:\n",
        "    input_ids, input_type_ids, input_mask, subtokens_mask = batch\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_ids),\n",
        "                  ort_session.get_inputs()[1].name: to_numpy(input_mask),\n",
        "                  ort_session.get_inputs()[2].name: to_numpy(input_type_ids),}\n",
        "    ologits = ort_session.run(None, ort_inputs)\n",
        "    alogits = np.asarray(ologits)\n",
        "    logits = torch.from_numpy(alogits[0])\n",
        "    preds = tensor2list(torch.argmax(logits, dim = -1))\n",
        "    processed_results = postprocessing(preds, {\"0\": \"negative\", \"1\": \"positive\"})\n",
        "\n",
        "    logging.info('The prediction results of some sample queries with the trained model:')\n",
        "    for query, result in zip(queries, processed_results):\n",
        "        logging.info(f'Query : {query}')\n",
        "        logging.info(f'Predicted label: {result}')\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}