# -*- coding: utf-8 -*-
"""Crisp‚ö° AutoGGUF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ERrYYafU15FwbCz6ZYcOhIp_V3P2gXpi
"""

# @title # ‚ö° AutoQuant

# @markdown > üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)

# @markdown ‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne) with a few minor adaptions. (I only kept the GGUF part, because i only use that. See his versions for more functionality.)

# @markdown **Usage:** Download the model by **running this cell** and then run the cells corresponding to your quantization methods of interest.

# @markdown To quantize a 7B model, GGUF only needs a T4 GPU.

# @markdown *See also the [AutoQuantize](https://colab.research.google.com/drive/1Li3USnl3yoYctqJLtYux3LAIy4Bnnv3J) notebook from zainulabideen.*

# @markdown ---

# @markdown ## ü§ó Download model (required)
# @markdown `HF_TOKEN` corresponds to the name of the secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens) in Colab.

MODEL_ID = "cstr/phi3-mini-4k-llamafied-sft-v5" # @param {type:"string"}
USERNAME = "cstr" # @param {type:"string"}
HF_TOKEN = "HF_TOK" # @param {type:"string"}

MODEL_NAME = MODEL_ID.split('/')[-1]

# Download model
!pip install huggingface_hub

from huggingface_hub import create_repo, HfApi, ModelCard, snapshot_download
from google.colab import userdata, runtime

# Defined in the secrets tab in Google Colab
hf_token = userdata.get(HF_TOKEN)

#import subprocess
#subprocess.run(["huggingface-cli", "login", "--token", hf_token])

savepath = f'/content/{MODEL_NAME}'
outpath = snapshot_download(repo_id=MODEL_ID, revision="main", cache_dir=savepath, token=hf_token)
print(f"Model downloaded to: {outpath}")

#!git lfs install
#!git clone --depth 1 https://{USERNAME}:{hf_token}@huggingface.co/{MODEL_ID}

api = HfApi()

# @title ## üß© GGUF

# @markdown Quantization methods: `q2_k`, `q3_k_l`, `q3_k_m`, `q3_k_s`, `q4_0`, `q4_1`, `q4_k_m`, `q4_k_s`, `q5_0`, `q5_1`, `q5_k_m`, `q5_k_s`, `q6_k`, `q8_0`

# @markdown Learn more about GGUF and quantization methods in [this article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html).
# @markdown For Llama3, you must select PAD_VOCAB and set VOCAB_TYPE to bpe

QUANTIZATION_FORMAT = "q4_k_m" # @param {type:"string"}
PAD_VOCAB = False # @param {type: "boolean"}
VOCAB_TYPE = "" # @param ["", "bpe", ""]
QUANTIZATION_METHODS = QUANTIZATION_FORMAT.replace(" ", "").split(",")

PAD_V = ""
if PAD_VOCAB:
  PAD_V = "--pad-vocab"

V_TYPE = ""
if VOCAB_TYPE != "":
  V_TYPE = "--vocab-type " + VOCAB_TYPE

# Install llama.cpp
#!git clone https://github.com/ggerganov/llama.cpp
#!cd llama.cpp && make
# !LLAMA_CUBLAS=1 make
#!pip install -r llama.cpp/requirements.txt

# Convert to fp16
#fp16 = f"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin"
#!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16} {V_TYPE} {PAD_V}

import os
from google.colab import userdata
!apt update && apt install gh -y

# Function to download and unzip binaries
def download_and_prepare_binaries(github_token, repo, release_tag, asset_name, target_path):
    if not os.path.exists(target_path):
        os.makedirs(target_path, exist_ok=True)
    # Login to GitHub CLI
    !echo {github_token} | gh auth login --with-token
    # Download release assets using GitHub CLI
    !gh release download {release_tag} --repo {repo} --pattern {asset_name} --dir {target_path}
    # Unzip the downloaded assets
    !unzip {target_path}/{asset_name} -d {target_path}

# GitHub repository and release information
github_token = userdata.get('github_repo')  # Get the GitHub token from Colab's secrets
repo = 'CrispStrobe/llama_cpp'  # GitHub repo where binaries are stored
release_tag = 'b2749'  # Tag of the release containing the binaries
asset_name = 'llama_cpp_binaries.zip'  # Name of the asset to download
binaries_path = '/content/llama.cpp'  # Local path to store binaries

# Download and prepare binaries if not already present
download_and_prepare_binaries(github_token, repo, release_tag, asset_name, binaries_path)

# Proceed with using the binaries
fp16 = f"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin"
!{binaries_path}/convert.py {outpath}/ --outtype f16 --outfile {fp16} {V_TYPE} {PAD_V}

for method in QUANTIZATION_METHODS:
    qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf"
    !{binaries_path}/quantize {fp16} {qtype} {method}

# Quantize the model for each method in the QUANTIZATION_METHODS list
for method in QUANTIZATION_METHODS:
    qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf"
    !./llama.cpp/quantize {fp16} {qtype} {method}

# Create model card
card = ModelCard.load(MODEL_ID)
card.data.tags.append("autoquant")
card.data.tags.append("gguf")
card.save(f'{MODEL_NAME}/README.md')

# Upload model
create_repo(
    repo_id = f"{USERNAME}/{MODEL_NAME}-GGUF",
    repo_type="model",
    exist_ok=True,
    token=hf_token
)
api.upload_folder(
    folder_path=MODEL_NAME,
    repo_id=f"{USERNAME}/{MODEL_NAME}-GGUF",
    allow_patterns=["*.gguf","$.md"],
    token=hf_token
)

#!cd /content/llama.cpp && zip -r llama_cpp_binaries.zip . # adjust path to include only necessary files

#!apt update && apt install gh
#from google.colab import userdata
#GITHUB_TOKEN = userdata.get('github_repo')  # Get the GitHub token from Colab's secrets
#!echo $GITHUB_TOKEN | gh auth login --with-token
#!gh release create b2749 /content/llama.cpp/llama_cpp_binaries.zip --repo CrispStrobe/llama_cpp --notes "Compiled binaries for llama_cpp"