# -*- coding: utf-8 -*-
"""LlamaParse_GPT_4o_mode_10K_report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9PHrLWlKB7aaVdOXEy3ztRbZenFAPmj

[Hanane D](https://www.linkedin.com/in/hanane-d-algo-trader)

1- Parsing Amazon 10k Financial Report using:

*   **LlamaParse** with **GPT-4o mode** to improve the parsing quality, particularly when financial charts are included in the report

*   **SimpleDirectoryParser**: a standard way of parsing.

2- I used HuggingFace local **embedding** (using LlamaIndex), to store data in a VectoreStore

3- I created a **query engine** using different LLMs and compare their results: **gpt-3.5-turbo**, **GPT-4o**, and **Claude Sonnet 3.5** .

# Install Lib
"""

!pip install llama-index llama-index-core llama-parse openai llama_index.embeddings.huggingface -q
!pip install llama-index-llms-anthropic -q

"""# Specify API Keys"""

import nest_asyncio
nest_asyncio.apply()

from google.colab import userdata
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
LLAMAPARSE_API_KEY = userdata.get('LLAMAPARSE_API_KEY')
ANTHROPIC_API_KEY = userdata.get("CLAUDE_API_KEY")

"""# Loading financial report: Amazon 2023 10K"""

!wget "https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf" -O amzn_2023_10k.pdf

"""# Standard Parsing with SimpleDirectoryReader:

## VectoreStore and specifying LLms for the query_engine
"""

# from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import nest_asyncio;
nest_asyncio.apply()

pdf_name = "amzn_2023_10k.pdf"
# use SimpleDirectoryReader to parse our file
documents = SimpleDirectoryReader(input_files=[pdf_name]).load_data()
from llama_index.core import Settings

embed_model = "local:BAAI/bge-small-en-v1.5" #https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d

vector_index_std = VectorStoreIndex(documents, embed_model = embed_model)

from llama_index.llms.openai import OpenAI

llm_gpt35 = OpenAI(model="gpt-3.5-turbo", api_key = OPENAI_API_KEY)
query_engine_gpt35 = vector_index_std.as_query_engine(similarity_top_k=3, llm=llm_gpt35)

llm_gpt4o = OpenAI(model="gpt-4o", api_key = OPENAI_API_KEY)
query_engine_gpt4o = vector_index_std.as_query_engine(similarity_top_k=3, llm=llm_gpt4o)


from llama_index.llms.anthropic import Anthropic
from llama_index.core import Settings

tokenizer = Anthropic().tokenizer
Settings.tokenizer = tokenizer

llm_claude = Anthropic(model="claude-3-5-sonnet-20240620", api_key=ANTHROPIC_API_KEY)
query_engine_claude = vector_index_std.as_query_engine(similarity_top_k=3, llm=llm_claude)

print(documents[36].text)

"""## Chatting with the LLMs: GPT-3.5-Turbo, GPT-4o, Claude 3.5 Sonnet"""

query1 = "What was the net income in 2023?"
response = query_engine_gpt35.query(query1)
print("GPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query1)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query1)
print("\nClaude 3.5 Sonnet")
print(str(resp))

# print(resp.source_nodes[0].get_content()) # to get the source_node used to answer

query1 = "What was the net income in 2022?"
response = query_engine_gpt35.query(query1)
print("GPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query1)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query1)
print("\nClaude 3.5 Sonnet")
print(str(resp))

query2 = "What was the net income in 2023 compared to 2022?"
print("query2:",query2)
response = query_engine_gpt35.query(query2)
print("\nGPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query2)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query2)
print("\nClaude 3.5 Sonnet")
print(str(resp))

print(resp.source_nodes[0].get_content())

"""# LlamaParse: Amazon 10K Financial report"""

from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex

pdf_name = "amzn_2023_10k.pdf"
# set up parser
# parser = LlamaParse(api_key=LLAMAPARSE_API_KEY, result_type="markdown", gpt4o_mode = True)
parser = LlamaParse(api_key=LLAMAPARSE_API_KEY, result_type="markdown", gpt4o_mode = True)
documents = parser.load_data(pdf_name)

parser

print(documents[43].text)

"""## Store documents: with embeddings for later retrieval"""

from llama_index.core import VectorStoreIndex

embed_model = "local:BAAI/bge-small-en-v1.5" #https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d
vector_index = VectorStoreIndex(documents, embed_model = embed_model)

# vector_index.storage_context.persist(persist_dir=path)

"""## Specify LLMs for chat"""

from llama_index.llms.openai import OpenAI

llm_gpt35 = OpenAI(model="gpt-3.5-turbo", api_key = OPENAI_API_KEY)
query_engine_gpt35 = vector_index.as_query_engine(similarity_top_k=3, llm=llm_gpt35)

llm_gpt4o = OpenAI(model="gpt-4o", api_key = OPENAI_API_KEY)
query_engine_gpt4o = vector_index.as_query_engine(similarity_top_k=3, llm=llm_gpt4o)

from llama_index.llms.anthropic import Anthropic
from llama_index.core import Settings

tokenizer = Anthropic().tokenizer
Settings.tokenizer = tokenizer

llm_claude = Anthropic(model="claude-3-5-sonnet-20240620", api_key=ANTHROPIC_API_KEY)
query_engine_claude = vector_index.as_query_engine(similarity_top_k=3, llm=llm_claude)

"""## Chatting with the LLMs: GPT-3.5-Turbo, GPT-4o, Claude 3.5 Sonnet"""

query1 = "What is the net income on 2023?"
response = query_engine_gpt35.query(query1)
print("GPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query1)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query1)
print("\nClaude 3.5 Sonnet")
print(str(resp))

query2 = "What was the net income in 2023 compared to 2022?"
print("query2:",query2)
response = query_engine_gpt35.query(query2)
print("\nGPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query2)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query2)
print("\nClaude 3.5 Sonnet")
print(str(resp))

query2 = "What was the reason of the net income loss in 2022?"
print("query2:",query2)
response = query_engine_gpt35.query(query2)
print("\nGPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query2)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query2)
print("\nClaude 3.5 Sonnet")
print(str(resp))

print(resp.source_nodes[0].get_content())

query3 = "What are the most important takeaways from the report?"
print("query3:",query3)
response = query_engine_gpt35.query(query3)
print("\nGPT-3.5 Turbo")
print(str(response))

resp = query_engine_gpt4o.query(query3)
print("\nGPT-4o")
print(str(resp))

resp = query_engine_claude.query(query3)
print("\nClaude 3.5 Sonnet")
print(str(resp))

# For this question, the cost went from $0.07 to $0.16 ==> $0.09

print(resp.source_nodes[0].get_content())

print(resp.source_nodes[1].get_content())

print(resp.source_nodes[2].get_content())

"""# Key Takeaways

1- **In terms of parsing**, since the 10k report includes only tables, using LlamaParse with markdown is sufficient (and also visually clear). There's no need for GPT-4o mode. However, as you don't know if there are charts in the PDFs, using GPT-4o mode could be beneficial. However, I found that Claude 3.5 Sonnet (in a previous post) was better than GPT-4o on parsing charts as images.

It could be great to have Claude 3.5 Sonnet mode on LlamaParse. It can also be more expensive.

The simple parsing (SimpleDirectoryReader), was good too for these kind of simple questions.

2- **In the chat comparison**, Claude 3.5 Sonnet was slightly better than GPT-4o. It provides accurate answers and offers valuable details on how some calculations are performed, as well as justifications for certain losses. However, GPT-3.5-Turbo lags significantly behind.
"""