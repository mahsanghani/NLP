{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_SynapseML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jJJ3CuqmbMu",
        "outputId": "84e6d57c-50ed-4ec5-910d-5310a0621dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 44.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=5ddee04831f0412a35b155f32d6f6a35195f9daf2b78f1190aed1166b6d78031\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCGnbERBl924"
      },
      "outputs": [],
      "source": [
        "sparknlpjsl_jar = \"spark-nlp-jsl.jar\"\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"16G\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.5,com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.0\")\\\n",
        "    .config(\"spark.jars\", sparknlpjsl_jar)\\\n",
        "    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxIDyWZ-nLPm",
        "outputId": "ae768e55-c824-4b88-cb29-82ec2948ecef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 71 kB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement spark-nlp== (from versions: 1.6.2, 1.6.3, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5, 2.3.6, 2.4.0rc1, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4rc1, 2.4.4, 2.4.5, 2.5.0rc1, 2.5.0rc2, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.5.4, 2.5.5, 2.6.0rc1, 2.6.0rc2, 2.6.0rc3, 2.6.0, 2.6.1, 2.6.2, 2.6.3rc1, 2.6.3rc2, 2.6.3rc3, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc1, 2.7.0, 2.7.0.post1, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.7.5, 3.0.0rc1, 3.0.0rc2, 3.0.0rc3, 3.0.0rc7, 3.0.0rc8, 3.0.0rc10, 3.0.0rc11, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0rc1, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.4.4, 4.0.0, 4.0.1, 4.0.2)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for spark-nlp==\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement spark-nlp-jsl== (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for spark-nlp-jsl==\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "import synapse.ml\n",
        "from synapse.ml.io import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r3AnFrwEmWCJ",
        "outputId": "e91e070f-259a-4a62-f298-189d9935ffa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9653bfdf50ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msparknlp_jsl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msynapse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msynapse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparknlp_jsl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetectorDL = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\", \"en\", 'clinical/models') \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "      .setInputCols([\"sentence\"])\\\n",
        "      .setOutputCol(\"token\")\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "  .setInputCols([\"sentence\", \"token\"])\\\n",
        "  .setOutputCol(\"word_embeddings\")\n",
        "\n",
        "clinical_ner = MedicalNerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
        "      .setInputCols([\"sentence\", \"token\", \"word_embeddings\"]) \\\n",
        "      .setOutputCol(\"ner\")\n",
        "\n",
        "ner_converter_icd = NerConverterInternal() \\\n",
        "      .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "      .setOutputCol(\"ner_chunk\")\\\n",
        "      .setWhiteList(['PROBLEM'])\\\n",
        "      .setPreservePosition(False)\n",
        "\n",
        "c2doc = Chunk2Doc()\\\n",
        "      .setInputCols(\"ner_chunk\")\\\n",
        "      .setOutputCol(\"ner_chunk_doc\") \n",
        "\n",
        "sbert_embedder = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
        "      .setInputCols([\"ner_chunk_doc\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\\\n",
        "      .setCaseSensitive(False)\n",
        "    \n",
        "icd_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_icd10cm_augmented_billable_hcc\",\"en\", \"clinical/models\") \\\n",
        "     .setInputCols([\"ner_chunk\", \"sentence_embeddings\"]) \\\n",
        "     .setOutputCol(\"icd10cm_code\")\\\n",
        "     .setDistanceFunction(\"EUCLIDEAN\")\n",
        "    \n",
        "resolver_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document_assembler,\n",
        "        sentenceDetectorDL,\n",
        "        tokenizer,\n",
        "        word_embeddings,\n",
        "        clinical_ner,\n",
        "        ner_converter_icd,\n",
        "        c2doc,\n",
        "        sbert_embedder,\n",
        "        icd_resolver\n",
        "  ])"
      ],
      "metadata": {
        "id": "_VHNksnVmfX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinical_note = \"\"\"A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting. Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. She was on metformin, glipizide, and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation. Physical examination on presentation was significant for dry oral mucosa; significantly, her abdominal examination was benign with no tenderness, guarding, or rigidity.\"\"\""
      ],
      "metadata": {
        "id": "leK-3MHrmj-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_json = {\"text\": clinical_note }"
      ],
      "metadata": {
        "id": "iy8c83YImlgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1: Creating the streaming server and transforming json to Spark Dataframe\n",
        "serving_input = spark.readStream.server() \\\n",
        "    .address(\"localhost\", 9999, \"benchmark_api\") \\\n",
        "    .option(\"name\", \"benchmark_api\") \\\n",
        "    .load() \\\n",
        "    .parseRequest(\"benchmark_api\", data.schema)\n",
        "#2: Applying transform to the dataframe using our Spark NLP pipeline\n",
        "serving_output = resolver_p_model.transform(serving_input) \\\n",
        "    .makeReply(\"icd10cm_code\")\n",
        "#3: Returning the response in json format\n",
        "server = serving_output.writeStream \\\n",
        "      .server() \\\n",
        "      .replyTo(\"benchmark_api\") \\\n",
        "      .queryName(\"benchmark_query\") \\\n",
        "      .option(\"checkpointLocation\", \"file:///tmp/checkpoints-{}\".format(uuid.uuid1())) \\\n",
        "      .start()"
      ],
      "metadata": {
        "id": "bhOb-SvjmnOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "res = requests.post(\"http://localhost:9999/benchmark_api\", data= json.dumps(data_json))"
      ],
      "metadata": {
        "id": "SdnGgMUYmqve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0, len(response_list.json())):\n",
        "  print(response_list.json()[i]['result'])"
      ],
      "metadata": {
        "id": "A0YgO1POmsWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6Y_b1XDymurl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}