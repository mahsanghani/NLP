# -*- coding: utf-8 -*-
"""Long Summarization

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQ-ZAuqG_GxUZX3IMBz5W-Ksrazwearu
"""

!apt install git-lfs
!pip install torch
!pip install Adam
!pip install wandb
!pip install flax
!pip install datsets transformers[sentencepiece]
!pip install transformers
!pip install sentencepiece
!pip install --upgrade pip
!pip install --upgrade jax jaxlib
!pip install datasets transformers rouge-score nltk
!pip install --upgrade git+https://github.com/google/flax.git

from huggingface_hub import notebook_login

# notebook_login()

import transformers

print(transformers.__version__)

from datasets import load_dataset, load_metric

import nltk
nltk.download('punkt')
# raw_datasets = load_dataset("xsum")
raw_datasets = load_dataset("ashraq/youtube-transcription", split="train")
raw_datasets = raw_datasets.train_test_split(test_size=0.2, shuffle=True)
metric = load_metric("rouge")

raw_datasets

raw_datasets["train"][0]

import datasets
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=5):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))

show_random_elements(raw_datasets["train"])

metric

# files = [
#          'data/by_speaker_3_10.csv',
#          'data/by_speaker_3_11.csv',
#          'data/by_speaker_3_16.csv'
#          ]
# data = {}
# for i,j in enumerate(files):
#     with open(j) as f:
#         lines = f.readlines()
#     data[files[i]] = lines

# word_count = 0
# truncate_dict = {}
# num_trun = 0
# tmp_list = []
# file_name = data['data/by_speaker_3_11.csv']
# for i in range(len(file_name)):
#     sent_len = len(file_name[i].split(':')[1].lstrip(' ').split(' '))
#     word_count += sent_len
#     num_trun_new = word_count // 300
#     if num_trun_new != num_trun:
#         tmp_list = []
#         tmp_list.append(file_name[i])
        
#     else:
#         tmp_list.append(file_name[i])
#     num_trun = num_trun_new
    
#     if (word_count // 300 != (word_count - sent_len))//300 & (word_count % 300 !=0):
#         tmp_list.pop()
#     else:
#         pass

#     truncate_dict[num_trun] = tmp_list

# sent_list = []
# truncate_num_list = []
# result = {'sentence':sent_list,'truncate_num':truncate_num_list}
# for key, values in truncate_dict.items():
#     for sent in values:
#         sent_list.append(sent)
#         truncate_num_list.append(key)

df1 = pd.read_csv('data/by_speaker_3_10.csv')
df1.head()

# word_count = 0
# truncate_dict = {}
# num_trun = 0
# tmp_list = []
# file_name = data['data/new_daily_3_10.txt']
# for i in range(len(file_name)):
#     sent_len = len(file_name[i].split(':')[1].lstrip(' ').split(' '))
#     word_count += sent_len
#     num_trun_new = word_count // 300
#     if num_trun_new != num_trun:
#         tmp_list = []
#         tmp_list.append(file_name[i])
        
#     else:
#         tmp_list.append(file_name[i])
#     num_trun = num_trun_new
    
#     if (word_count // 300 != (word_count - sent_len))//300 & (word_count % 300 !=0):
#         tmp_list.pop()
#     else:
#         pass

#     truncate_dict[num_trun] = tmp_list

# sent_list = []
# truncate_num_list = []
# result = {'sentence':sent_list,'truncate_num':truncate_num_list}
# for key, values in truncate_dict.items():
#     for sent in values:
#         sent_list.append(sent)
#         truncate_num_list.append(key)

df1 = pd.read_csv('data/by_speaker_3_11.csv')
df1.head()

# word_count = 0
# truncate_dict = {}
# num_trun = 0
# tmp_list = []
# file_name = data['data/new_daily_3_16.txt']
# for i in range(len(file_name)):
#     sent_len = len(file_name[i].split(':')[1].lstrip(' ').split(' '))
#     word_count += sent_len
#     num_trun_new = word_count // 300
#     if num_trun_new != num_trun:
#         tmp_list = []
#         tmp_list.append(file_name[i])
        
#     else:
#         tmp_list.append(file_name[i])
#     num_trun = num_trun_new
    
#     if (word_count // 300 != (word_count - sent_len))//300 & (word_count % 300 !=0):
#         tmp_list.pop()
#     else:
#         pass

#     truncate_dict[num_trun] = tmp_list

# sent_list = []
# truncate_num_list = []
# result = {'sentence':sent_list,'truncate_num':truncate_num_list}
# for key, values in truncate_dict.items():
#     for sent in values:
#         sent_list.append(sent)
#         truncate_num_list.append(key)

df1 = pd.read_csv('data/by_speaker_3_16.csv')
df1.head()

df = pd.concat([df1, df2, df3], axis=0)

df1

ann = pd.read_excel('data/new_daily_3_11.xlsx')

gold = []
gold += [i for i in ann.long_summary.to_list() if type(i)==str]
gold

# model_out = ["he began by starting a five person war cabinet and included chamberlain as lord president of the council",
#              "the siege lasted from 250 to 241 bc, the romans laid siege to lilybaeum",
#              "the original ocean water was found in aquaculture"]

# references = ["he began his premiership by forming a five-man war cabinet which included chamberlain as lord president of the council",
#              "the siege of lilybaeum lasted from 250 to 241 bc, as the roman army laid siege to the carthaginian-held sicilian city of lilybaeum",
#              "the original mission was for research into the uses of deep ocean water in ocean thermal energy conversion (otec) renewable energy production and in aquaculture"]

# metric.compute(predictions=model_out, references=references)

text =""" 
President Donald Trump's physician, Navy Cmdr Dr. Sean Conley, held a second medical briefing that again raised more questions 
than answers about the President's condition.In another jarring news conference on Sunday, Trump's doctors said that even 
though the President has had at least two concerning drops in oxygen levels, they are hoping he could be discharged as early as
tomorrow from Walter Reed National Military Medical Center.Conley and other doctors involved in the President's care offered 
some more information about the President's condition -- but there were still significant gaps that made it hard to decipher
the full picture.Conley failed to answer basic questions about the President's condition and admitted that he had omitted 
those alarming drops in the President's oxygen levels during a news conference Saturday because he wanted to "reflect the upbeat 
attitude" that the team and the President had about his condition and didn't want "to give any information that might steer the 
course of illness in another direction."Conley acknowledged that his evasive answers "came off that we were trying to hide 
something" but said that "wasn't necessarily true," adding that the President is "doing really well" and is responding to treatment.
During the briefing Sunday, Conley acknowledged that the President has experienced "two episodes of transient drops in his oxygen 
saturation" and said the team debated the reasons for that and whether to intervene. He said the President was given supplemental oxygen and has also been treated with the steroid dexamethasone, and his current blood oxygen level is 98%.
But Conley refused to say how low the President's blood oxygen levels had dropped. When asked if they had dropped below 90, 
he replied, "We don't have any recordings here of that." Pressed again on whether they had dropped below 90, Conley said 
the President's blood oxygen levels didn't get down into "the low 80s."
He offered no detail about what X-rays or CT scans have shown about whether there has been any damage to the President's lungs.
"There's some expected findings, but nothing of any major clinical concern," Conley said, not explaining whether they were 
expected findings in a normal patient or a Covid-19 patient.
Some seven months into a pandemic that has killed more than 209,000 Americans, the nation is now facing a 
grave governing crisis with its commander in chief hospitalized -- his condition hinging on his progress over 
the coming days -- as the White House events of the past week serve as a textbook example of how not to handle a deadly virus"""

from transformers import AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration
import wandb
import torch

wandb.init(project="long", entity="mahsanghani")
# epochs = 1
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

tokenizer = AutoTokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")
# .to(device)
optimizer = torch.optim.Adam(model.parameters(),lr=3e-4,amsgrad=True)
wandb.watch(model,log='all')
inputs = tokenizer.encode("summarize: " + text,
                          return_tensors='pt',
                          max_length=512,
                          truncation=True)

# Generate Summary
# inputs.to(device)
# model.to(device)
summary_ids = model.generate(
    inputs, 
    max_length=150, 
    min_length=80, 
    length_penalty=5., 
    num_beams=2)
# model.to(device)
print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))

from transformers import AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration
import wandb

wandb.init(project="long", entity="mahsanghani")
epochs = 1
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

tokenizer = AutoTokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")
# .to(device)
optimizer = torch.optim.Adam(model.parameters(),lr=3e-4,amsgrad=True)
wandb.watch(model,log='all')
inputs = tokenizer.encode("summarize: " + ' '.join(df1['sentence']),
                          return_tensors='pt',
                          max_length=512,
                          truncation=True)

# Generate Summary
# inputs.to(device)
# model.to(device)
summary_ids = model.generate(
    inputs, 
    max_length=150, 
    min_length=80, 
    length_penalty=5., 
    num_beams=2)
print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))

from transformers import T5Tokenizer, T5ForConditionalGeneration
qa_input = """question: What is the capital of Syria? context: The name "Syria" historically referred to a wider region,
 broadly synonymous with the Levant, and known in Arabic as al-Sham. The modern state encompasses the sites of several ancient 
 kingdoms and empires, including the Eblan civilization of the 3rd millennium BC. Aleppo and the capital city Damascus are 
 among the oldest continuously inhabited cities in the world."""
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')
input_ids = tokenizer.encode(qa_input, return_tensors="pt")  # Batch size 1
outputs = model.generate(input_ids)
output_str = tokenizer.decode(outputs.reshape(-1))
output_str



model_checkpoint = "t5-small"
if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""

max_input_length = 1024
max_target_length = 128

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["text"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

preprocess_function(raw_datasets['train'][:2])

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

batch_size = 16
model_name = model_checkpoint.split("/")[-1]
args = Seq2SeqTrainingArguments(
    f"{model_name}-finetuned-youtube-transcription",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=False,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

import nltk
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    # Extract a few results
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    
    # Add mean generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)
    
    return {k: round(v, 4) for k, v in result.items()}

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

# trainer.push_to_hub()

