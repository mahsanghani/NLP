# -*- coding: utf-8 -*-
"""CometLLM_Prompts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zh1xR1TB0nOiyq4AlFycrTlt2-8kBU9L

<p align="center">
    <picture>
        <source alt="cometLLM" media="(prefers-color-scheme: dark)" srcset="https://github.com/comet-ml/comet-llm/raw/main/logo-dark.svg">
        <img alt="cometLLM" src="https://github.com/comet-ml/comet-llm/raw/main/logo.svg">
    </picture>
</p>
<p align="center">
    <a href="https://pypi.org/project/comet-llm">
        <img src="https://img.shields.io/pypi/v/comet-llm" alt="PyPI version"></a>
    <a rel="nofollow" href="https://opensource.org/license/mit/">
        <img alt="GitHub" src="https://img.shields.io/badge/License-MIT-blue.svg"></a>   
    <a href="https://www.comet.com/docs/v2/guides/large-language-models/overview/" rel="nofollow">
        <img src="https://img.shields.io/badge/cometLLM-Docs-blue.svg" alt="cometLLM Documentation"></a>
    <a rel="nofollow" href="https://pepy.tech/project/comet-llm">
        <img style="max-width: 100%;" src="https://static.pepy.tech/badge/comet-llm" alt="Downloads"></a>   
</p>
<p align="center">

CometLLM is a new suite of LLMOps tools designed to help you effortlessly track and visualize your LLM prompts and chains. Use CometLLM to identify effective prompt strategies, streamline your troubleshooting, and ensure reproducible workflows.  

CometLLM complements Comet experiment tracking and production model management tools to arm LLM practitioners with everything they need to interact with, manage, and optimize their models with ease.  

üëâ The best part? [It's 100% free to get started!](https://www.comet.com/signup/?utm_source=comet_llm&utm_medium=referral&utm_content=intro_colab)

__________
This guide will cover some of the basic features for logging prompts to Comet LLM.

For a preview of what's possible with CometLLM, head over to one of our example projects in the [public Comet workspace](https://www.comet.com/signup/?utm_source=comet_llm&utm_medium=referral&utm_content=intro_colab)!

# üöß Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q comet_llm torch torchdata transformers datasets

import comet_llm

import os

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer
from transformers import GenerationConfig
import time

"""If you don't already have a Comet account, [create one here for free](https://www.comet.com/signup/?utm_source=comet_llm&utm_medium=referral&utm_content=intro_colab) and grab your API credentials."""

os.environ['COMET_API_KEY'] = YOUR-COMET-API-KEY
os.environ['COMET_PROJECT_NAME'] = YOUR-PROJECT-NAME

"""Now we're ready to start logging prompts to Comet! At the simplest level, logging a prompt to CometLLM just requires an input prompt and an output prompt:"""

comet_llm.log_prompt(
    prompt = "What is this conversation about?",
    output = "A customer wants to return a purchase."
)

"""It's really that simple! To check out your logged prompt in the Comet UI, click on the link above.

In most real-world scenarios, however, we'll want to log a lot more information than just the input and output. In the following examples we'll cover how to log a prompt with:

- üó∫ Instructions

- üìÖ Metadata

- üéì In-context learning:

    - ‚öΩ One-shot-inference

    - üèÄ üéæ Few-shot-inference

- üéõ [Hyperparameter](https://www.comet.com/production/site/lp/your-ultimate-guide-to-hyperparameter-tuning/) configurations

# ü§ñ Our application

For this tutorial, **imagine you lead the Customer Support team at your company. It's the end of the quarter, and you want to summarize all of the support issues your team has dealt with to identify some possible areas of improvement.**

We'll be using the [dialogsum dataset](https://huggingface.co/datasets/knkarthick/dialogsum) from Hugging Face, which consists of 13,460 short conversations with corresponding manually labeled summaries and topics.

To summarize these conversations, we'll use Hugging Face's implementation of [FLAN-T5](https://huggingface.co/google/flan-t5-base).
"""

DATASET_NAME = "knkarthick/dialogsum"
MODEL_NAME = "google/flan-t5-base"

dataset = load_dataset(DATASET_NAME, split = "test") #[120, 255, 303, 321, 333, 348, 354]
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

"""## üó∫ Prompt with instructions

One of the most basic ways to improve our prompt is with a set of simple instructions. You may want to play around with the format and wording of these instructions to determine the prompt that best helps your model understand the task. Sometimes even slight rephrasings of a prompt can significantly alter the output.

This form of prompt engineering is typically the first one a practitioner will engage in because it's very, very inexpensive and gives you quick insights into whether you're on the right path.
"""

def summarize_v1(user_prompt):

    input = tokenizer(user_prompt, return_tensors = "pt")
    output = tokenizer.decode(
        model.generate(
            input["input_ids"],
            max_new_tokens = 50,
        )[0],
        skip_special_tokens=True,
    )

    comet_llm.log_prompt(prompt = user_prompt, output = output)
    return output

user_prompt = f"""
Summarize the following conversation.

{dataset['dialogue'][255]}

Summary:
    """

"""Let's take a look at what our final prompt will look like:"""

print(user_prompt)

"""Now let's see how well our model summarizes the conversation."""

summarize_v1(user_prompt)

"""Finally, let's compare this with our ground-truth label:"""

dataset['summary'][255]

"""[![SgRF2.gif](https://s11.gifyu.com/images/SgRF2.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)


Not bad, but we can do better! Let's try a few more prompt engineering techniques.

## üìÖ Prompt with metadata

As we begin to alter, or "engineer," our prompts, we might also want to log some important metadata. If we're comparing the output of several models, we'll want to log which models produced which results. We may also want to play around with different hyperparameter values, or "generation configurations" (more on that later).

Some other relevant pieces of information to log might include:
- ‚è∞ How long does each prompt take to process? (duration)
- üó£ Which task is the model performing? (summarization, text-generation, translation, etc.)
- üè∑ Do we have ground truth labels? (usually human-generated responses)
"""

def summarize_v2(user_prompt, prompt_template, tags, metadata):
    start = time.time()

    variables = {"user_prompt": user_prompt}
    final_prompt = prompt_template.format(**variables)

    input = tokenizer(final_prompt, return_tensors="pt")
    output = tokenizer.decode(
        model.generate(
            input["input_ids"],
            max_new_tokens=metadata["max_new_tokens"],
        )[0],
        skip_special_tokens=True,
    )

    duration = time.time() - start

    comet_llm.log_prompt(
        prompt=final_prompt,
        prompt_template=prompt_template,
        prompt_template_variables=variables,
        output=output,
        tags=tags,
        duration=duration * 1000,
        metadata=metadata,
    )

    return output

user_prompt = dataset['dialogue'][348]
ground_truth = dataset['summary'][348]

prompt_template = """
Summarize the following conversation.

{user_prompt}

Summary:
    """

"""Our final prompt:"""

print(user_prompt)

METADATA = {
    "model": MODEL_NAME,
    "max_new_tokens": 50,
    "skip_special_tokens": True,
    "ground_truth" : ground_truth
}

TAGS = ["prompt-with-instructions", "summarization"]

"""Our output:"""

summarize_v2(user_prompt, prompt_template, TAGS, METADATA)

"""[![SgRFz.gif](https://s11.gifyu.com/images/SgRFz.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)

## üéì Prompt template with in-context learning

Once we've done everything we can to optimize the prompt instructions, we might choose to further improve performance by including examples in our prompt. This is called [in-context learning](https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610#:~:text=Now%2C%20we%20can%20give%20a,source).

In one-shot inference, we provide a single example within the prompt. In few-shot learning, we provide multiple examples within the prompt. Generally, if you need more than five or six examples to get the output you're looking for, you may want to consider fine-tuning your model or selecting a different model.

If our few-shot example doesn't perform much better than our one-shot example, we might consider using the one-shot example for better latency (good thing we're keeping tracking of that in our metadata!). We'll also have to be aware of our model's context window (in this case, 512 tokens), which limits how many examples we can provide.

For our use case one "example" will include both a conversation (to be summarized), as well as an accurate summarization (available in the ground truth labels of our dataset).

### üéæ One-shot inference

We provide a single example within our prompt:
"""

def summarize_v3(user_prompt, prompt_template, tags, metadata):
    start = time.time()

    variables = {
        "user_prompt": user_prompt,
        "example_1": example_1,
        "summary_1": summary_1,
    }
    final_prompt = prompt_template.format(**variables)

    input = tokenizer(final_prompt, return_tensors="pt")
    output = tokenizer.decode(
        model.generate(
            input["input_ids"],
            max_new_tokens=metadata["max_new_tokens"],
        )[0],
        skip_special_tokens=True,
    )

    duration = time.time() - start

    comet_llm.log_prompt(
        prompt=final_prompt,
        prompt_template=prompt_template,
        prompt_template_variables=variables,
        output=output,
        tags=tags,
        duration=duration * 1000,
        metadata=metadata,
    )

    return output

"""‚≠ê **Note** that because the context window of FLAN-T5 is limited to 512 tokens, we specifically used samples from the dataset with the shortest lengths. If you're experimenting with different examples at home, make sure that your total input sequence length doesn't exceed 512 tokens. If it does, the model will truncate the input."""

user_prompt = dataset["dialogue"][321]
example_1 = dataset["dialogue"][120]
summary_1 = dataset["dialogue"][120]

prompt_template = f"""
Summarize the following conversation.

{example_1}

Summary:
{summary_1}


"""
prompt_template += """
Summarize the following conversation.

{user_prompt}

Summary:
"""

METADATA = {
    "model": MODEL_NAME,
    "max_new_tokens": 50,
    "skip_special_tokens": True,
}

TAGS = ["one-shot-inference", "summarization"]

"""Our final prompt:"""

print(user_prompt)

"""Our output:"""

summarize_v3(user_prompt, prompt_template, TAGS, METADATA)

"""[![SgRFL.gif](https://s11.gifyu.com/images/SgRFL.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)

### ‚öΩ üèÄ Few-shot inference
We provide multiple examples within our prompt:
"""

def summarize_v4(user_prompt, prompt_template, tags, metadata):
    start = time.time()

    variables = {
        "user_prompt": user_prompt,
        "example_1": example_1,
        "summary_1": summary_1,
        "example_2": example_2,
        "summary_2": summary_2
    }
    final_prompt = prompt_template.format(**variables)

    input = tokenizer(final_prompt, return_tensors="pt")
    output = tokenizer.decode(
        model.generate(
            input["input_ids"],
            max_new_tokens=metadata["max_new_tokens"],
        )[0],
        skip_special_tokens=True,
    )

    duration = time.time() - start

    comet_llm.log_prompt(
        prompt=final_prompt,
        prompt_template=prompt_template,
        prompt_template_variables=variables,
        output=output,
        tags=tags,
        duration=duration * 1000,
        metadata=metadata,
    )

    return output

METADATA = {
    "model": MODEL_NAME,
    "max_new_tokens": 50,
    "skip_special_tokens": True,
}

TAGS = ["few-shot-inference", "summarization"]

user_prompt = dataset["dialogue"][57]
example_1 = dataset["dialogue"][555]
summary_1 = dataset["summary"][555]
example_2 = dataset["dialogue"][933]
summary_2 = dataset["summary"][933]

prompt_template = f"""
Summarize the following conversation.

{example_1}

Summary:
{summary_1}


Summarize the following conversation.

{example_2}

Summary:
{summary_2}



"""
prompt_template += """
Summarize the following conversation.

{user_prompt}

Summary:
"""

"""Our final prompt:"""

print(user_prompt)

"""Our output:"""

summarize_v4(user_prompt, prompt_template, TAGS, METADATA)

"""[![SgRFK.gif](https://s11.gifyu.com/images/SgRFK.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)

## üéõ Optimizing generation configurations

In much the same way that we tune and optimize our hyperparameter values in traditional machine learning applications, in generative AI, we can tweak our "generation configuration."

Generation configuration parameters for this model include sampling methods, temperature, maximum new tokens, and more. The role of each of these parameters is beyond the scope of this tutorial, but generally speaking, we can think of the `temperature` as controlling the "creativity" of the model and the `sampling method` as controlling the "relevance" of the model.

Because we're using the same prompt template, this time we'll only need to define the `user_prompt` (test prompt).
"""

user_prompt = dataset["dialogue"][1155]

def summarize_v5(user_prompt, prompt_template, tags, metadata):
    start = time.time()

    variables = {
        "user_prompt": user_prompt,
        "example_1": example_1,
        "summary_1": summary_1,
        "example_2": example_2,
        "summary_2": summary_2
    }
    final_prompt = prompt_template.format(**variables)

    input = tokenizer(final_prompt, return_tensors="pt")
    output = tokenizer.decode(
        model.generate(
            input["input_ids"],
            generation_config=GenerationConfig(
                max_new_tokens=metadata["max_new_tokens"],
                do_sample=metadata["do_sample"],
                temperature=metadata["temperature"],
            ),
        )[0],
        skip_special_tokens=True,
    )

    duration = time.time() - start

    comet_llm.log_prompt(
        prompt=final_prompt,
        prompt_template=prompt_template,
        prompt_template_variables=variables,
        output=output,
        tags=tags,
        duration=duration * 1000,
        metadata=metadata,
    )

    return output

METADATA = {
    "model": MODEL_NAME,
    "max_new_tokens": 50,
    "skip_special_tokens": True,
    "do_sample": True,
    "temperature": 0.1,
}

TAGS = ["optimizing-config", "summarization"]

"""Our final prompt:"""

print(user_prompt)

"""Our output:"""

summarize_v5(user_prompt, prompt_template, TAGS, METADATA)

"""Note that we can also sort our rows by ascending or descending column values:

[![SgRFT.gif](https://s11.gifyu.com/images/SgRFT.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)

# üîé Prompt search

Prompt engineering is a highly iterative process, so you're likely to repeat these processes many, many times. To make it easier to sift through all of your prompts, CometLLM has a search feature that allows you to isolate experiment runs based on keywords.

Maybe we run our experiments a few dozen times (or more!) before realizing that one of our example prompts (for in-context learning) has been incorrectly labeled. We want to remove any runs containing that example, because the output isn't relevant anymore.

Simply select the prompt variable you'd like to search and the filtering operator you'd like to use. Then type in your keyword and Comet will do the rest! Now we've found our erroneous prompts and can remove them!

_____

[![SgRjL.gif](https://s11.gifyu.com/images/SgRjL.gif)](https://www.comet.com/examples/cometllm-prompt-example/prompts)

____

# üìì Additional Resources

- [Read our full CometLLM announcement](https://heartbeat.comet.ml/organize-your-prompt-engineering-with-cometllm-66e390ef6645)
- [Read Comet's prompt engineering blog post](https://heartbeat.comet.ml/organize-your-prompt-engineering-with-cometllm-66e390ef6645)
- [Check out our GitHub repo and give us a star](https://github.com/comet-ml/comet-llm)
- [Connect with us on our Community Slack channel](https://cometml.slack.com/join/shared_invite/enQtMzM0OTMwNTQ0Mjc5LWE4NzcxMzdiMmFjYzEzM2E5OTczOTk1MDZmZDg2MGJmODUwYWI0YWQ0YWMyMjlmMjQ5YmVmNzEyYjNlNzFhNjQ#/shared-invite/email)
"""