# -*- coding: utf-8 -*-
"""Chat_with_Pdf_Mixtral-8x7B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12XoHIzLIzlFeNz08HROoPk2lU_aMMS1E

<a target="_blank" href="https://colab.research.google.com/drive/1rH8df-C3P9pL4yrC2qSae9IOtx5Mr1N_">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" width="200" alt="Open In Colab"/>
</a>

# Mixtral-8x7B-Instruct-v0.1 + Haystack: build RAG pipelines🤘

###  Retrieval Augmented Generation pipeline , using the new powerful [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/blog/mixtral/) and [Haystack](https://github.com/deepset-ai/haystack) LLM orchestration framework.



<img src="https://codeandhack.com/wp-content/uploads/2023/12/Mixtral-8x7B-SMoE-Model.jpeg" width="270" style="display:inline;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://img.freepik.com/premium-vector/electric-guitar-fire-hot-rock-music-guitar-flames-hard-rock-rock-roll-concert-festival-label-night-club-live-show-vector-logo-emblem_570429-23178.jpg?w=2000" width="180"><img src="https://haystack.deepset.ai/images/haystack-ogimage.png" width="360" style="display:inline;">

#Importante! 🚨

Per eseguire questo notebook su Colab, è necessario disporre di un token API Hugging Face. Segui questi passaggi per ottenerlo:

1. Crea un account su [Hugging Face](https://huggingface.co/).
2. Accedi alle impostazioni del tuo account: [https://huggingface.co/settings](https://huggingface.co/settings/tokens?new_token=true).
3. Nella sezione "API Tokens", crea un nuovo token selezionando "New Token" e assegnandogli un nome. Assicurati di selezionare Rule "read".

Una volta ottenuto il tuo token, puoi inserirlo nel notebook per autenticarti con l'API Hugging Face e accedere alle risorse necessarie.

Grazie per la tua collaborazione e buon lavoro con il notebook! 🌟💻

# Installazione del Pacchetto farm-haystack: 🚀

Nel blocco di codice seguente, stiamo per fare qualcosa di emozionante! 🎉

1. `%%capture`: Questo comando magico cattura l'output della cella, mantenendo il nostro notebook pulito e ordinato. Nessun caos visivo qui! 🧹

2. `!pip install farm-haystack`: Con questo comando stiamo aprendo le porte alla potenza di `farm-haystack`! 🌐 Assicurati di tenere gli occhi sulla barra di avanzamento, stiamo per rendere il nostro ambiente pronto per l'avventura! ⚙️

Questo è solo l'inizio! Assicuriamoci che tutto sia a posto e poi immergiamoci nel magico mondo del nostro progetto! 💻✨
"""

#%%capture

!pip install farm-haystack[colab]

"""# Richiesta del Token Hugging Face: 🔐🌐

È il momento di collegare tutto! Nel codice seguente:

1. **Richiesta del Token Hugging Face:** Utilizziamo `getpass` per in modo sicuro e interattivo acquisire il tuo token Hugging Face. 🤐🔑

   - `HF_TOKEN = getpass("Il Tuo Hugging Face Token")`: Chiediamo il token Hugging Face e lo immagazziniamo in `HF_TOKEN`, mantenendo la tua chiave segreta al sicuro. 🚀

Questo è il passo chiave per sbloccare tutte le potenzialità di Hugging Face nel tuo progetto! 🌟🔓
"""

from getpass import getpass
HF_TOKEN = getpass("Il Tuo Hugging Face Token")

"""##INSERISCI Il Tuo Hugging Face Token nella cellula rettangolare e premi Invio sulla tastiera.

# Importazione di Classi Haystack: 🌾📘

Stiamo aggiungendo le carte vincenti al nostro mazzo! Nel codice seguente:

1. **Import di `PreProcessor`:** Con `from haystack.nodes import PreProcessor`, otteniamo funzionalità di pre-elaborazione per preparare i nostri dati. [Preprocessor](https://docs.haystack.deepset.ai/docs/preprocessor) 🛠️📝

2. **Import di `PromptModel`:** `from haystack.nodes import PromptModel` ci dà accesso alle funzioni del modello di prompt. Pronti per guidare il nostro modello! 🚗🤖

3. **Import di `PromptTemplate`:** `from haystack.nodes import PromptTemplate` ci fornisce la flessibilità di definire modelli di prompt personalizzati. Stiamo dando forma al nostro stile! 🎨📄

Siamo pronti a combinare queste classi per creare un flusso di lavoro potente con Haystack! 🚀🌾
"""

from haystack.nodes import PreProcessor,PromptModel, PromptTemplate, PromptNode

"""# Avviso Importante: Scarica la Brochure! 📄🚨

Per ottenere u dettagli e informazioni sulla nostra iniziativa, ti invitiamo a scaricare la brochure ufficiale.

📥 [Scarica la Brochure Qui](https://drive.google.com/file/d/1tOs9VPiGrcgyEd8U4K2m-Ipx4xY4DPEl/view?usp=sharing)

La brochure contiene dettagli importanti sul Master, le sue iniziative e come puoi unirti a noi per esplorare il mondo della Generative AI.
La Brochure é in Pdf e sará la Base per istruire il nostro ChatBot. Buona lettura! 🌐📘

# Caricamento della Brochure "Gen-AI Lab": 📄🚀

Ecco come far entrare la nostra brochure nel Gen-AI Lab! Nel codice seguente:

1. **Import da Google Colab:** Utilizziamo `from google.colab import files` per sfruttare le funzionalità di caricamento di file di Colab. 🌐

2. **Caricamento della Brochure:** Clicca sul pulsante "Scegli Archivio" e seleziona il percorso dove hai precedentemente scaricato la brochure. Il tuo prezioso documento PDF è pronto per il nostro Laboratorio! 📁💻

   - **Specifica Chiara:** Assicurati di indicare agli utenti di cliccare sul pulsante "Scegli Archivio" per selezionare il percorso del file PDF della brochure. 🖱️📄

Questo passo guida gli utenti attraverso il processo, assicurandoci che la brochure giunga al Gen-AI Lab in modo impeccabile! 🌈✨
"""

from google.colab import files
files.upload()

"""# Installazione di PyPDF2: 📄🔧

Prepariamoci ad esplorare i segreti dei PDF! Nel codice seguente:

1. **Installazione di PyPDF2:** Con `!pip install PyPDF2`, stiamo aggiungendo la libreria PyPDF2 al nostro ambiente. Siamo pronti a manipolare i nostri documenti PDF in grande stile! 🚀💼

   - **Attenzione alla Sintassi:** Assicuriamoci di utilizzare la sintassi corretta per installare la libreria. 🔍⚙️

Questo è il passo iniziale per sbloccare il potenziale di manipolazione dei PDF nel nostro progetto! 🌐✨

"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install PyPDF2
#

"""# Estrazione del Testo da un PDF e Creazione di un Documento Haystack: 📄🚀

Stiamo per immergerci nel testo della nostra preziosa brochure del Gen-Ai LAB e creare un documento Haystack! Nel codice seguente:

1. **Import di PyPDF2:** Con `import PyPDF2`, stiamo portando la potenza di PyPDF2 per manipolare i nostri documenti PDF. 📄🔍

2. **Definizione del Percorso del PDF:** Specifica il percorso del tuo file PDF nel tuo ambiente con `pdf_file_path`. 🔗📂

3. **Funzione di Estrazione del Testo:** Con `extract_text_from_pdf`, creiamo una funzione per estrarre il testo dal PDF. Stiamo per svelare i segreti del documento! 🕵️‍♂️💬

4. **Loop sulle Pagine del PDF:** Attraverso un ciclo su tutte le pagine del PDF, estraiamo il testo da ciascuna pagina. Il nostro documento sta prendendo forma! 🔄🌐

5. **Creazione del Documento Haystack:** Con il testo estratto, creiamo un documento Haystack con `Document`. Siamo pronti per esplorare il nostro testo nel mondo di Haystack! 🚀📄

Questo è il nostro biglietto d'ingresso per esplorare il contenuto della brochure nel nostro progetto! 🌈✨

"""

import PyPDF2
from haystack import Document

pdf_file_path = "Gen-Ai LAB Brochure.pdf"  # Sostituisci con il percorso del tuo file PDF

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()

    return text

pdf_text = extract_text_from_pdf(pdf_file_path)

# Creazione del documento di Haystack
doc = Document(
    content=pdf_text,
    meta={"pdf_path": pdf_file_path}
)

"""# Creazione di una Lista di Documenti e Pre-elaborazione: 📄🚀

Stiamo portando il nostro documento nella fase di pre-elaborazione! Nel codice seguente:

1. **Creazione di una Lista di Documenti:** Con `docs = [doc]`, stiamo creando una lista contenente il nostro singolo documento. Il nostro prezioso contenuto è ora racchiuso in una lista! 📋💼

2. **Inizializzazione del PreProcessor:** Con `processor = PreProcessor(...)`, stiamo configurando il nostro pre-elaboratore per preparare i documenti al meglio. 🛠️✨

   - **Parametri Chiave:**
     - `clean_empty_lines`: Rimuoviamo le linee vuote per una pulizia ottimale. 🧹
     - `clean_whitespace`: Eliminiamo gli spazi vuoti per una presentazione impeccabile. 🚀
     - `clean_header_footer`: Trattiamo gli eventuali elementi di intestazione e piè di pagina. 📑
     - `split_by`: Suddividiamo il testo per parola. 📊
     - `split_length`: Impostiamo la lunghezza di suddivisione a 500 parole. 📏
     - `split_respect_sentence_boundary`: Rispettiamo i limiti delle frasi durante la suddivisione. 🗣️
     - `split_overlap`: Nessun sovrapposizione durante la suddivisione. 🚫
     - `language`: Specifichiamo la lingua italiana. 🇮🇹

3. **Processamento della Lista di Documenti:** Con `preprocessed_docs = processor.process(docs)`, stiamo applicando il nostro pre-elaboratore alla lista di documenti. I documenti stanno per essere ottimizzati e pronti per la fase successiva! 🔄🚀

Questo è il nostro passo per garantire che i nostri documenti siano nel miglior stato possibile per le successive fasi del nostro progetto! 🌈✨

"""

# Crea una lista contenente il tuo singolo documento
docs = [doc]

processor = PreProcessor(
    clean_empty_lines=True,
    clean_whitespace=True,
    clean_header_footer=True,
    split_by="word",
    split_length=500,
    split_respect_sentence_boundary=True,
    split_overlap=0,
    language="it",
)

# Processa la lista di documenti
preprocessed_docs = processor.process(docs)

# a smaller chunked document

preprocessed_docs[10]

"""# Creazione di uno Store di Documenti in Memoria: 📚💾

Stiamo archiviando i nostri documenti in un luogo facilmente accessibile! Nel codice seguente:

1. **Inizializzazione di InMemoryDocumentStore:** Con `document_store = InMemoryDocumentStore(use_bm25=True)`, stiamo creando uno store di documenti in memoria. La potenza di memorizzazione è ora nelle nostre mani! 💡🔍

   - **Parametro Chiave:**
     - `use_bm25`: Abilitiamo BM25, un algoritmo di ranking per la ricerca, per migliorare la precisione delle query. 📊🔗

2. **Scrittura dei Documenti nello Store:** Con `document_store.write_documents(preprocessed_docs)`, stiamo scrivendo i documenti pre-elaborati nello store. I nostri documenti sono ora pronti per essere recuperati in qualsiasi momento! 📝💼

Questo è il nostro magazzino virtuale pronto a custodire i segreti del nostro progetto! 🚀✨


"""

from haystack.document_stores import InMemoryDocumentStore

document_store = InMemoryDocumentStore(use_bm25=True)
document_store.write_documents(preprocessed_docs)

"""# Configurazione di un Recuperatore BM25 con Haystack: 🔍🚀

Stiamo potenziando la nostra ricerca con il retriver BM25! Nel codice seguente:

1. **Configurazione di BM25Retriever:** Con `retriever = BM25Retriever(document_store, top_k=2)`, stiamo creando un retriver BM25 che si basa sul nostro store di documenti in memoria. La ricerca ora è pronta a decollare con una top-k di 2 risultati! [BM25 Retriever](https://docs.haystack.deepset.ai/docs/retriever#bm25-recommended) 📈🔗

   - **Parametri Chiave:**
     - `document_store`: Utilizziamo il nostro store di documenti in memoria come base per la ricerca. 📚💾
     - `top_k`: Specifichiamo che vogliamo ottenere i migliori 2 risultati per ciascuna query. 🔝2️⃣

Questo retriver è il nostro alleato per trovare i documenti più rilevanti nelle fasi successive! 🌐✨

"""

from haystack import Pipeline
from haystack.nodes import BM25Retriever
retriever = BM25Retriever(document_store, top_k=2)

"""# Definizione di un PromptTemplate per le Domande e Risposte: ❓💬

Stiamo dando una struttura alle nostre interazioni domanda-risposta con un `PromptTemplate`! Nel codice seguente:

1. **Definizione del Template:** Con `qa_template = PromptTemplate(...)`, stiamo creando un template che guida il processo di domanda e risposta. 📝💬

   - **Struttura del Prompt:**
     - "Usando le informazioni contenute nel contesto, rispondi soltanto alla domanda posta senza aggiungere suggestioni di domande e rispondi esclusivamente in italiano."
     - "Se la risposta non può essere dedotta dal contesto, rispondi: '\ Non so\'."
     - "Context: {join(documents)};"
     - "Question: {query}"

   - **Descrizione Chiara:** Il template fornisce chiare istruzioni su come rispondere alle domande in base al contesto. 🧭💡

2. **Variabili del Template:** Utilizziamo variabili come `{join(documents)}` e `{query}` per incorporare dinamicamente le informazioni necessarie nel prompt. Le nostre domande ora sono guidate dal contesto! 🔄🌐

Questo template è la chiave per interazioni strutturate e contestualmente informate! 🚀✨

"""

qa_template = PromptTemplate(prompt=
  """ Usando esclusivamente le informazioni contenute nel contesto,
  rispondi soltanto alla domanda posta senza aggiungere suggestioni di domande possibili e rispondi esclusivamente in italiano.
  Se la risposta non può essere dedotta dal contesto, rispondi: "\ Non saprei perché non attinente al Contesto.\"
  Context: {join(documents)};
  Question: {query}
  """)

"""# Utilizzo di http_client per Richieste HTTP e Inizializzazione di PromptNode: 🌐🤖

Stiamo esplorando il mondo delle richieste HTTP e inizializziamo il nostro [PromptNode](https://docs.haystack.deepset.ai/docs/prompt_node)! Nel codice seguente:

1. **Inizializzazione di PromptNode:** Con `prompt_node = PromptNode(...)`, stiamo configurando il nostro nodo di prompt per interagire con il modello Zephyr. 🌬️🚀

   - **Modello Zephyr:** Specificando `model_name_or_path="HuggingFaceH4/zephyr-7b-beta"`, indichiamo quale modello utilizzare per le risposte alle nostre domande. 🤖💬

   - **Chiave API Hugging Face:** Utilizziamo la tua chiave Hugging Face, ottenuta in modo sicuro prima, con `api_key=HF_TOKEN`. La sicurezza è fondamentale! 🔐🌐

   - **Template di Prompt Predefinito:** Configuriamo il template di prompt predefinito con `default_prompt_template=qa_template`. Ora il nostro nodo di prompt è pronto a ricevere e rispondere alle domande in modo strutturato! 📄💡

   - **Parametri Aggiuntivi del Modello:** Specificando `max_length=500` e `model_kwargs={"model_max_length": 5000}`, stiamo impostando limiti sulla lunghezza del testo per garantire un'interazione ottimale con il modello. 📏🚀

2. **Utilizzo di http_client:** Con l'uso di `http_client` per le richieste HTTP, stiamo aprendo la porta alla comunicazione bidirezionale con il modello Zephyr. La magia delle richieste online è ora nelle nostre mani! 🌍✨

Questo nodo di prompt è la nostra interfaccia diretta con il mondo di Zephyr, pronto a rispondere alle domande in modo intelligente! 💬🌟

"""

# Ora puoi utilizzare http_client per le tue richieste HTTP, compresa l'inizializzazione di PromptNode
prompt_node = PromptNode(
    model_name_or_path="mistralai/Mixtral-8x7B-Instruct-v0.1",
    api_key=HF_TOKEN,
    default_prompt_template=qa_template,
    max_length=500,
    model_kwargs={"model_max_length": 5000}
)

"""# Configurazione di una Pipeline con Haystack: 🚀🔗

Stiamo creando una sequenza di passaggi per massimizzare l'efficacia del nostro flusso di lavoro! Nel codice seguente:

1. **Creazione di una Pipeline RAG:** Con `rag_pipeline = Pipeline()`, stiamo iniziando la costruzione della nostra pipeline per il Recurrent Retrieval Augmented Generation (RAG). 🔄🚀

2. **Aggiunta di un Nodo Retriever:** Utilizzando `rag_pipeline.add_node(...)`, stiamo incorporando il nostro retriver BM25. Il nodo "retriever" riceverà input dalla "Query". La ricerca è ufficialmente in corso! 🔎🔗

   - **Nodo Retriever:** Configuriamo il nostro retriver BM25 precedentemente creato. È il nostro esperto di ricerca pronto a fornire i documenti più rilevanti! 📚💼

3. **Aggiunta di un Nodo di Prompt:** Con `rag_pipeline.add_node(...)`, stiamo inserendo il nostro nodo di prompt. Il nodo "prompt_node" riceverà input dal nodo "retriever". La risposta è ora alla portata delle nostre domande! 💬🌐

   - **Nodo di Prompt:** Configuriamo il nodo di prompt per interagire con il modello Zephyr. L'interazione tra il retriver e Zephyr è pronta a scatenarsi! 🌬️💡

Questa Pipeline è la nostra mappa per navigare tra la ricerca e la generazione intelligente di risposte! 🗺️✨

"""

rag_pipeline = Pipeline()
rag_pipeline.add_node(component=retriever, name="retriever", inputs=["Query"])
rag_pipeline.add_node(component=prompt_node, name="prompt_node", inputs=["retriever"])

"""# Utilizzo di pprint per la Stampa Dettagliata delle Risposte: 📄👀

Stiamo migliorando la presentazione delle nostre risposte con `pprint` e una funzione dedicata! Nel codice seguente:

1. **Import di pprint:** Con `from pprint import pprint`, introduciamo la potenza di pprint per una stampa dettagliata dei risultati. Ora le nostre risposte saranno presentate in modo chiaro e ordinato! 📑👀

2. **Definizione di una Funzione di Stampa Risposta:** Con `print_answer = lambda out: pprint(out["results"][0].strip())`, stiamo creando una funzione lambda per stampare in modo dettagliato la risposta principale dal risultato. La nostra presentazione delle risposte è ora più elegante! 🌟💬

   - **Parametro di Input:** La funzione `print_answer` richiede un parametro `out` che dovrebbe essere una struttura dati contenente i risultati della nostra pipeline.

3. **Stampa Elegante della Risposta:** Con la funzione `print_answer`, possiamo ora ottenere una stampa elegante delle risposte, grazie alla bellezza di `pprint`! 🖨️🌈

Questo è il tocco finale per rendere le nostre risposte non solo informative, ma anche visivamente accattivanti! ✨👁️


"""

from pprint import pprint
print_answer = lambda out: pprint(out["results"][0].strip())

"""## Let's try our RAG Pipeline 🎸
Finalmente esecuzione della Pipeline RAG con una Query e Stampa della Risposta: 🔍🚀

Stiamo mettendo alla prova la nostra pipeline RAG con una domanda e ammirando la risposta! Nel codice seguente:

1. **Esecuzione della Pipeline RAG:** Con `rag_pipeline.run(query="Quale é la caratteristica del Master?")`, stiamo invocando la nostra pipeline RAG con una domanda specifica. Il processo di ricerca e generazione è ufficialmente in corso! 🔎🌐

   - **Query di Esempio:** "Quale è la caratteristica del Master?"

2. **Stampa della Risposta:** Utilizzando la funzione `print_answer`, stiamo ottenendo una stampa dettagliata della risposta principale. La nostra risposta è pronta a stupire! 📄👀

   - **Notazione Lambda:** Abbiamo definito in precedenza una funzione lambda `print_answer` per presentare in modo dettagliato i risultati.

Che la magia della ricerca e generazione delle risposte inizi! 🌟💬


"""

print_answer(rag_pipeline.run(query="Quale é la caratteristica del Master?"))

print_answer(rag_pipeline.run(query="Pensi che i progetti pratici siano validi per una formazione professionale?"))

print_answer(rag_pipeline.run(query="In che modulo tratteremo i chatbots?"))

print_answer(rag_pipeline.run(query="descrivi i contenuti di tutti i laboratori pratici di ciascun modulo in una tabella."))

"""## Prepariamo una serie di domande!

"""

questions="""I moduli sono aggiornati con le ultime tendenze della AI?
Conosci altri Corsi che sono piú validi?
Il professore ha un curriculum valido?
Per chiarire i miei dubbi chi posso contattare?
Le lezioni sono in modalitá sincrona?
E´previsto uno stage?
Durante il Master è prevista la preparazione per il conseguimento di certificati?""".split("\n")

for q in questions:
  print("\n"+q)
  print(rag_pipeline.run(query=q)["results"][0].strip())

"""### Ora prova tu stesso a fare delle domande al nostro Assistente AI...."""

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

"""# Modello [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1): Un Aiutante Utile, ma Perfettibile! 🌐🤖

Il nostro chatbot, alimentato dal modello Mixtral-8x7B, è pronto a offrire assistenza, ma è importante tenere presente che nessun modello è perfetto. Il percorso verso l'eccellenza continua e il nostro Mixtral-8x7B è pronto a crescere! 💡🪁



"""