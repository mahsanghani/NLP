# -*- coding: utf-8 -*-
"""Chat_with_Pdf_Mixtral-8x7B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12XoHIzLIzlFeNz08HROoPk2lU_aMMS1E

<a target="_blank" href="https://colab.research.google.com/drive/1rH8df-C3P9pL4yrC2qSae9IOtx5Mr1N_">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" width="200" alt="Open In Colab"/>
</a>

# Mixtral-8x7B-Instruct-v0.1 + Haystack: build RAG pipelinesğŸ¤˜

###  Retrieval Augmented Generation pipeline , using the new powerful [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/blog/mixtral/) and [Haystack](https://github.com/deepset-ai/haystack) LLM orchestration framework.



<img src="https://codeandhack.com/wp-content/uploads/2023/12/Mixtral-8x7B-SMoE-Model.jpeg" width="270" style="display:inline;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://img.freepik.com/premium-vector/electric-guitar-fire-hot-rock-music-guitar-flames-hard-rock-rock-roll-concert-festival-label-night-club-live-show-vector-logo-emblem_570429-23178.jpg?w=2000" width="180"><img src="https://haystack.deepset.ai/images/haystack-ogimage.png" width="360" style="display:inline;">

#Importante! ğŸš¨

Per eseguire questo notebook su Colab, Ã¨ necessario disporre di un token API Hugging Face. Segui questi passaggi per ottenerlo:

1. Crea un account su [Hugging Face](https://huggingface.co/).
2. Accedi alle impostazioni del tuo account: [https://huggingface.co/settings](https://huggingface.co/settings/tokens?new_token=true).
3. Nella sezione "API Tokens", crea un nuovo token selezionando "New Token" e assegnandogli un nome. Assicurati di selezionare Rule "read".

Una volta ottenuto il tuo token, puoi inserirlo nel notebook per autenticarti con l'API Hugging Face e accedere alle risorse necessarie.

Grazie per la tua collaborazione e buon lavoro con il notebook! ğŸŒŸğŸ’»

# Installazione del Pacchetto farm-haystack: ğŸš€

Nel blocco di codice seguente, stiamo per fare qualcosa di emozionante! ğŸ‰

1. `%%capture`: Questo comando magico cattura l'output della cella, mantenendo il nostro notebook pulito e ordinato. Nessun caos visivo qui! ğŸ§¹

2. `!pip install farm-haystack`: Con questo comando stiamo aprendo le porte alla potenza di `farm-haystack`! ğŸŒ Assicurati di tenere gli occhi sulla barra di avanzamento, stiamo per rendere il nostro ambiente pronto per l'avventura! âš™ï¸

Questo Ã¨ solo l'inizio! Assicuriamoci che tutto sia a posto e poi immergiamoci nel magico mondo del nostro progetto! ğŸ’»âœ¨
"""

#%%capture

!pip install farm-haystack[colab]

"""# Richiesta del Token Hugging Face: ğŸ”ğŸŒ

Ãˆ il momento di collegare tutto! Nel codice seguente:

1. **Richiesta del Token Hugging Face:** Utilizziamo `getpass` per in modo sicuro e interattivo acquisire il tuo token Hugging Face. ğŸ¤ğŸ”‘

   - `HF_TOKEN = getpass("Il Tuo Hugging Face Token")`: Chiediamo il token Hugging Face e lo immagazziniamo in `HF_TOKEN`, mantenendo la tua chiave segreta al sicuro. ğŸš€

Questo Ã¨ il passo chiave per sbloccare tutte le potenzialitÃ  di Hugging Face nel tuo progetto! ğŸŒŸğŸ”“
"""

from getpass import getpass
HF_TOKEN = getpass("Il Tuo Hugging Face Token")

"""##INSERISCI Il Tuo Hugging Face Token nella cellula rettangolare e premi Invio sulla tastiera.

# Importazione di Classi Haystack: ğŸŒ¾ğŸ“˜

Stiamo aggiungendo le carte vincenti al nostro mazzo! Nel codice seguente:

1. **Import di `PreProcessor`:** Con `from haystack.nodes import PreProcessor`, otteniamo funzionalitÃ  di pre-elaborazione per preparare i nostri dati. [Preprocessor](https://docs.haystack.deepset.ai/docs/preprocessor) ğŸ› ï¸ğŸ“

2. **Import di `PromptModel`:** `from haystack.nodes import PromptModel` ci dÃ  accesso alle funzioni del modello di prompt. Pronti per guidare il nostro modello! ğŸš—ğŸ¤–

3. **Import di `PromptTemplate`:** `from haystack.nodes import PromptTemplate` ci fornisce la flessibilitÃ  di definire modelli di prompt personalizzati. Stiamo dando forma al nostro stile! ğŸ¨ğŸ“„

Siamo pronti a combinare queste classi per creare un flusso di lavoro potente con Haystack! ğŸš€ğŸŒ¾
"""

from haystack.nodes import PreProcessor,PromptModel, PromptTemplate, PromptNode

"""# Avviso Importante: Scarica la Brochure! ğŸ“„ğŸš¨

Per ottenere u dettagli e informazioni sulla nostra iniziativa, ti invitiamo a scaricare la brochure ufficiale.

ğŸ“¥ [Scarica la Brochure Qui](https://drive.google.com/file/d/1tOs9VPiGrcgyEd8U4K2m-Ipx4xY4DPEl/view?usp=sharing)

La brochure contiene dettagli importanti sul Master, le sue iniziative e come puoi unirti a noi per esplorare il mondo della Generative AI.
La Brochure Ã© in Pdf e sarÃ¡ la Base per istruire il nostro ChatBot. Buona lettura! ğŸŒğŸ“˜

# Caricamento della Brochure "Gen-AI Lab": ğŸ“„ğŸš€

Ecco come far entrare la nostra brochure nel Gen-AI Lab! Nel codice seguente:

1. **Import da Google Colab:** Utilizziamo `from google.colab import files` per sfruttare le funzionalitÃ  di caricamento di file di Colab. ğŸŒ

2. **Caricamento della Brochure:** Clicca sul pulsante "Scegli Archivio" e seleziona il percorso dove hai precedentemente scaricato la brochure. Il tuo prezioso documento PDF Ã¨ pronto per il nostro Laboratorio! ğŸ“ğŸ’»

   - **Specifica Chiara:** Assicurati di indicare agli utenti di cliccare sul pulsante "Scegli Archivio" per selezionare il percorso del file PDF della brochure. ğŸ–±ï¸ğŸ“„

Questo passo guida gli utenti attraverso il processo, assicurandoci che la brochure giunga al Gen-AI Lab in modo impeccabile! ğŸŒˆâœ¨
"""

from google.colab import files
files.upload()

"""# Installazione di PyPDF2: ğŸ“„ğŸ”§

Prepariamoci ad esplorare i segreti dei PDF! Nel codice seguente:

1. **Installazione di PyPDF2:** Con `!pip install PyPDF2`, stiamo aggiungendo la libreria PyPDF2 al nostro ambiente. Siamo pronti a manipolare i nostri documenti PDF in grande stile! ğŸš€ğŸ’¼

   - **Attenzione alla Sintassi:** Assicuriamoci di utilizzare la sintassi corretta per installare la libreria. ğŸ”âš™ï¸

Questo Ã¨ il passo iniziale per sbloccare il potenziale di manipolazione dei PDF nel nostro progetto! ğŸŒâœ¨

"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install PyPDF2
#

"""# Estrazione del Testo da un PDF e Creazione di un Documento Haystack: ğŸ“„ğŸš€

Stiamo per immergerci nel testo della nostra preziosa brochure del Gen-Ai LAB e creare un documento Haystack! Nel codice seguente:

1. **Import di PyPDF2:** Con `import PyPDF2`, stiamo portando la potenza di PyPDF2 per manipolare i nostri documenti PDF. ğŸ“„ğŸ”

2. **Definizione del Percorso del PDF:** Specifica il percorso del tuo file PDF nel tuo ambiente con `pdf_file_path`. ğŸ”—ğŸ“‚

3. **Funzione di Estrazione del Testo:** Con `extract_text_from_pdf`, creiamo una funzione per estrarre il testo dal PDF. Stiamo per svelare i segreti del documento! ğŸ•µï¸â€â™‚ï¸ğŸ’¬

4. **Loop sulle Pagine del PDF:** Attraverso un ciclo su tutte le pagine del PDF, estraiamo il testo da ciascuna pagina. Il nostro documento sta prendendo forma! ğŸ”„ğŸŒ

5. **Creazione del Documento Haystack:** Con il testo estratto, creiamo un documento Haystack con `Document`. Siamo pronti per esplorare il nostro testo nel mondo di Haystack! ğŸš€ğŸ“„

Questo Ã¨ il nostro biglietto d'ingresso per esplorare il contenuto della brochure nel nostro progetto! ğŸŒˆâœ¨

"""

import PyPDF2
from haystack import Document

pdf_file_path = "Gen-Ai LAB Brochure.pdf"  # Sostituisci con il percorso del tuo file PDF

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text()

    return text

pdf_text = extract_text_from_pdf(pdf_file_path)

# Creazione del documento di Haystack
doc = Document(
    content=pdf_text,
    meta={"pdf_path": pdf_file_path}
)

"""# Creazione di una Lista di Documenti e Pre-elaborazione: ğŸ“„ğŸš€

Stiamo portando il nostro documento nella fase di pre-elaborazione! Nel codice seguente:

1. **Creazione di una Lista di Documenti:** Con `docs = [doc]`, stiamo creando una lista contenente il nostro singolo documento. Il nostro prezioso contenuto Ã¨ ora racchiuso in una lista! ğŸ“‹ğŸ’¼

2. **Inizializzazione del PreProcessor:** Con `processor = PreProcessor(...)`, stiamo configurando il nostro pre-elaboratore per preparare i documenti al meglio. ğŸ› ï¸âœ¨

   - **Parametri Chiave:**
     - `clean_empty_lines`: Rimuoviamo le linee vuote per una pulizia ottimale. ğŸ§¹
     - `clean_whitespace`: Eliminiamo gli spazi vuoti per una presentazione impeccabile. ğŸš€
     - `clean_header_footer`: Trattiamo gli eventuali elementi di intestazione e piÃ¨ di pagina. ğŸ“‘
     - `split_by`: Suddividiamo il testo per parola. ğŸ“Š
     - `split_length`: Impostiamo la lunghezza di suddivisione a 500 parole. ğŸ“
     - `split_respect_sentence_boundary`: Rispettiamo i limiti delle frasi durante la suddivisione. ğŸ—£ï¸
     - `split_overlap`: Nessun sovrapposizione durante la suddivisione. ğŸš«
     - `language`: Specifichiamo la lingua italiana. ğŸ‡®ğŸ‡¹

3. **Processamento della Lista di Documenti:** Con `preprocessed_docs = processor.process(docs)`, stiamo applicando il nostro pre-elaboratore alla lista di documenti. I documenti stanno per essere ottimizzati e pronti per la fase successiva! ğŸ”„ğŸš€

Questo Ã¨ il nostro passo per garantire che i nostri documenti siano nel miglior stato possibile per le successive fasi del nostro progetto! ğŸŒˆâœ¨

"""

# Crea una lista contenente il tuo singolo documento
docs = [doc]

processor = PreProcessor(
    clean_empty_lines=True,
    clean_whitespace=True,
    clean_header_footer=True,
    split_by="word",
    split_length=500,
    split_respect_sentence_boundary=True,
    split_overlap=0,
    language="it",
)

# Processa la lista di documenti
preprocessed_docs = processor.process(docs)

# a smaller chunked document

preprocessed_docs[10]

"""# Creazione di uno Store di Documenti in Memoria: ğŸ“šğŸ’¾

Stiamo archiviando i nostri documenti in un luogo facilmente accessibile! Nel codice seguente:

1. **Inizializzazione di InMemoryDocumentStore:** Con `document_store = InMemoryDocumentStore(use_bm25=True)`, stiamo creando uno store di documenti in memoria. La potenza di memorizzazione Ã¨ ora nelle nostre mani! ğŸ’¡ğŸ”

   - **Parametro Chiave:**
     - `use_bm25`: Abilitiamo BM25, un algoritmo di ranking per la ricerca, per migliorare la precisione delle query. ğŸ“ŠğŸ”—

2. **Scrittura dei Documenti nello Store:** Con `document_store.write_documents(preprocessed_docs)`, stiamo scrivendo i documenti pre-elaborati nello store. I nostri documenti sono ora pronti per essere recuperati in qualsiasi momento! ğŸ“ğŸ’¼

Questo Ã¨ il nostro magazzino virtuale pronto a custodire i segreti del nostro progetto! ğŸš€âœ¨


"""

from haystack.document_stores import InMemoryDocumentStore

document_store = InMemoryDocumentStore(use_bm25=True)
document_store.write_documents(preprocessed_docs)

"""# Configurazione di un Recuperatore BM25 con Haystack: ğŸ”ğŸš€

Stiamo potenziando la nostra ricerca con il retriver BM25! Nel codice seguente:

1. **Configurazione di BM25Retriever:** Con `retriever = BM25Retriever(document_store, top_k=2)`, stiamo creando un retriver BM25 che si basa sul nostro store di documenti in memoria. La ricerca ora Ã¨ pronta a decollare con una top-k di 2 risultati! [BM25 Retriever](https://docs.haystack.deepset.ai/docs/retriever#bm25-recommended) ğŸ“ˆğŸ”—

   - **Parametri Chiave:**
     - `document_store`: Utilizziamo il nostro store di documenti in memoria come base per la ricerca. ğŸ“šğŸ’¾
     - `top_k`: Specifichiamo che vogliamo ottenere i migliori 2 risultati per ciascuna query. ğŸ”2ï¸âƒ£

Questo retriver Ã¨ il nostro alleato per trovare i documenti piÃ¹ rilevanti nelle fasi successive! ğŸŒâœ¨

"""

from haystack import Pipeline
from haystack.nodes import BM25Retriever
retriever = BM25Retriever(document_store, top_k=2)

"""# Definizione di un PromptTemplate per le Domande e Risposte: â“ğŸ’¬

Stiamo dando una struttura alle nostre interazioni domanda-risposta con un `PromptTemplate`! Nel codice seguente:

1. **Definizione del Template:** Con `qa_template = PromptTemplate(...)`, stiamo creando un template che guida il processo di domanda e risposta. ğŸ“ğŸ’¬

   - **Struttura del Prompt:**
     - "Usando le informazioni contenute nel contesto, rispondi soltanto alla domanda posta senza aggiungere suggestioni di domande e rispondi esclusivamente in italiano."
     - "Se la risposta non puÃ² essere dedotta dal contesto, rispondi: '\ Non so\'."
     - "Context: {join(documents)};"
     - "Question: {query}"

   - **Descrizione Chiara:** Il template fornisce chiare istruzioni su come rispondere alle domande in base al contesto. ğŸ§­ğŸ’¡

2. **Variabili del Template:** Utilizziamo variabili come `{join(documents)}` e `{query}` per incorporare dinamicamente le informazioni necessarie nel prompt. Le nostre domande ora sono guidate dal contesto! ğŸ”„ğŸŒ

Questo template Ã¨ la chiave per interazioni strutturate e contestualmente informate! ğŸš€âœ¨

"""

qa_template = PromptTemplate(prompt=
  """ Usando esclusivamente le informazioni contenute nel contesto,
  rispondi soltanto alla domanda posta senza aggiungere suggestioni di domande possibili e rispondi esclusivamente in italiano.
  Se la risposta non puÃ² essere dedotta dal contesto, rispondi: "\ Non saprei perchÃ© non attinente al Contesto.\"
  Context: {join(documents)};
  Question: {query}
  """)

"""# Utilizzo di http_client per Richieste HTTP e Inizializzazione di PromptNode: ğŸŒğŸ¤–

Stiamo esplorando il mondo delle richieste HTTP e inizializziamo il nostro [PromptNode](https://docs.haystack.deepset.ai/docs/prompt_node)! Nel codice seguente:

1. **Inizializzazione di PromptNode:** Con `prompt_node = PromptNode(...)`, stiamo configurando il nostro nodo di prompt per interagire con il modello Zephyr. ğŸŒ¬ï¸ğŸš€

   - **Modello Zephyr:** Specificando `model_name_or_path="HuggingFaceH4/zephyr-7b-beta"`, indichiamo quale modello utilizzare per le risposte alle nostre domande. ğŸ¤–ğŸ’¬

   - **Chiave API Hugging Face:** Utilizziamo la tua chiave Hugging Face, ottenuta in modo sicuro prima, con `api_key=HF_TOKEN`. La sicurezza Ã¨ fondamentale! ğŸ”ğŸŒ

   - **Template di Prompt Predefinito:** Configuriamo il template di prompt predefinito con `default_prompt_template=qa_template`. Ora il nostro nodo di prompt Ã¨ pronto a ricevere e rispondere alle domande in modo strutturato! ğŸ“„ğŸ’¡

   - **Parametri Aggiuntivi del Modello:** Specificando `max_length=500` e `model_kwargs={"model_max_length": 5000}`, stiamo impostando limiti sulla lunghezza del testo per garantire un'interazione ottimale con il modello. ğŸ“ğŸš€

2. **Utilizzo di http_client:** Con l'uso di `http_client` per le richieste HTTP, stiamo aprendo la porta alla comunicazione bidirezionale con il modello Zephyr. La magia delle richieste online Ã¨ ora nelle nostre mani! ğŸŒâœ¨

Questo nodo di prompt Ã¨ la nostra interfaccia diretta con il mondo di Zephyr, pronto a rispondere alle domande in modo intelligente! ğŸ’¬ğŸŒŸ

"""

# Ora puoi utilizzare http_client per le tue richieste HTTP, compresa l'inizializzazione di PromptNode
prompt_node = PromptNode(
    model_name_or_path="mistralai/Mixtral-8x7B-Instruct-v0.1",
    api_key=HF_TOKEN,
    default_prompt_template=qa_template,
    max_length=500,
    model_kwargs={"model_max_length": 5000}
)

"""# Configurazione di una Pipeline con Haystack: ğŸš€ğŸ”—

Stiamo creando una sequenza di passaggi per massimizzare l'efficacia del nostro flusso di lavoro! Nel codice seguente:

1. **Creazione di una Pipeline RAG:** Con `rag_pipeline = Pipeline()`, stiamo iniziando la costruzione della nostra pipeline per il Recurrent Retrieval Augmented Generation (RAG). ğŸ”„ğŸš€

2. **Aggiunta di un Nodo Retriever:** Utilizzando `rag_pipeline.add_node(...)`, stiamo incorporando il nostro retriver BM25. Il nodo "retriever" riceverÃ  input dalla "Query". La ricerca Ã¨ ufficialmente in corso! ğŸ”ğŸ”—

   - **Nodo Retriever:** Configuriamo il nostro retriver BM25 precedentemente creato. Ãˆ il nostro esperto di ricerca pronto a fornire i documenti piÃ¹ rilevanti! ğŸ“šğŸ’¼

3. **Aggiunta di un Nodo di Prompt:** Con `rag_pipeline.add_node(...)`, stiamo inserendo il nostro nodo di prompt. Il nodo "prompt_node" riceverÃ  input dal nodo "retriever". La risposta Ã¨ ora alla portata delle nostre domande! ğŸ’¬ğŸŒ

   - **Nodo di Prompt:** Configuriamo il nodo di prompt per interagire con il modello Zephyr. L'interazione tra il retriver e Zephyr Ã¨ pronta a scatenarsi! ğŸŒ¬ï¸ğŸ’¡

Questa Pipeline Ã¨ la nostra mappa per navigare tra la ricerca e la generazione intelligente di risposte! ğŸ—ºï¸âœ¨

"""

rag_pipeline = Pipeline()
rag_pipeline.add_node(component=retriever, name="retriever", inputs=["Query"])
rag_pipeline.add_node(component=prompt_node, name="prompt_node", inputs=["retriever"])

"""# Utilizzo di pprint per la Stampa Dettagliata delle Risposte: ğŸ“„ğŸ‘€

Stiamo migliorando la presentazione delle nostre risposte con `pprint` e una funzione dedicata! Nel codice seguente:

1. **Import di pprint:** Con `from pprint import pprint`, introduciamo la potenza di pprint per una stampa dettagliata dei risultati. Ora le nostre risposte saranno presentate in modo chiaro e ordinato! ğŸ“‘ğŸ‘€

2. **Definizione di una Funzione di Stampa Risposta:** Con `print_answer = lambda out: pprint(out["results"][0].strip())`, stiamo creando una funzione lambda per stampare in modo dettagliato la risposta principale dal risultato. La nostra presentazione delle risposte Ã¨ ora piÃ¹ elegante! ğŸŒŸğŸ’¬

   - **Parametro di Input:** La funzione `print_answer` richiede un parametro `out` che dovrebbe essere una struttura dati contenente i risultati della nostra pipeline.

3. **Stampa Elegante della Risposta:** Con la funzione `print_answer`, possiamo ora ottenere una stampa elegante delle risposte, grazie alla bellezza di `pprint`! ğŸ–¨ï¸ğŸŒˆ

Questo Ã¨ il tocco finale per rendere le nostre risposte non solo informative, ma anche visivamente accattivanti! âœ¨ğŸ‘ï¸


"""

from pprint import pprint
print_answer = lambda out: pprint(out["results"][0].strip())

"""## Let's try our RAG Pipeline ğŸ¸
Finalmente esecuzione della Pipeline RAG con una Query e Stampa della Risposta: ğŸ”ğŸš€

Stiamo mettendo alla prova la nostra pipeline RAG con una domanda e ammirando la risposta! Nel codice seguente:

1. **Esecuzione della Pipeline RAG:** Con `rag_pipeline.run(query="Quale Ã© la caratteristica del Master?")`, stiamo invocando la nostra pipeline RAG con una domanda specifica. Il processo di ricerca e generazione Ã¨ ufficialmente in corso! ğŸ”ğŸŒ

   - **Query di Esempio:** "Quale Ã¨ la caratteristica del Master?"

2. **Stampa della Risposta:** Utilizzando la funzione `print_answer`, stiamo ottenendo una stampa dettagliata della risposta principale. La nostra risposta Ã¨ pronta a stupire! ğŸ“„ğŸ‘€

   - **Notazione Lambda:** Abbiamo definito in precedenza una funzione lambda `print_answer` per presentare in modo dettagliato i risultati.

Che la magia della ricerca e generazione delle risposte inizi! ğŸŒŸğŸ’¬


"""

print_answer(rag_pipeline.run(query="Quale Ã© la caratteristica del Master?"))

print_answer(rag_pipeline.run(query="Pensi che i progetti pratici siano validi per una formazione professionale?"))

print_answer(rag_pipeline.run(query="In che modulo tratteremo i chatbots?"))

print_answer(rag_pipeline.run(query="descrivi i contenuti di tutti i laboratori pratici di ciascun modulo in una tabella."))

"""## Prepariamo una serie di domande!

"""

questions="""I moduli sono aggiornati con le ultime tendenze della AI?
Conosci altri Corsi che sono piÃº validi?
Il professore ha un curriculum valido?
Per chiarire i miei dubbi chi posso contattare?
Le lezioni sono in modalitÃ¡ sincrona?
EÂ´previsto uno stage?
Durante il Master Ã¨ prevista la preparazione per il conseguimento di certificati?""".split("\n")

for q in questions:
  print("\n"+q)
  print(rag_pipeline.run(query=q)["results"][0].strip())

"""### Ora prova tu stesso a fare delle domande al nostro Assistente AI...."""

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

rag_pipeline.run(query="**************************")["results"][0].strip()

"""# Modello [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1): Un Aiutante Utile, ma Perfettibile! ğŸŒğŸ¤–

Il nostro chatbot, alimentato dal modello Mixtral-8x7B, Ã¨ pronto a offrire assistenza, ma Ã¨ importante tenere presente che nessun modello Ã¨ perfetto. Il percorso verso l'eccellenza continua e il nostro Mixtral-8x7B Ã¨ pronto a crescere! ğŸ’¡ğŸª



"""