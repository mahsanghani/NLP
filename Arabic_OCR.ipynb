{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is0mDGfImQh6"
      },
      "source": [
        "Usage:<br>\n",
        "1- goto the dataset repository:<br>\n",
        "https://drive.google.com/drive/u/2/folders/1mRefmN4Yzy60Uh7z3B6cllyyOXaxQrgg\n",
        " \n",
        "and select one of the datasets e.g. 1_nice_60000_rows\n",
        "\n",
        "2- download the related files, for example:<br>\n",
        "a- 1_nice_60000_rows.bin<br>\n",
        "and<br>\n",
        "b- 1_nice_60000_rows.txt<br><br>\n",
        "\n",
        "2- upload the dataset files you downloaded from the dataset repository into your Google Colab drive.\n",
        "if you uploaded the files into Colab drive, they will be deleted once the session is over. Hence, you can upload the downloaded dataset files into your own google drive and mount your google drive in Colab.\n",
        "\n",
        "4- Set the path for the dataset files in config.py below i.e. whether in Colab drive or your own google drive e.g. /content/drive/MyDrive/\n",
        "\n",
        "5- initially, we need to train the model, so we set the \n",
        "**OPERATION_TYPE = OperationType.Training** value in config.py below to Training.\n",
        "\n",
        "6- If your are using the dataset for the first time, make sure to empty your output folder from any previously generated files (dataset split files and model files...etc).\n",
        "\n",
        "7- Set the experiment name via the experiment_name constant below (if required)\n",
        "\n",
        "8- Run the training process, the training process will stop after 5 epoches with no improvements, you can change this value in config.py below.\n",
        "\n",
        "9- After finishing the training process, you can test the model on the testing dataset by setting the **OPERATION_TYPE = OperationType.Testing**\n",
        "\n",
        "10- Finally, you can make inference on single images by setting **OPERATION_TYPE = OperationType.Infer**\n",
        "To make inference, you need to place the required word image file in the designated folder defined by the INDIVIDUAL_TEST_IMAGE_PATH constant value below.\n",
        "You also need to set the filename for the image you are trying to infer in the fnInfer variable below e.g. 0.png.\n",
        "\n",
        "You can find sample files in the dataset repository in Google drive as well as the dataset folder in GitHub (under the sample_dataset_imgages folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOA2Pgbk238L"
      },
      "source": [
        "Config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Z0Topi238M",
        "outputId": "10d4fcc8-6d91-448d-ec54-d3872b759d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/ArabicMultiFontsDataset/'\n",
            "/content\n",
            "1_nice_60000_rows.bin  1_nice_60000_rows.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ArabicMultiFontsDataset/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqwen4i13iEc"
      },
      "outputs": [],
      "source": [
        "# The location/path of the uploaded dataset files (after downloading them from the dataset repository)\n",
        "# Make sure to mount your Google drive in Colab\n",
        "BASE_PATH = \"/content/\"\n",
        "\n",
        "# The name of the dataset files i.e. the binary file and the labels file.\n",
        "BASE_FILENAME = \"1_nice_60000_rows\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyjcmAyrusoI"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import os\n",
        "from enum import Enum\n",
        "import numpy as np\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import time\n",
        "import cv2\n",
        "import random\n",
        "import sys\n",
        "from shutil import Error\n",
        "import tensorflow as tf\n",
        "import editdistance\n",
        "\n",
        "class OperationType(Enum):\n",
        "    Training = 1\n",
        "    Validation = 2\n",
        "    Testing = 3\n",
        "    Infer = 4\n",
        "\n",
        "\n",
        "class DecoderType:\n",
        "    BestPath = 0\n",
        "    BeamSearch = 1\n",
        "    WordBeamSearch = 2\n",
        "\n",
        "\n",
        "# Experiment name, to be saved in the audit log.\n",
        "EXPERIMENT_NAME = \"Test Drive Training Process\"\n",
        "\n",
        "# The type of the run session, depending on this type, designated datasets will be loaded.\n",
        "# Set this value to training to run a training process using the testing dataset.\n",
        "# Set this value to testing to run a testing process using the testing dataset.\n",
        "# Set this value to infer, to make an inference for a single word image file, make sure\n",
        "# to place the required word image file in the directory defined by the INDIVIDUAL_TEST_IMAGE_PATH below, and\n",
        "# make sure to set the fnInfer to the name of the image file your are trying to infer\n",
        "OPERATION_TYPE = OperationType.Training\n",
        "\n",
        "DECODER_TYPE = DecoderType.BestPath\n",
        "\n",
        "# Use this value to regenerate the training/validation/test datasets, as well as\n",
        "# the other support files. Usually this is needed when we start the training process\n",
        "# It is not needed during the Testing process so we set it to true\n",
        "# in order to regenerate all the required files, we have to delete to old ones train/validate/test and delete\n",
        "# and then its value to true. After running the app, the files are generated and we can set it back to false unless\n",
        "# we need to generate a new set of data i.e. train/validate/test\n",
        "REGENERATE_CHARLIST_AND_CORPUS = True\n",
        "\n",
        "\n",
        "#You can modify these folder settings according to your preference\n",
        "\n",
        "#Set the path where the train, validate,test datasets are saved\n",
        "DATA_PATH = BASE_PATH \n",
        "#set the path where to save the generated tensorflow model\n",
        "MODEL_PATH = BASE_PATH\n",
        "#set the path for the autogenerated files\n",
        "OUTPUT_PATH = BASE_PATH\n",
        "\n",
        "#Set the path of the single image files that you want to recognize\n",
        "INDIVIDUAL_TEST_IMAGE_PATH = DATA_PATH\n",
        "\n",
        "BASE_IMAGES_FILE = DATA_PATH + BASE_FILENAME + \".bin\"\n",
        "BASE_LABELS_FILE = DATA_PATH + BASE_FILENAME + \".txt\"\n",
        "TRAINING_LABELS_FILE = DATA_PATH + \"TRAINING_DATA_\" + BASE_FILENAME + \".txt\"\n",
        "VALIDATION_LABELS_FILE = DATA_PATH + \\\n",
        "    \"VALIDATION_DATA_\" + BASE_FILENAME + \".txt\"\n",
        "TESTING_LABELS_FILE = DATA_PATH + \"TESTING_DATA_\" + BASE_FILENAME + \".txt\"\n",
        "fnCharList = OUTPUT_PATH + 'charList.txt'\n",
        "fnResult = OUTPUT_PATH + 'result.txt'\n",
        "\n",
        "# define the name of the word image file you want to test/infer\n",
        "# for example, you can select 0.png file from as a sample\n",
        "#https://github.com/msfasha/Arabic-Deep-Learning-OCR/tree/master/dataset/sample_files\n",
        "fnInfer = INDIVIDUAL_TEST_IMAGE_PATH + \"sample_files/\" + \"0.png\"\n",
        "\n",
        "\n",
        "fnCorpus = OUTPUT_PATH + 'corpus.txt'\n",
        "fnwordCharList = OUTPUT_PATH + 'wordCharList.txt'\n",
        "\n",
        "# Number of batches for each epoch = SAMPLES_PER_EPOCH / BATCH_SIZE\n",
        "TRAINING_SAMPLES_PER_EPOCH = 5000\n",
        "BATCH_SIZE = 100\n",
        "VALIDATIOIN_SAMPLES_PER_STEP = (int)(TRAINING_SAMPLES_PER_EPOCH * .2)\n",
        "\n",
        "\n",
        "TRAINING_DATASET_SIZE = .9\n",
        "# .5 of the remaining ==> (Total - TRAINING_DATASET_SIZE) / 2\n",
        "VALIDATION_DATASET_SPLIT_SIZE = .5\n",
        "# stop after no improvements for this number of epochs\n",
        "MAXIMUM_NONIMPROVED_EPOCHS = 5\n",
        "MAXIMUM_MODELS_TO_KEEP = 3  # usually only 1, the last one\n",
        "\n",
        "#IMAGE_SIZE = (128, 32)\n",
        "IMAGE_WIDTH = 128\n",
        "IMAGE_HEIGHT = 32\n",
        "MAX_TEXT_LENGTH = 32\n",
        "RESIZE_IMAGE = True\n",
        "CONVERT_IMAGE_TO_MONOCHROME = False\n",
        "MONOCHROME_BINARY_THRESHOLD = 127\n",
        "AUGMENT_IMAGE = False\n",
        "\n",
        "def auditLog(logStr):\n",
        "    open(fnResult, 'a').write(logStr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBKs5s3SmU4P"
      },
      "source": [
        "Model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfu99pHXl88m"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    \"minimalistic TF model for HTR\"\n",
        "\n",
        "    def __init__(self, decoderType = DecoderType.BestPath, mustRestore=False, dump=False):\n",
        "        \"init model: add CNN, RNN and CTC and initialize TF\"\n",
        "        self.dump = dump\n",
        "        self.charList = open(fnCharList, encoding=\"utf-8\").read()\n",
        "        self.decoderType = decoderType\n",
        "        self.mustRestore = mustRestore\n",
        "        self.snapID = 0\n",
        "\n",
        "        # Whether to use normalization over a batch or a population\n",
        "        self.is_train = tf.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "        # input image batch\n",
        "        self.inputImgs = tf.placeholder(tf.float32, shape=(\n",
        "            None, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "\n",
        "        # setup CNN, RNN and CTC\n",
        "        self.setup5LayersCNN()\n",
        "        self.setupRNN()\n",
        "        self.setupCTC()\n",
        "\n",
        "        # setup optimizer to train NN\n",
        "        self.batchesTrained = 0\n",
        "        self.learningRate = tf.placeholder(tf.float32, shape=[])\n",
        "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(self.update_ops):\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(\n",
        "                self.learningRate).minimize(self.loss)\n",
        "\n",
        "        self.auditModelDetails()\n",
        "        # initialize TF\n",
        "        (self.sess, self.saver) = self.setupTF()\n",
        "\n",
        "    def auditModelDetails(self):\n",
        "        total_parameters = 0\n",
        "        saveString = \"Model Details\" + \"\\n\"\n",
        "        for variable in tf.trainable_variables():\n",
        "            # shape is an array of tf.Dimension\n",
        "            shape = variable.get_shape()\n",
        "            saveString = saveString + \"Shape:\" + \\\n",
        "                str(shape) + \" ,shape length:\" + str(len(shape))\n",
        "            variable_parameters = 1\n",
        "            for dim in shape:\n",
        "                variable_parameters *= dim.value\n",
        "            saveString = saveString + \" , parameters: \" + \\\n",
        "                str(variable_parameters) + \"\\n\"\n",
        "            total_parameters += variable_parameters\n",
        "\n",
        "        saveString = saveString + \"Total Parameters: \" + \\\n",
        "            str(total_parameters) + \"\\n\\n\"\n",
        "\n",
        "        print(saveString)\n",
        "        auditLog(saveString)\n",
        "\n",
        "    def setup5LayersCNN(self):\n",
        "        \"create CNN layers and return output of these layers\"\n",
        "        cnnIn4d = tf.expand_dims(input=self.inputImgs, axis=3)\n",
        "        pool = cnnIn4d  # input to first CNN layer\n",
        "\n",
        "        self.kernel1 = tf.Variable(\n",
        "            tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
        "        self.conv1 = tf.nn.conv2d(\n",
        "            pool, self.kernel1, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        conv_norm = tf.layers.batch_normalization(\n",
        "            self.conv1, training=self.is_train)\n",
        "        self.relu1 = tf.nn.relu(conv_norm)\n",
        "        self.pool1 = tf.nn.max_pool(\n",
        "            self.relu1, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')\n",
        "\n",
        "        kernel = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
        "        self.conv2 = tf.nn.conv2d(\n",
        "            self.pool1, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        conv_norm = tf.layers.batch_normalization(\n",
        "            self.conv2, training=self.is_train)\n",
        "        relu = tf.nn.relu(conv_norm)\n",
        "        pool = tf.nn.max_pool(relu, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')\n",
        "\n",
        "        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev=0.1))\n",
        "        self.conv3 = tf.nn.conv2d(\n",
        "            pool, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        conv_norm = tf.layers.batch_normalization(\n",
        "            self.conv3, training=self.is_train)\n",
        "        relu = tf.nn.relu(conv_norm)\n",
        "        pool = tf.nn.max_pool(relu, (1, 1, 2, 1), (1, 1, 2, 1), 'VALID')\n",
        "\n",
        "        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], stddev=0.1))\n",
        "        self.conv4 = tf.nn.conv2d(\n",
        "            pool, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        conv_norm = tf.layers.batch_normalization(\n",
        "            self.conv4, training=self.is_train)\n",
        "        relu = tf.nn.relu(conv_norm)\n",
        "        pool = tf.nn.max_pool(relu, (1, 1, 2, 1), (1, 1, 2, 1), 'VALID')\n",
        "\n",
        "        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], stddev=0.1))\n",
        "        self.conv5 = tf.nn.conv2d(\n",
        "            pool, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        conv_norm = tf.layers.batch_normalization(\n",
        "            self.conv5, training=self.is_train)\n",
        "        relu = tf.nn.relu(conv_norm)\n",
        "        pool = tf.nn.max_pool(relu, (1, 1, 2, 1), (1, 1, 2, 1), 'VALID')\n",
        "\n",
        "        self.cnnOut4d = pool\n",
        "\n",
        "    def setupCNN7Layers(self):\n",
        "        \"create CNN layers and return output of these layers\"\n",
        "        cnnIn4d = tf.expand_dims(input=self.inputImgs, axis=3)\n",
        "\n",
        "        kernel1 = tf.Variable(tf.truncated_normal([3, 3, 1, 64], stddev=0.1))\n",
        "        conv1 = tf.nn.conv2d(\n",
        "            cnnIn4d, kernel1, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        pool1 = tf.nn.max_pool(conv1, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')\n",
        "\n",
        "        kernel2 = tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev=0.1))\n",
        "        conv2 = tf.nn.conv2d(\n",
        "            pool1, kernel2, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        pool2 = tf.nn.max_pool(conv2, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')\n",
        "\n",
        "        kernel3 = tf.Variable(tf.truncated_normal(\n",
        "            [3, 3, 128, 256], stddev=0.1))\n",
        "        conv3 = tf.nn.conv2d(\n",
        "            pool2, kernel3, padding='SAME', strides=(1, 1, 1, 1))\n",
        "\n",
        "        kernel4 = tf.Variable(tf.truncated_normal(\n",
        "            [3, 3, 256, 256], stddev=0.1))\n",
        "        conv4 = tf.nn.conv2d(\n",
        "            conv3, kernel4, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        pool3 = tf.nn.max_pool(conv4, (1, 1, 2, 1), (1, 1, 2, 1), 'VALID')\n",
        "\n",
        "        kernel5 = tf.Variable(tf.truncated_normal(\n",
        "            [3, 3, 256, 512], stddev=0.1))\n",
        "        conv5 = tf.nn.conv2d(\n",
        "            pool3, kernel5, padding='SAME', strides=(1, 1, 1, 1))\n",
        "        batch_norm1 = tf.layers.batch_normalization(\n",
        "            conv4, training=self.is_train)\n",
        "\n",
        "        kernel6 = tf.Variable(tf.truncated_normal(\n",
        "            [3, 3, 512, 512], stddev=0.1))\n",
        "        conv6 = tf.nn.conv2d(batch_norm1, kernel6,\n",
        "                             padding='SAME', strides=(1, 1, 1, 1))\n",
        "        batch_norm2 = tf.layers.batch_normalization(\n",
        "            conv6, training=self.is_train)\n",
        "        pool4 = tf.nn.max_pool(batch_norm2, (1, 1, 2, 1),\n",
        "                               (1, 1, 2, 1), 'VALID')\n",
        "\n",
        "        kernel7 = tf.Variable(tf.truncated_normal(\n",
        "            [2, 2, 512, 512], stddev=0.1))\n",
        "        conv7 = tf.nn.conv2d(batch_norm1, kernel7,\n",
        "                             padding='SAME', strides=(1, 1, 1, 1))\n",
        "\n",
        "        self.cnnOut4d = conv7\n",
        "\n",
        "    def setupRNN(self):\n",
        "        \"create RNN layers and return output of these layers\"\n",
        "        rnnIn3d = tf.squeeze(self.cnnOut4d, axis=[2])\n",
        "\n",
        "        # basic cells which is used to build RNN\n",
        "        numHidden = 256\n",
        "        cells = [tf.contrib.rnn.LSTMCell(\n",
        "            num_units=numHidden, state_is_tuple=True) for _ in range(2)]  # 2 layers\n",
        "\n",
        "        # stack basic cells\n",
        "        stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "        # bidirectional RNN\n",
        "        # BxTxF -> BxTx2H\n",
        "        ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d,\n",
        "                                                        dtype=rnnIn3d.dtype)\n",
        "\n",
        "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
        "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
        "\n",
        "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
        "        kernel = tf.Variable(tf.truncated_normal(\n",
        "            [1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n",
        "        self.rnnOut3d = tf.squeeze(tf.nn.atrous_conv2d(\n",
        "            value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
        "\n",
        "    def setupCTC(self):\n",
        "        \"create CTC loss and decoder and return them\"\n",
        "        # BxTxC -> TxBxC\n",
        "        self.ctcIn3dTBC = tf.transpose(self.rnnOut3d, [1, 0, 2])\n",
        "\n",
        "        # ground truth text as sparse tensor\n",
        "        self.gtTexts = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]), tf.placeholder(tf.int32, [None]),\n",
        "                                       tf.placeholder(tf.int64, [2]))\n",
        "\n",
        "        # calc loss for batch\n",
        "        self.seqLen = tf.placeholder(tf.int32, [None])\n",
        "        self.loss = tf.reduce_mean(\n",
        "            tf.nn.ctc_loss(labels=self.gtTexts, inputs=self.ctcIn3dTBC, sequence_length=self.seqLen,\n",
        "                           ctc_merge_repeated=True))\n",
        "\n",
        "        # calc loss for each element to compute label probability\n",
        "        self.savedCtcInput = tf.placeholder(\n",
        "            tf.float32, shape=[MAX_TEXT_LENGTH, None, len(self.charList) + 1])\n",
        "        self.lossPerElement = tf.nn.ctc_loss(labels=self.gtTexts, inputs=self.savedCtcInput,\n",
        "                                             sequence_length=self.seqLen, ctc_merge_repeated=True)\n",
        "\n",
        "        # decoder: either best path decoding or beam search decoding\n",
        "        if self.decoderType == DecoderType.BestPath:\n",
        "            self.decoder = tf.nn.ctc_greedy_decoder(\n",
        "                inputs=self.ctcIn3dTBC, sequence_length=self.seqLen)\n",
        "        elif self.decoderType == DecoderType.BeamSearch:\n",
        "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctcIn3dTBC, sequence_length=self.seqLen,\n",
        "                                                         beam_width=50, merge_repeated=False)\n",
        "        elif self.decoderType == DecoderType.WordBeamSearch:\n",
        "            # import compiled word beam search operation (see https://github.com/githubharald/CTCWordBeamSearch)\n",
        "            word_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n",
        "\n",
        "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
        "            chars = str().join(self.charList)\n",
        "            wordChars = open(fnwordCharList).read().splitlines()[0]\n",
        "            corpus = open(fnCorpus).read()\n",
        "\n",
        "            # decode using the \"Words\" mode of word beam search\n",
        "            self.decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(self.ctcIn3dTBC, dim=2), 50, 'Words',\n",
        "                                                                    0.0, corpus.encode(\n",
        "                                                                        'utf8'), chars.encode('utf8'),\n",
        "                                                                    wordChars.encode('utf8'))\n",
        "\n",
        "    def setupTF(self):\n",
        "        \"initialize TF\"\n",
        "        print('Python: ' + sys.version)\n",
        "        print('Tensorflow: ' + tf.__version__)\n",
        "\n",
        "        sess = tf.Session()  # TF session\n",
        "\n",
        "        # saver saves model to file\n",
        "        saver = tf.train.Saver(max_to_keep=MAXIMUM_MODELS_TO_KEEP)\n",
        "        modelDir = MODEL_PATH\n",
        "        latestSnapshot = tf.train.latest_checkpoint(\n",
        "            modelDir)  # is there a saved model?\n",
        "\n",
        "        # if model must be restored (for inference), there must be a snapshot\n",
        "        if self.mustRestore and not latestSnapshot:\n",
        "            raise Exception('No saved model found in: ' + modelDir)\n",
        "\n",
        "        # load saved model if available\n",
        "        if latestSnapshot:\n",
        "            print('Init with stored values from ' + latestSnapshot)\n",
        "            saver.restore(sess, latestSnapshot)\n",
        "        else:\n",
        "            print('Init with new values')\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        return (sess, saver)\n",
        "\n",
        "    def toSparse(self, texts):\n",
        "        \"put ground truth texts into sparse tensor for ctc_loss\"\n",
        "        indices = []\n",
        "        values = []\n",
        "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
        "\n",
        "        # go over all texts\n",
        "        for (batchElement, text) in enumerate(texts):\n",
        "            # convert to string of label (i.e. class-ids)\n",
        "            CharactersIndexesOflabels = [self.charList.index(c) for c in text]\n",
        "            # sparse tensor must have size of max. label-string\n",
        "            if len(CharactersIndexesOflabels) > shape[1]:\n",
        "                shape[1] = len(CharactersIndexesOflabels)\n",
        "            # put each label into sparse tensor\n",
        "            for (i, label) in enumerate(CharactersIndexesOflabels):\n",
        "                indices.append([batchElement, i])\n",
        "                values.append(label)\n",
        "\n",
        "        return (indices, values, shape)\n",
        "\n",
        "    def decoderOutputToText(self, ctcOutput, batchSize):\n",
        "        \"extract texts from output of CTC decoder\"\n",
        "\n",
        "        # contains string of labels for each batch element\n",
        "        encodedLabelStrs = [[] for i in range(batchSize)]\n",
        "\n",
        "        # word beam search: label strings terminated by blank\n",
        "        if self.decoderType == DecoderType.WordBeamSearch:\n",
        "            blank = len(self.charList)\n",
        "            for b in range(batchSize):\n",
        "                for label in ctcOutput[b]:\n",
        "                    if label == blank:\n",
        "                        break\n",
        "                    encodedLabelStrs[b].append(label)\n",
        "\n",
        "        # TF decoders: label strings are contained in sparse tensor\n",
        "        else:\n",
        "            # ctc returns tuple, first element is SparseTensor\n",
        "            decoded = ctcOutput[0][0]\n",
        "\n",
        "            # go over all indices and save mapping: batch -> values\n",
        "            idxDict = {b: [] for b in range(batchSize)}\n",
        "            for (idx, idx2d) in enumerate(decoded.indices):\n",
        "                label = decoded.values[idx]\n",
        "                batchElement = idx2d[0]  # index according to [b,t]\n",
        "                encodedLabelStrs[batchElement].append(label)\n",
        "\n",
        "        # map labels to chars for all batch elements\n",
        "        return [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n",
        "\n",
        "    def trainBatch(self, batch):\n",
        "        \"feed a batch into the NN to train it\"\n",
        "        numBatchElements = len(batch.imgs)\n",
        "        sparse = self.toSparse(batch.gtTexts)\n",
        "        rate = 0.01 if self.batchesTrained < 10 else (\n",
        "            0.001 if self.batchesTrained < 10000 else 0.0001)  # decay learning rate\n",
        "        evalList = [self.optimizer, self.loss]\n",
        "        feedDict = {self.inputImgs: batch.imgs, self.gtTexts: sparse,\n",
        "                    self.seqLen: [MAX_TEXT_LENGTH] * numBatchElements, self.learningRate: rate,\n",
        "                    self.is_train: True}\n",
        "\n",
        "        (_, lossVal) = self.sess.run(evalList, feedDict)\n",
        "\n",
        "        self.batchesTrained += 1\n",
        "        return lossVal\n",
        "\n",
        "    def dumpNNOutput(self, rnnOutput):\n",
        "        \"dump the output of the NN to CSV file(s)\"\n",
        "        dumpDir = '../dump/'\n",
        "        if not os.path.isdir(dumpDir):\n",
        "            os.mkdir(dumpDir)\n",
        "\n",
        "        # iterate over all batch elements and create a CSV file for each one\n",
        "        maxT, maxB, maxC = rnnOutput.shape\n",
        "        for b in range(maxB):\n",
        "            csv = ''\n",
        "            for t in range(maxT):\n",
        "                for c in range(maxC):\n",
        "                    csv += str(rnnOutput[t, b, c]) + ';'\n",
        "                csv += '\\n'\n",
        "            fn = dumpDir + 'rnnOutput_' + str(b) + '.csv'\n",
        "            print('Write dump of NN to file: ' + fn)\n",
        "            with open(fn, 'w') as f:\n",
        "                f.write(csv)\n",
        "\n",
        "    def inferBatch(self, batch, calcProbability=False, probabilityOfGT=False):\n",
        "        \"feed a batch into the NN to recognize the texts\"\n",
        "\n",
        "        # decode, optionally save RNN output\n",
        "        numBatchElements = len(batch.imgs)\n",
        "        evalRnnOutput = self.dump or calcProbability\n",
        "        evalList = [self.decoder] + \\\n",
        "            ([self.ctcIn3dTBC] if evalRnnOutput else [])\n",
        "        feedDict = {self.inputImgs: batch.imgs, self.seqLen: [MAX_TEXT_LENGTH] * numBatchElements,\n",
        "                    self.is_train: False}\n",
        "\n",
        "        evalRes = self.sess.run(evalList, feedDict)\n",
        "\n",
        "        decoded = evalRes[0]\n",
        "        texts = self.decoderOutputToText(decoded, numBatchElements)\n",
        "\n",
        "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
        "        probs = None\n",
        "        if calcProbability:\n",
        "            sparse = self.toSparse(\n",
        "                batch.gtTexts) if probabilityOfGT else self.toSparse(texts)\n",
        "            ctcInput = evalRes[1]\n",
        "            evalList = self.lossPerElement\n",
        "            feedDict = {self.savedCtcInput: ctcInput, self.gtTexts: sparse,\n",
        "                        self.seqLen: [MAX_TEXT_LENGTH] * numBatchElements, self.is_train: False}\n",
        "\n",
        "            lossVals = self.sess.run(evalList, feedDict)\n",
        "\n",
        "            probs = np.exp(-lossVals)\n",
        "\n",
        "        # dump the output of the NN to CSV file(s)\n",
        "        if self.dump:\n",
        "            self.dumpNNOutput(evalRes[1])\n",
        "\n",
        "        return (texts, probs)\n",
        "\n",
        "    def save(self):\n",
        "        \"save model to file\"\n",
        "        self.snapID += 1\n",
        "        self.saver.save(self.sess, MODEL_PATH +\n",
        "                        EXPERIMENT_NAME, global_step=self.snapID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i5T0Evj21j4"
      },
      "source": [
        "SamplePreprosessor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQEGXWO32yFa"
      },
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "    \"scale image into the desired imgSize, transpose it for TF and normalize gray-values\"\n",
        "\n",
        "    # increase dataset size by applying random stretches to the images\n",
        "    if AUGMENT_IMAGE:\n",
        "        stretch = (random.random() - 0.5)  # -0.5 .. +0.5\n",
        "        # random width, but at least 1\n",
        "        wStretched = max(int(img.shape[1] * (1 + stretch)), 1)\n",
        "        # stretch horizontally by factor 0.5 .. 1.5\n",
        "        img = cv2.resize(img, (wStretched, img.shape[0]))\n",
        "\n",
        "    # create target image and copy sample image into it\n",
        "    (h, w) = img.shape\n",
        "    fx = w / IMAGE_WIDTH\n",
        "    fy = h / IMAGE_HEIGHT\n",
        "    f = max(fx, fy)\n",
        "    # scale according to f (result at least 1 and at most wt or ht)\n",
        "    newSize = (max(min(IMAGE_WIDTH, int(w / f)), 1),\n",
        "               max(min(IMAGE_HEIGHT, int(h / f)), 1))\n",
        "    img = cv2.resize(img, newSize)\n",
        "    target = np.ones([IMAGE_HEIGHT, IMAGE_WIDTH]) * 255\n",
        "    target[0:newSize[1], 0:newSize[0]] = img\n",
        "\n",
        "    # transpose for TF\n",
        "    img = cv2.transpose(target)\n",
        "\n",
        "    # normalize\n",
        "    (m, s) = cv2.meanStdDev(img)\n",
        "    m = m[0][0]\n",
        "    s = s[0][0]\n",
        "    img = img - m\n",
        "    img = img / s if s > 0 else img\n",
        "\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsrcMrBvw9ll"
      },
      "source": [
        "DataGenerator_BinaryFile.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCnawxzlmHXM"
      },
      "outputs": [],
      "source": [
        "class Sample:\n",
        "    \"a single sample from the dataset\"\n",
        "\n",
        "    def __init__(self, gtText, imageIdx, imageHeight, imageWidth, imageSize, imageStartPosition):\n",
        "        self.gtText = gtText\n",
        "        self.imageIdx = imageIdx\n",
        "        self.imageHeight = imageHeight\n",
        "        self.imageWidth = imageWidth\n",
        "        self.imageSize = imageSize\n",
        "        self.imageStartPosition = imageStartPosition\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    \"batch containing images and ground truth texts\"\n",
        "\n",
        "    def __init__(self, gtTexts, imgs):\n",
        "        self.gtTexts = gtTexts\n",
        "        self.imgs = np.stack(imgs, axis=0)\n",
        "\n",
        "\n",
        "class DataGenerator:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.binaryImageFile = open(BASE_IMAGES_FILE, \"rb\")\n",
        "        self.currIdx = 0\n",
        "        self.samples = []\n",
        "        self.trainSamples = []\n",
        "        self.validationSamples = []\n",
        "        self.testSamples = []\n",
        "\n",
        "    def LoadData(self, operationType):\n",
        "        if not os.path.isfile(TRAINING_LABELS_FILE) \\\n",
        "                or not os.path.isfile(VALIDATION_LABELS_FILE) \\\n",
        "                or not os.path.isfile(TESTING_LABELS_FILE):\n",
        "            self.createDataFiles()\n",
        "\n",
        "        if operationType == OperationType.Training:\n",
        "            self.loadDataFile(OperationType.Training)\n",
        "            self.loadDataFile(OperationType.Validation)\n",
        "        elif operationType == OperationType.Validation:\n",
        "            self.loadDataFile(OperationType.Validation)\n",
        "        elif operationType == OperationType.Testing:\n",
        "            self.loadDataFile(OperationType.Testing)\n",
        "\n",
        "    def createDataFiles(self):\n",
        "        charsSet = set()\n",
        "        wordsSet = set()\n",
        "\n",
        "        f = open(BASE_LABELS_FILE, encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            # read all samples ==> append line as is\n",
        "            self.samples.append(line)\n",
        "\n",
        "            if REGENERATE_CHARLIST_AND_CORPUS:\n",
        "                # extract unique characters from text\n",
        "                lineSplit = line.split(';')\n",
        "                gtText = lineSplit[8]\n",
        "                gtText = gtText[5:]\n",
        "                wordsSet.add(gtText)\n",
        "                charsSet = charsSet.union(set(list(gtText)))\n",
        "\n",
        "        f.close()\n",
        "\n",
        "        # create a text file that contains all the characters in the dataset\n",
        "        # this list shall used to create the CTC model\n",
        "        # There might be a problem if a previously saved model used larger data, consequently, not all\n",
        "        # the characters in the previous model will be generated and therefore RNN creation will fail\n",
        "        # note that a problem might arise when we try to open a saved model that was saved on a larger dataset\n",
        "        # conseuqnelty some represented characters might be abscent and the new model will fail to load previous one\n",
        "        # a solution for this problem is to use a static character set for the used dataset\n",
        "        # also create the corpus data file for BeamSearch (if required)\n",
        "\n",
        "        # DONT CREATE THEM UNLESS U R USING LARGER DATASET, ALREADY CREATED IN DIRECTORY\n",
        "        if REGENERATE_CHARLIST_AND_CORPUS:\n",
        "            localCharList = sorted(list(charsSet))\n",
        "            open(fnCharList, 'w',\n",
        "                 encoding=\"utf-8\").write(str().join(localCharList))\n",
        "            open(fnCorpus, 'w',\n",
        "                 encoding=\"utf-8\").write(str().join(sorted(list(wordsSet))))\n",
        "\n",
        "        # first of all, make sure to randomly shuffle the main lables file\n",
        "        # random.shuffle(self.samples)\n",
        "\n",
        "        # split into training, validation, testing\n",
        "        lenOfAllSamples = len(self.samples)\n",
        "        lenOfTrainSamples = int(TRAINING_DATASET_SIZE * lenOfAllSamples)\n",
        "        lenOfTrainingAndValidationSamples = lenOfAllSamples - lenOfTrainSamples\n",
        "        lenOfValidationSamples = int(\n",
        "            VALIDATION_DATASET_SPLIT_SIZE * lenOfTrainingAndValidationSamples)\n",
        "\n",
        "        with open(TRAINING_LABELS_FILE, 'w', encoding=\"utf-8\") as f:\n",
        "            for item in self.samples[:lenOfTrainSamples]:\n",
        "                f.write(item)\n",
        "\n",
        "        with open(VALIDATION_LABELS_FILE, 'w', encoding=\"utf-8\") as f:\n",
        "            for item in self.samples[lenOfTrainSamples:lenOfTrainSamples + lenOfValidationSamples]:\n",
        "                f.write(item)\n",
        "\n",
        "        with open(TESTING_LABELS_FILE, 'w', encoding=\"utf-8\") as f:\n",
        "            for item in self.samples[lenOfTrainSamples + lenOfValidationSamples:]:\n",
        "                f.write(item)\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "    def loadDataFile(self, operationType):\n",
        "        if operationType == OperationType.Training:\n",
        "            fileName = TRAINING_LABELS_FILE\n",
        "        elif operationType == OperationType.Validation:\n",
        "            fileName = VALIDATION_LABELS_FILE\n",
        "        elif operationType == OperationType.Testing:\n",
        "            fileName = TESTING_LABELS_FILE\n",
        "\n",
        "        f = open(fileName, encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            lineSplit = line.split(';')\n",
        "\n",
        "            imgIdx = lineSplit[0]\n",
        "            imgIdx = imgIdx[10:]\n",
        "\n",
        "            imgStartPosition = lineSplit[1]\n",
        "            imgStartPosition = int(imgStartPosition[15:])\n",
        "\n",
        "            imgHeight = lineSplit[2]\n",
        "            imgHeight = int(imgHeight[13:])\n",
        "            imgWidth = lineSplit[3]\n",
        "            imgWidth = int(imgWidth[12:])\n",
        "            imgSize = imgHeight * imgWidth\n",
        "\n",
        "            gtText = lineSplit[8]\n",
        "            gtText = gtText[5:]\n",
        "            #gtText = self.truncateLabel(' '.join(gtText), MAX_TEXT_LENGTH)\n",
        "\n",
        "            # put sample into list\n",
        "            if operationType == OperationType.Training:\n",
        "                self.trainSamples.append(\n",
        "                    Sample(gtText, imgIdx, imgHeight, imgWidth, imgSize, imgStartPosition))\n",
        "            elif operationType == OperationType.Validation:\n",
        "                self.validationSamples.append(\n",
        "                    Sample(gtText, imgIdx, imgHeight, imgWidth, imgSize, imgStartPosition))\n",
        "            elif operationType == OperationType.Testing:\n",
        "                self.testSamples.append(\n",
        "                    Sample(gtText, imgIdx, imgHeight, imgWidth, imgSize, imgStartPosition))\n",
        "\n",
        "    def truncateLabel(self, text, maxTextLen):\n",
        "        # ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
        "        # labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "        # If a too-long label is provided, ctc_loss returns an infinite gradient\n",
        "        cost = 0\n",
        "        for i in range(len(text)):\n",
        "            if i != 0 and text[i] == text[i - 1]:\n",
        "                cost += 2\n",
        "            else:\n",
        "                cost += 1\n",
        "            if cost > maxTextLen:\n",
        "                return text[:i]\n",
        "        return text\n",
        "\n",
        "    def selectTrainingSet(self):\n",
        "        \"switch to randomly chosen subset of training set\"\n",
        "        self.currIdx = 0\n",
        "        random.shuffle(self.trainSamples)\n",
        "        self.samples = self.trainSamples[:TRAINING_SAMPLES_PER_EPOCH]\n",
        "\n",
        "    def selectValidationSet(self):\n",
        "        \"switch to validation set\"\n",
        "        self.currIdx = 0\n",
        "        random.shuffle(self.validationSamples)\n",
        "        self.samples = self.validationSamples[:\n",
        "                                              VALIDATIOIN_SAMPLES_PER_STEP]\n",
        "\n",
        "    def selectTestSet(self):\n",
        "        \"switch to validation set\"\n",
        "        self.currIdx = 0\n",
        "        random.shuffle(self.testSamples)\n",
        "        self.samples = self.testSamples[:VALIDATIOIN_SAMPLES_PER_STEP]\n",
        "\n",
        "    def getIteratorInfo(self):\n",
        "        \"current batch index and overall number of batches\"\n",
        "        return (self.currIdx // BATCH_SIZE + 1, len(self.samples) // BATCH_SIZE)\n",
        "\n",
        "    def hasNext(self):\n",
        "        \"iterator\"\n",
        "        return self.currIdx + BATCH_SIZE <= len(self.samples)\n",
        "\n",
        "    def getNext(self):\n",
        "        \"iterator\"\n",
        "        batchRange = range(self.currIdx, self.currIdx + BATCH_SIZE)\n",
        "        gtTexts = [self.samples[i].gtText for i in batchRange]\n",
        "\n",
        "        imgs = []\n",
        "        for i in batchRange:\n",
        "            try:\n",
        "                self.binaryImageFile.seek(self.samples[i].imageStartPosition)\n",
        "                img = np.frombuffer(self.binaryImageFile.read(\n",
        "                    self.samples[i].imageSize), np.dtype('B'))\n",
        "                img = img.reshape(\n",
        "                    self.samples[i].imageHeight, self.samples[i].imageWidth)\n",
        "                img = preprocess(img)\n",
        "                # img = preprocess(img, IMAGE_WIDTH, IMAGE_HEIGHT, RESIZE_IMAGE,\n",
        "                #                  CONVERT_IMAGE_TO_MONOCHROME, AUGMENT_IMAGE)\n",
        "                imgs.append(img)\n",
        "            except IOError as e:\n",
        "                print(\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
        "                pass\n",
        "            except ValueError as e:\n",
        "                print(e)\n",
        "                pass\n",
        "            except Error as e:\n",
        "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "                print(\"Value error({0}): {1}\".format(e.errno, e.strerror))\n",
        "\n",
        "                pass\n",
        "\n",
        "        self.currIdx += BATCH_SIZE\n",
        "        return Batch(gtTexts, imgs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GxWYW6hmiMJ"
      },
      "source": [
        "main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLw1ko6fmIap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53b15433-0b2e-4d51-cebe-f0fa91a14f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT_NAME: Test Drive Training Process\n",
            "Training Using Dataset: OperationType.Training\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:64: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:64: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:158: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:158: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:161: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:161: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:166: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-1f81bbb38ca4>:166: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Details\n",
            "Shape:(5, 5, 1, 32) ,shape length:4 , parameters: 800\n",
            "Shape:(32,) ,shape length:1 , parameters: 32\n",
            "Shape:(32,) ,shape length:1 , parameters: 32\n",
            "Shape:(5, 5, 32, 64) ,shape length:4 , parameters: 51200\n",
            "Shape:(64,) ,shape length:1 , parameters: 64\n",
            "Shape:(64,) ,shape length:1 , parameters: 64\n",
            "Shape:(3, 3, 64, 128) ,shape length:4 , parameters: 73728\n",
            "Shape:(128,) ,shape length:1 , parameters: 128\n",
            "Shape:(128,) ,shape length:1 , parameters: 128\n",
            "Shape:(3, 3, 128, 128) ,shape length:4 , parameters: 147456\n",
            "Shape:(128,) ,shape length:1 , parameters: 128\n",
            "Shape:(128,) ,shape length:1 , parameters: 128\n",
            "Shape:(3, 3, 128, 256) ,shape length:4 , parameters: 294912\n",
            "Shape:(256,) ,shape length:1 , parameters: 256\n",
            "Shape:(256,) ,shape length:1 , parameters: 256\n",
            "Shape:(512, 1024) ,shape length:2 , parameters: 524288\n",
            "Shape:(1024,) ,shape length:1 , parameters: 1024\n",
            "Shape:(512, 1024) ,shape length:2 , parameters: 524288\n",
            "Shape:(1024,) ,shape length:1 , parameters: 1024\n",
            "Shape:(1, 1, 512, 47) ,shape length:4 , parameters: 24064\n",
            "Total Parameters: 1644000\n",
            "\n",
            "\n",
            "Python: 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow: 1.15.2\n",
            "Init with new values\n",
            "____________________________________________________________\n",
            "Experiment Name: Test Drive Training Process\n",
            "Base File Name: 1_nice_60000_rows\n",
            "Start Execution Time :07/28/2022, 22:18:47\n",
            "Training set size: 54000\n",
            "Validation set size: 3000\n",
            "Training Samples per epoch: 5000\n",
            "Validation Samples per step: 1000\n",
            "Batch size: 100\n",
            "TRAINING_SAMPLES_PER_EPOCH: 5000\n",
            "BATCH_SIZE: 100\n",
            "VALIDATIOIN_SAMPLES_PER_STEP: 1000\n",
            "TRAINING_DATASET_SIZE: 0.9\n",
            "VALIDATION_DATASET_SPLIT_SIZE: 0.5\n",
            "IMAGE_WIDTH: 128\n",
            "IMAGE_HEIGHT: 32\n",
            "MAX_TEXT_LENGTH: 32\n",
            "RESIZE_IMAGE: True\n",
            "CONVERT_IMAGE_TO_MONOCHROME: False\n",
            "MONOCHROME_BINARY_THRESHOLD: 127\n",
            "AUGMENT_IMAGE: False\n",
            "\n",
            "\n",
            "Current Time = 2022-07-28 22:18:51.748601\n",
            "Epoch: 1\n",
            "cannot reshape array of size 0 into shape (35,60)\n",
            "cannot reshape array of size 0 into shape (35,79)\n",
            "cannot reshape array of size 0 into shape (35,74)\n",
            "cannot reshape array of size 0 into shape (35,64)\n",
            "cannot reshape array of size 0 into shape (35,69)\n",
            "cannot reshape array of size 0 into shape (35,85)\n",
            "cannot reshape array of size 0 into shape (35,59)\n",
            "cannot reshape array of size 0 into shape (35,64)\n",
            "cannot reshape array of size 0 into shape (35,73)\n",
            "cannot reshape array of size 0 into shape (35,77)\n",
            "cannot reshape array of size 0 into shape (35,93)\n",
            "cannot reshape array of size 0 into shape (35,75)\n",
            "cannot reshape array of size 0 into shape (35,65)\n",
            "cannot reshape array of size 0 into shape (35,60)\n",
            "cannot reshape array of size 0 into shape (35,62)\n",
            "cannot reshape array of size 0 into shape (35,66)\n",
            "cannot reshape array of size 0 into shape (35,87)\n",
            "cannot reshape array of size 0 into shape (35,95)\n",
            "cannot reshape array of size 0 into shape (35,57)\n",
            "cannot reshape array of size 0 into shape (35,63)\n",
            "cannot reshape array of size 0 into shape (35,59)\n",
            "cannot reshape array of size 0 into shape (35,51)\n",
            "cannot reshape array of size 0 into shape (35,59)\n",
            "cannot reshape array of size 0 into shape (35,62)\n",
            "cannot reshape array of size 0 into shape (35,77)\n",
            "cannot reshape array of size 0 into shape (35,80)\n",
            "cannot reshape array of size 0 into shape (35,58)\n",
            "cannot reshape array of size 0 into shape (35,71)\n",
            "cannot reshape array of size 0 into shape (35,85)\n",
            "cannot reshape array of size 0 into shape (35,68)\n",
            "cannot reshape array of size 0 into shape (35,91)\n",
            "cannot reshape array of size 0 into shape (35,76)\n",
            "cannot reshape array of size 0 into shape (35,75)\n",
            "cannot reshape array of size 0 into shape (35,65)\n",
            "cannot reshape array of size 0 into shape (35,97)\n",
            "cannot reshape array of size 0 into shape (35,68)\n",
            "cannot reshape array of size 0 into shape (35,71)\n",
            "cannot reshape array of size 0 into shape (35,65)\n",
            "cannot reshape array of size 0 into shape (35,61)\n",
            "cannot reshape array of size 0 into shape (35,83)\n",
            "cannot reshape array of size 0 into shape (35,78)\n",
            "cannot reshape array of size 0 into shape (35,72)\n",
            "cannot reshape array of size 0 into shape (35,66)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: label SparseTensor is not valid: indices[502] = [57,0] is out of bounds: need 0 <= index < [57,11]\n\t [[{{node CTCLoss}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aba0a7181c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-aba0a7181c17>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDECODER_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmustRestore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mOPERATION_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOperationType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mOPERATION_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOperationType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTesting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mauditString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"EXPERIMENT_NAME: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEXPERIMENT_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-aba0a7181c17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(paraModel)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0miterInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetIteratorInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# #stop execution after reaching a certain threashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1f81bbb38ca4>\u001b[0m in \u001b[0;36mtrainBatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    306\u001b[0m                     self.is_train: True}\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossVal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchesTrained\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: label SparseTensor is not valid: indices[502] = [57,0] is out of bounds: need 0 <= index < [57,11]\n\t [[node CTCLoss (defined at /tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'CTCLoss':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 661, in <lambda>\n    self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 577, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 606, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 556, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-aba0a7181c17>\", line 213, in <module>\n    main()\n  File \"<ipython-input-14-aba0a7181c17>\", line 188, in main\n    model = Model(DECODER_TYPE, mustRestore=False, dump=False)\n  File \"<ipython-input-11-1f81bbb38ca4>\", line 22, in __init__\n    self.setupCTC()\n  File \"<ipython-input-11-1f81bbb38ca4>\", line 190, in setupCTC\n    ctc_merge_repeated=True))\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/ctc_ops.py\", line 176, in ctc_loss\n    ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/gen_ctc_ops.py\", line 336, in ctc_loss\n    name=name)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "startTime = datetime.now()\n",
        "totalProcessingTime = 0\n",
        "\n",
        "\n",
        "# we only need DataGenerator in training, validation, testing inorder to access the related datasets\n",
        "if OPERATION_TYPE != OperationType.Infer:\n",
        "    dataGenerator = DataGenerator()\n",
        "\n",
        "def accumulateProcessingTime(paraTimeSnapshot):\n",
        "    totalProcessingTime = time.time()\n",
        "    #totalProcessingTime = totalProcessingTime + (time.time() - paraTimeSnapshot)\n",
        "\n",
        "def train(paraModel):\n",
        "    \"train NN\"\n",
        "    epoch = 0  # number of training epochs since start\n",
        "    bestCharErrorRate = float('inf')  # best valdiation character error rate\n",
        "    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\n",
        "\n",
        "    auditString = get_initial_status_log()\n",
        "    print(auditString)\n",
        "    auditLog(auditString)\n",
        "\n",
        "    continueLooping = True\n",
        "\n",
        "    while continueLooping:\n",
        "        print(\"Current Time =\", datetime.now())\n",
        "\n",
        "        epoch += 1\n",
        "        print('Epoch:', epoch)\n",
        "\n",
        "        dataGenerator.selectTrainingSet()\n",
        "\n",
        "        while dataGenerator.hasNext():\n",
        "\n",
        "            timeSnapshot = time.time()\n",
        "\n",
        "            iterInfo = dataGenerator.getIteratorInfo()\n",
        "            batch = dataGenerator.getNext()\n",
        "            loss = paraModel.trainBatch(batch)\n",
        "\n",
        "            # #stop execution after reaching a certain threashold\n",
        "            # if (int(loss) == 1):\n",
        "            #     noImprovementSince = MAXIMUM_NONIMPROVED_EPOCHS;\n",
        "\n",
        "            print('Training Batch:', iterInfo[0],\n",
        "                  '/', iterInfo[1], 'Loss:', loss)\n",
        "\n",
        "            accumulateProcessingTime(timeSnapshot)\n",
        "\n",
        "        # validate\n",
        "        charErrorRate, charSuccessRate, wordsSuccessRate = validate(\n",
        "            paraModel, OperationType.Validation)\n",
        "        auditString = \"Epoch Number %d.\" % epoch + \"\\n\"\n",
        "\n",
        "        # if best validation accuracy so far, save model parameters\n",
        "        if charErrorRate < bestCharErrorRate:\n",
        "            auditString = auditString + 'Character error rate improved, saving model'\n",
        "            paraModel.save()\n",
        "            bestCharErrorRate = charErrorRate\n",
        "            noImprovementSince = 0\n",
        "        else:\n",
        "            auditString = auditString + \"Character error rate not improved\\n\"\n",
        "            noImprovementSince += 1\n",
        "\n",
        "        # stop training if no more improvement in the last x epochs\n",
        "        if noImprovementSince >= MAXIMUM_NONIMPROVED_EPOCHS:\n",
        "            auditString = auditString + \\\n",
        "                \"No more improvement since %d epochs.\" % MAXIMUM_NONIMPROVED_EPOCHS + \"\\n\"\n",
        "\n",
        "            # gracefull termination\n",
        "            continueLooping = False\n",
        "\n",
        "        # Model did not finish, print log and save it\n",
        "        auditString = auditString + \\\n",
        "            get_execution_log(charSuccessRate, wordsSuccessRate)\n",
        "        print(auditString)\n",
        "        auditLog(auditString)\n",
        "\n",
        "\n",
        "def validate(paraModel, paraOperationType):\n",
        "    if paraOperationType == OperationType.Validation:\n",
        "        dataGenerator.selectValidationSet()\n",
        "\n",
        "    elif paraOperationType == OperationType.Testing:\n",
        "        dataGenerator.selectTestSet()\n",
        "\n",
        "    numCharErr = 0\n",
        "    numCharTotal = 0\n",
        "    numWordOK = 0\n",
        "    numWordTotal = 0\n",
        "    timeSnapshot = 0.0\n",
        "\n",
        "    while dataGenerator.hasNext():\n",
        "        timeSnapshot = time.time()\n",
        "\n",
        "        iterInfo = dataGenerator.getIteratorInfo()\n",
        "        print('Validating Batch:', iterInfo[0], '/', iterInfo[1])\n",
        "        batch = dataGenerator.getNext()\n",
        "        (recognized, _) = paraModel.inferBatch(batch)\n",
        "\n",
        "        accumulateProcessingTime(timeSnapshot)\n",
        "\n",
        "        # print('Ground truth -> Recognized')\n",
        "        for i in range(len(recognized)):\n",
        "            numWordTotal += 1\n",
        "            numCharTotal += len(batch.gtTexts[i])\n",
        "\n",
        "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
        "\n",
        "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
        "            numCharErr += dist\n",
        "\n",
        "            # remove remark to see each success and error values\n",
        "            #print('[OK]' if dist==0 else '[ERR:%d]' % dist,'\"' + batch.gtTexts[i] + '\"', '->', '\"' + recognized[i] + '\"')\n",
        "\n",
        "    # print validation result\n",
        "    charErrorRate = numCharErr / numCharTotal\n",
        "    charSuccessRate = 1 - (numCharErr / numCharTotal)\n",
        "    wordsSuccessRate = numWordOK / numWordTotal\n",
        "\n",
        "    # print and save validation result, this includes post epoch operation as well as when\n",
        "    # running standalone testing or validation processes\n",
        "\n",
        "    return charErrorRate, charSuccessRate, wordsSuccessRate\n",
        "\n",
        "\n",
        "def inferSingleImage(paraModel, paraFnImg):\n",
        "    \"recognize text in image provided by file path\"\n",
        "    img = cv2.imread(paraFnImg, cv2.IMREAD_GRAYSCALE)\n",
        "    img = preprocess(img)\n",
        "    # img = preprocess(img, IMAGE_WIDTH,\n",
        "    #                  IMAGE_HEIGHT, True, False, False)\n",
        "\n",
        "    batch = Batch(None, [img])\n",
        "    #(recognized, probability) = model.inferBatch(batch)\n",
        "    (recognized, probability) = paraModel.inferBatch(batch, True)\n",
        "    print('Recognized:', '\"' + recognized[0] + '\"')\n",
        "    print('Probability:', probability[0])\n",
        "\n",
        "\n",
        "def get_initial_status_log():\n",
        "    auditString = \"____________________________________________________________\" + \"\\n\"\n",
        "    auditString = auditString + \"Experiment Name: \" + EXPERIMENT_NAME + \"\\n\"\n",
        "    auditString = auditString + \"Base File Name: \" + BASE_FILENAME + \"\\n\"\n",
        "    auditString = auditString + 'Start Execution Time :' + \\\n",
        "        startTime.strftime(\"%m/%d/%Y, %H:%M:%S\") + \"\\n\"\n",
        "    auditString = auditString + \"Training set size: \" + \\\n",
        "        str(len(dataGenerator.trainSamples)) + \"\\n\"\n",
        "    auditString = auditString + \"Validation set size: \" + \\\n",
        "        str(len(dataGenerator.validationSamples)) + \"\\n\"\n",
        "    auditString = auditString + \"Training Samples per epoch: \" + \\\n",
        "        str(TRAINING_SAMPLES_PER_EPOCH) + \"\\n\"\n",
        "    auditString = auditString + \"Validation Samples per step: \" + \\\n",
        "        str(VALIDATIOIN_SAMPLES_PER_STEP) + \"\\n\"\n",
        "    auditString = auditString + \"Batch size: \" + str(BATCH_SIZE) + \"\\n\"\n",
        "    auditString = auditString + \"TRAINING_SAMPLES_PER_EPOCH: \" + \\\n",
        "        str(TRAINING_SAMPLES_PER_EPOCH) + \"\\n\"\n",
        "    auditString = auditString + \"BATCH_SIZE: \" + str(BATCH_SIZE) + \"\\n\"\n",
        "    auditString = auditString + \"VALIDATIOIN_SAMPLES_PER_STEP: \" + \\\n",
        "        str(VALIDATIOIN_SAMPLES_PER_STEP) + \"\\n\"\n",
        "    auditString = auditString + \"TRAINING_DATASET_SIZE: \" + \\\n",
        "        str(TRAINING_DATASET_SIZE) + \"\\n\"\n",
        "    auditString = auditString + \"VALIDATION_DATASET_SPLIT_SIZE: \" + \\\n",
        "        str(VALIDATION_DATASET_SPLIT_SIZE) + \"\\n\"\n",
        "    auditString = auditString + \"IMAGE_WIDTH: \" + \\\n",
        "        str(IMAGE_WIDTH) + \"\\n\"\n",
        "    auditString = auditString + \"IMAGE_HEIGHT: \" + \\\n",
        "        str(IMAGE_HEIGHT) + \"\\n\"\n",
        "    auditString = auditString + \"MAX_TEXT_LENGTH: \" + \\\n",
        "        str(MAX_TEXT_LENGTH) + \"\\n\"\n",
        "    auditString = auditString + \"RESIZE_IMAGE: \" + \\\n",
        "        str(RESIZE_IMAGE) + \"\\n\"\n",
        "    auditString = auditString + \"CONVERT_IMAGE_TO_MONOCHROME: \" + \\\n",
        "        str(CONVERT_IMAGE_TO_MONOCHROME) + \"\\n\"\n",
        "    auditString = auditString + \"MONOCHROME_BINARY_THRESHOLD: \" + \\\n",
        "        str(MONOCHROME_BINARY_THRESHOLD) + \"\\n\"\n",
        "    auditString = auditString + \"AUGMENT_IMAGE: \" + \\\n",
        "        str(AUGMENT_IMAGE) + \"\\n\\n\"\n",
        "\n",
        "    return auditString\n",
        "\n",
        "\n",
        "def get_execution_log(paraCharSuccessRate, paraWordsSuccessRate):\n",
        "    auditString = \"Start Execution Time : \" + \\\n",
        "        startTime.strftime(\"%m/%d/%Y, %H:%M:%S\") + \"\\n\"\n",
        "    auditString = auditString + \"End Execution Time  :\" + \\\n",
        "        datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\") + \"\\n\"\n",
        "    auditString = auditString + \"Accumulated Processing Time : \" + \\\n",
        "        str(totalProcessingTime / 60) + \" minutes\" + \"\\n\"\n",
        "    auditString = auditString + \"Characters Success Rate: \" + \\\n",
        "        str(paraCharSuccessRate * 100.0) + \"%\\n\"\n",
        "    auditString = auditString + \"Words Success Rate: \" + \\\n",
        "        str(paraWordsSuccessRate * 100.0) + \"%\\n\\n\"\n",
        "\n",
        "    return auditString\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    if OPERATION_TYPE != OperationType.Infer:\n",
        "        dataGenerator.LoadData(OPERATION_TYPE)\n",
        "\n",
        "    if OPERATION_TYPE == OperationType.Training:\n",
        "        auditString = \"EXPERIMENT_NAME: \" + EXPERIMENT_NAME + \"\\n\"\n",
        "        auditString = auditString + \"Training Using Dataset: \" + \\\n",
        "            str(OPERATION_TYPE) + \"\\n\"\n",
        "        print(auditString)\n",
        "        auditLog(auditString)\n",
        "\n",
        "        model = Model(DECODER_TYPE, mustRestore=False, dump=False)\n",
        "        train(model)\n",
        "    elif OPERATION_TYPE == OperationType.Validation or OPERATION_TYPE == OperationType.Testing:\n",
        "        auditString = \"EXPERIMENT_NAME: \" + EXPERIMENT_NAME + \"\\n\"\n",
        "        auditString = auditString + \"Validation/Tesing Using Dataset: \" + \\\n",
        "            str(OPERATION_TYPE) + \"\\n\"\n",
        "        print(auditString)\n",
        "\n",
        "        model = Model(DECODER_TYPE, mustRestore=True, dump=False)\n",
        "        charErrorRate, charSuccessRate, wordsSuccessRate = validate(\n",
        "            model, OPERATION_TYPE)\n",
        "\n",
        "        auditString = auditString + \\\n",
        "            get_execution_log(charSuccessRate, wordsSuccessRate) + \"\\n\"\n",
        "        print(auditString)\n",
        "\n",
        "        auditLog(auditString)\n",
        "\n",
        "    elif OPERATION_TYPE == OperationType.Infer:  # infer text on test image\n",
        "        print(open(fnResult).read())\n",
        "        tf.reset_default_graph()\n",
        "        #model = Model(open(fnCharList, encoding=\"utf-8\").read(), decoderType, mustRestore=True, dump=args.dump)\n",
        "        model = Model(DECODER_TYPE, mustRestore=True, dump=False)\n",
        "        inferSingleImage(model, fnInfer)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Arabic OCR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}