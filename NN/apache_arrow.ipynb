{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em-Ab59IhqZK",
        "outputId": "543b1240-c071-4fb8-f3d4-4cdb412f622f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io) (0.37.1)\n",
            "Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_io\n",
            "Successfully installed tensorflow_io-0.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIHUxvgzgm8e",
        "outputId": "de7a6350-c9e8-4daf-95b1-f89680c3cc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label        x0        x1\n",
            "0      0 -0.003454  0.434645\n",
            "1      0  0.070342  0.143294\n",
            "2      1  4.431127  5.518084\n",
            "3      0  1.162140 -0.817346\n",
            "4      0 -0.158896  1.471649\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = {'label': np.random.binomial(1, 0.5, 10)}\n",
        "data['x0'] = np.random.randn(10) + 5 * data['label']\n",
        "data['x1'] = np.random.randn(10) + 5 * data['label']\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_io.arrow as arrow_io\n",
        "\n",
        "ds = arrow_io.ArrowDataset.from_pandas(\n",
        "    df,\n",
        "    batch_size=2,\n",
        "    preserve_index=False)\n",
        "\n",
        "# Make an iterator to the dataset\n",
        "ds_iter = iter(ds)\n",
        "\n",
        "# Print the first batch\n",
        "print(next(ds_iter))"
      ],
      "metadata": {
        "id": "g2WJA8u4g3Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow_io.arrow as arrow_io\n",
        "from pyarrow.feather import write_feather\n",
        "\n",
        "# Write the Pandas DataFrame to a Feather file\n",
        "write_feather(df, '/path/to/df.feather')\n",
        "\n",
        "# Create the dataset with one or more filenames\n",
        "ds = arrow_io.ArrowFeatherDataset(\n",
        "    ['/path/to/df.feather'],\n",
        "    columns=(0, 1, 2),\n",
        "    output_types=(tf.int64, tf.float64, tf.float64),\n",
        "    output_shapes=([], [], []))\n",
        "\n",
        "# Iterate over each row of each file\n",
        "for record in ds:\n",
        "   label, x0, x1 = record\n",
        "   # use label and feature tensors"
      ],
      "metadata": {
        "id": "0Am2Zc0zg5of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_io.arrow as arrow_io\n",
        "\n",
        "ds = arrow_io.ArrowStreamDataset.from_pandas(\n",
        "    df,\n",
        "    batch_size=2,\n",
        "    preserve_index=False)"
      ],
      "metadata": {
        "id": "FRksdbpVimfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fit(ds):\n",
        "  \"\"\"Create and fit a Keras logistic regression model.\"\"\"\n",
        "\n",
        "  # Build the Keras model\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(1, input_shape=(2,),\n",
        "            activation='sigmoid'))\n",
        "  model.compile(optimizer='sgd', loss='mean_squared_error',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # Fit the model on the given dataset\n",
        "  model.fit(ds, epochs=5, shuffle=False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "HhZu7FDdio_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_process(filename):\n",
        "  \"\"\"Read the given CSV file and yield processed Arrow batches.\"\"\"\n",
        "\n",
        "  # Read a CSV file into an Arrow Table with threading enabled and\n",
        "  # set block_size in bytes to break the file into chunks for granularity,\n",
        "  # which determines the number of batches in the resulting pyarrow.Table\n",
        "  opts = pyarrow.csv.ReadOptions(use_threads=True, block_size=4096)\n",
        "  table = pyarrow.csv.read_csv(filename, opts)\n",
        "\n",
        "  # Fit the feature transform\n",
        "  df = table.to_pandas()\n",
        "  scaler = StandardScaler().fit(df[['x0', 'x1']])\n",
        "\n",
        "  # Iterate over batches in the pyarrow.Table and apply processing\n",
        "  for batch in table.to_batches():\n",
        "    df = batch.to_pandas()\n",
        "\n",
        "    # Process the batch and apply feature transform\n",
        "    X_scaled = scaler.transform(df[['x0', 'x1']])\n",
        "    df_scaled = pd.DataFrame({'label': df['label'],\n",
        "                              'x0': X_scaled[:, 0],\n",
        "                              'x1': X_scaled[:, 1]})\n",
        "    batch_scaled = pa.RecordBatch.from_pandas(df_scaled, preserve_index=False)\n",
        "\n",
        "    yield batch_scaled"
      ],
      "metadata": {
        "id": "XmZQum1-iu1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_local_dataset(filename):\n",
        "  \"\"\"Make a TensorFlow Arrow Dataset that reads from a local CSV file.\"\"\"\n",
        "\n",
        "  # Read the local file and get a record batch iterator\n",
        "  batch_iter = read_and_process(filename)\n",
        "\n",
        "  # Create the Arrow Dataset as a stream from local iterator of record batches\n",
        "  ds = arrow_io.ArrowStreamDataset.from_record_batches(\n",
        "    batch_iter,\n",
        "    output_types=(tf.int64, tf.float64, tf.float64),\n",
        "    output_shapes=(tf.TensorShape([]), tf.TensorShape([]), tf.TensorShape([])),\n",
        "    batch_mode='auto',\n",
        "    record_batch_iter_factory=partial(read_and_process, filename))\n",
        "\n",
        "  # Map the dataset to combine feature columns to single tensor\n",
        "  ds = ds.map(lambda l, x0, x1: (tf.stack([x0, x1], axis=1), l))\n",
        "  return ds"
      ],
      "metadata": {
        "id": "D4pK9UcMi-Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = make_local_dataset(filename)\n",
        "model = model_fit(ds)\n",
        "\n",
        "print(\"Fit model with weights: {}\".format(model.get_weights()))"
      ],
      "metadata": {
        "id": "3To_1897jEcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_process_dir(directory):\n",
        "  \"\"\"Read a directory of CSV files and yield processed Arrow batches.\"\"\"\n",
        "\n",
        "  for f in os.listdir(directory):\n",
        "    if f.endswith(\".csv\"):\n",
        "      filename = os.path.join(directory, f)\n",
        "      for batch in read_and_process(filename):\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "thKMDIqTjJMd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serve_csv_data(ip_addr, port_num, directory):\n",
        "  \"\"\"\n",
        "  Create a socket and serve Arrow record batches as a stream read from the\n",
        "  given directory containing CVS files.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the socket\n",
        "  sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "  sock.bind((ip_addr, port_num))\n",
        "  sock.listen(1)\n",
        "\n",
        "  # Serve forever, each client will get one iteration over data\n",
        "  while True:\n",
        "    conn, _ = sock.accept()\n",
        "    outfile = conn.makefile(mode='wb')\n",
        "    writer = None\n",
        "    try:\n",
        "\n",
        "      # Read directory and iterate over each batch in each file\n",
        "      batch_iter = read_and_process_dir(directory)\n",
        "      for batch in batch_iter:\n",
        "\n",
        "        # Initialize the pyarrow writer on first batch\n",
        "        if writer is None:\n",
        "          writer = pa.RecordBatchStreamWriter(outfile, batch.schema)\n",
        "\n",
        "        # Write the batch to the client stream\n",
        "        writer.write_batch(batch)\n",
        "\n",
        "    # Cleanup client connection\n",
        "    finally:\n",
        "      if writer is not None:\n",
        "        writer.close()\n",
        "      outfile.close()\n",
        "      conn.close()\n",
        "  sock.close()"
      ],
      "metadata": {
        "id": "0h4yQm0jjMih"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_remote_dataset(endpoint):\n",
        "  \"\"\"Make a TensorFlow Arrow Dataset that reads from a remote Arrow stream.\"\"\"\n",
        "\n",
        "  # Create the Arrow Dataset from a remote host serving a stream\n",
        "  ds = arrow_io.ArrowStreamDataset(\n",
        "      [endpoint],\n",
        "      columns=(0, 1, 2),\n",
        "      output_types=(tf.int64, tf.float64, tf.float64),\n",
        "      output_shapes=(tf.TensorShape([]), tf.TensorShape([]), tf.TensorShape([])),\n",
        "      batch_mode='auto')\n",
        "\n",
        "  # Map the dataset to combine feature columns to single tensor\n",
        "  ds = ds.map(lambda l, x0, x1: (tf.stack([x0, x1], axis=1), l))\n",
        "  return ds"
      ],
      "metadata": {
        "id": "YWgly_pPjQHJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BubhO7zIjRVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}