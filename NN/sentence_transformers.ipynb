{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets sentence-transformers wandb --upgrade"
      ],
      "metadata": {
        "id": "jPnE7lRulVeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C225LBEolGd2"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "import logging\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        "    SentenceTransformerModelCardData,\n",
        ")\n",
        "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO\n",
        ")\n",
        "\n",
        "# 1. Load a model to finetune with 2. (Optional) model card data\n",
        "model = SentenceTransformer(\n",
        "    \"microsoft/mpnet-base\",\n",
        "    model_card_data=SentenceTransformerModelCardData(\n",
        "        language=\"en\",\n",
        "        license=\"apache-2.0\",\n",
        "        model_name=\"MPNet base trained on GooAQ pairs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "name = \"mpnet-base-gooaq-streaming\"\n",
        "\n",
        "# 2. Load a streaming dataset to finetune on\n",
        "train_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\", streaming=True)\n",
        "\n",
        "# 3. Define a loss function\n",
        "loss = MultipleNegativesRankingLoss(model)\n",
        "\n",
        "# 4. (Optional) Specify training arguments\n",
        "train_batch_size = 64\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    # Required parameter:\n",
        "    output_dir=f\"models/{name}\",\n",
        "    # Optional training parameters:\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    max_steps=5,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
        "    # Optional tracking/debugging parameters:\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=250,\n",
        "    logging_first_step=True,\n",
        "    run_name=name,  # Will be used in W&B if `wandb` is installed\n",
        ")\n",
        "\n",
        "# 5. Create a trainer & train\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=loss,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 6. Save the trained model\n",
        "model.save_pretrained(f\"models/{name}/final\")\n",
        "\n",
        "# 7. (Optional) Push it to the Hugging Face Hub\n",
        "model.push_to_hub(name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "buh3l1jnlSxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}