# -*- coding: utf-8 -*-
"""DTensors_Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RrBK4butZlqYqcw4ybeq_4CNp5liBqfX
"""

!pip install keras-nightly
!pip install --quiet tensorflow-dtensor
!pip install --quiet --upgrade tensorflow-datasets

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.experimental import dtensor

def configure_virtual_cpus(ncpu):
  phy_devices = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(
        phy_devices[0],
        [tf.config.LogicalDeviceConfiguration()] * ncpu)

configure_virtual_cpus(8)
tf.config.list_logical_devices('CPU')

devices = [f'CPU:{i}' for i in range(8)]

try:
    tf.keras.backend.experimental.enable_tf_random_generator()
except AttributeError:
    try:
        tf.keras.backend.enable_tf_random_generator()
    except AttributeError:
        try:
          tf.random.set_seed(1337)
        except AttributeError:
          print("Warning: enable_tf_random_generator not found. Randomness might not be fully controlled.")
          pass

mesh = dtensor.create_mesh([("batch", 8)], devices=devices)

example_weight_layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)  # or
example_weight_layout = dtensor.Layout.replicated(mesh, rank=2)

example_data_layout = dtensor.Layout(['batch', dtensor.UNSHARDED], mesh)  # or
example_data_layout = dtensor.Layout.batch_sharded(mesh, 'batch', rank=2)

unsharded_layout_2d = dtensor.Layout.replicated(mesh, 2)
unsharded_layout_1d = dtensor.Layout.replicated(mesh, 1)

# model = tf.keras.models.Sequential([
#   tf.keras.layers.Flatten(input_shape=(28, 28)),
#   tf.keras.layers.Dense(128,
#                         activation='relu',
#                         name='d1',
#                         kernel_initializer=unsharded_layout_2d,
#                         bias_initializer=unsharded_layout_1d),
#   tf.keras.layers.Dense(10,
#                         name='d2',
#                         kernel_initializer=unsharded_layout_2d,
#                         bias_initializer=unsharded_layout_1d)
# ])

model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128,
                              activation='relu',
                              name='d1',
                              kernel_initializer=tf.keras.initializers.GlorotUniform(),  # Initialize using Keras initializer directly
                              bias_initializer=tf.keras.initializers.Zeros()),  # Initialize using Keras initializer directly
                              # kernel_layout=unsharded_layout_2d,  # Remove these lines
                              # bias_layout=unsharded_layout_1d), # Remove these lines
        tf.keras.layers.Dense(10,
                              name='d2',
                              kernel_initializer=tf.keras.initializers.GlorotUniform(),  # Initialize using Keras initializer directly
                              bias_initializer=tf.keras.initializers.Zeros())  # Initialize using Keras initializer directly
                              # kernel_layout=unsharded_layout_2d,  # Remove these lines
                              # bias_layout=unsharded_layout_1d)  # Remove these lines
    ])

model.summary()

# # Creating DTensor variables with desired layouts
# def glorot_uniform_dtensor(shape, dtype=tf.float32, layout=None):
#   """Create a DTensor variable initialized with GlorotUniform."""
#   initializer = tf.keras.initializers.GlorotUniform()
#   # Call the initializer to get a Tensor and then pack it into a DTensor
#   # Important: Convert to DTensor after initialization
#   tensor = initializer(shape, dtype=dtype)
#   with tf.device(layout.mesh.local_devices()[0]):  # Initialize on the first device of the mesh
#     dtensor_var = dtensor.DVariable(tensor, layout=layout)
#   return dtensor_var


# def zeros_dtensor(shape, dtype=tf.float32, layout=None):
#   """Create a DTensor variable initialized with zeros."""
#   initializer = tf.keras.initializers.Zeros()
#   # Call the initializer to get a Tensor and then pack it into a DTensor
#   # Important: Convert to DTensor after initialization
#   tensor = initializer(shape, dtype=dtype)
#   with tf.device(layout.mesh.local_devices()[0]):  # Initialize on the first device of the mesh
#     dtensor_var = dtensor.DVariable(tensor, layout=layout)
#   return dtensor_var


# # Defining the model with DTensor variables
# model = tf.keras.models.Sequential([
#     tf.keras.layers.Flatten(input_shape=(28, 28)),
#     tf.keras.layers.Dense(128,
#                           activation='relu',
#                           name='d1',
#                           kernel_initializer=lambda shape, dtype: glorot_uniform_dtensor(shape, dtype, layout=unsharded_layout_2d),
#                           bias_initializer=lambda shape, dtype: zeros_dtensor(shape, dtype, layout=unsharded_layout_1d)),
#     tf.keras.layers.Dense(10,
#                           name='d2',
#                           kernel_initializer=lambda shape, dtype: glorot_uniform_dtensor(shape, dtype, layout=unsharded_layout_2d),
#                           bias_initializer=lambda shape, dtype: zeros_dtensor(shape, dtype, layout=unsharded_layout_1d))
# ])

# # Accessing DTensor layouts
# for weight in model.weights:
#   # Weights are now DTensors, so they have the 'layout' attribute
#   print(f'Weight name: {weight.name} with layout: {weight.layout}')
#   break

# for weight in model.weights:
#   print(f'Weight name: {weight.name} with layout: {weight.layout}')
#   break

for layer in model.layers:
    for weight in layer.weights:
        print(f"Layer: {layer.name}, Weight shape: {weight.shape}, dtype: {weight.dtype}")

(ds_train, ds_test), ds_info = tfds.load(
    'mnist',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

def normalize_img(image, label):
  """Normalizes images: `uint8` -> `float32`."""
  return tf.cast(image, tf.float32) / 255., label

batch_size = 128

ds_train = ds_train.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

ds_test = ds_test.map(
    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(batch_size)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.AUTOTUNE)

@tf.function
def train_step(model, x, y, optimizer, metrics):
  with tf.GradientTape() as tape:
    logits = model(x, training=True)
    # tf.reduce_sum sums the batch sharded per-example loss to a replicated
    # global loss (scalar).
    loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(
        y, logits, from_logits=True))

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  for metric in metrics.values():
    metric.update_state(y_true=y, y_pred=logits)

  loss_per_sample = loss / len(x)
  results = {'loss': loss_per_sample}
  return results

@tf.function
def eval_step(model, x, y, metrics):
  logits = model(x, training=False)
  loss = tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(
        y, logits, from_logits=True))

  for metric in metrics.values():
    metric.update_state(y_true=y, y_pred=logits)

  loss_per_sample = loss / len(x)
  results = {'eval_loss': loss_per_sample}
  return results

def pack_dtensor_inputs(images, labels, image_layout, label_layout):
  num_local_devices = image_layout.mesh.num_local_devices()
  images = tf.split(images, num_local_devices)
  labels = tf.split(labels, num_local_devices)
  images = dtensor.pack(images, image_layout)
  labels = dtensor.pack(labels, label_layout)
  return  images, labels

optimizer = tf.keras.optimizers.Adam(0.01)#, mesh=mesh)
metrics = {'accuracy': tf.keras.metrics.SparseCategoricalAccuracy()}#mesh=mesh)}
eval_metrics = {'eval_accuracy': tf.keras.metrics.SparseCategoricalAccuracy()}#mesh=mesh)}

num_epochs = 3

image_layout = dtensor.Layout.batch_sharded(mesh, 'batch', rank=4)
label_layout = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)

for epoch in range(num_epochs):
  print("============================")
  print("Epoch: ", epoch)
  for metric in metrics.values():
    metric.reset_state()
  step = 0
  results = {}
  pbar = tf.keras.utils.Progbar(target=None, stateful_metrics=[])
  for input in ds_train:
    images, labels = input[0], input[1]
    images, labels = pack_dtensor_inputs(
        images, labels, image_layout, label_layout)

    results.update(train_step(model, images, labels, optimizer, metrics))
    for metric_name, metric in metrics.items():
      results[metric_name] = metric.result()

    pbar.update(step, values=results.items(), finalize=False)
    step += 1
  pbar.update(step, values=results.items(), finalize=True)

  for metric in eval_metrics.values():
    metric.reset_state()
  for input in ds_test:
    images, labels = input[0], input[1]
    images, labels = pack_dtensor_inputs(
        images, labels, image_layout, label_layout)
    results.update(eval_step(model, images, labels, eval_metrics))

  for metric_name, metric in eval_metrics.items():
    results[metric_name] = metric.result()

  for metric_name, metric in results.items():
    print(f"{metric_name}: {metric.numpy()}")

class SubclassedModel(tf.keras.Model):

  def __init__(self, name=None):
    super().__init__(name=name)
    self.feature = tf.keras.layers.Dense(16)
    self.feature_2 = tf.keras.layers.Dense(24)
    self.dropout = tf.keras.layers.Dropout(0.1)

  def call(self, inputs, training=None):
    x = self.feature(inputs)
    x = self.dropout(x, training=training)
    return self.feature_2(x)

layout_map = tf.keras.dtensor.experimental.LayoutMap(mesh=mesh)

layout_map['feature.*kernel'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=2)
layout_map['feature.*bias'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)

with layout_map.scope():
  subclassed_model = SubclassedModel()

dtensor_input = dtensor.copy_to_mesh(tf.zeros((16, 16)), layout=unsharded_layout_2d)
# Trigger the weights creation for subclass model
subclassed_model(dtensor_input)

print(subclassed_model.feature.kernel.layout)

layout_map = tf.keras.dtensor.experimental.LayoutMap(mesh=mesh)

layout_map['feature.*kernel'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=2)
layout_map['feature.*bias'] = dtensor.Layout.batch_sharded(mesh, 'batch', rank=1)

with layout_map.scope():
  inputs = tf.keras.Input((16,), batch_size=16)
  x = tf.keras.layers.Dense(16, name='feature')(inputs)
  x = tf.keras.layers.Dropout(0.1)(x)
  output = tf.keras.layers.Dense(32, name='feature_2')(x)
  model = tf.keras.Model(inputs, output)

print(model.layers[1].kernel.layout)

with layout_map.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(16, name='feature', input_shape=(16,)),
      tf.keras.layers.Dropout(0.1),
      tf.keras.layers.Dense(32, name='feature_2')
  ])

print(model.layers[2].kernel.layout)

