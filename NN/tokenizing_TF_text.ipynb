{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1f4xDr_IkIk"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"tensorflow-text==2.11.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "ORcR4_JbInxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Splitter {\n",
        "  @abstractmethod\n",
        "  def split(self, input)\n",
        "}"
      ],
      "metadata": {
        "id": "Va6-RbT5Ip2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitterWithOffsets(Splitter) {\n",
        "  @abstractmethod\n",
        "  def split_with_offsets(self, input)\n",
        "}"
      ],
      "metadata": {
        "id": "24rluaTHIrpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Detokenizer {\n",
        "  @abstractmethod\n",
        "  def detokenize(self, input)\n",
        "}"
      ],
      "metadata": {
        "id": "nOnMAMkQIutV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "aT4-tJ-pI2sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "vGXHH6vNI39E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "mErGysZgI6lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
        "r = requests.get(url)\n",
        "filepath = \"vocab.txt\"\n",
        "open(filepath, 'wb').write(r.content)"
      ],
      "metadata": {
        "id": "dqNNAa3ZI9lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subtokenizer = tf_text.UnicodeScriptTokenizer(filepath)\n",
        "subtokens = tokenizer.tokenize(tokens)\n",
        "print(subtokens.to_list())"
      ],
      "metadata": {
        "id": "iXZ2b6WYI-gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.BertTokenizer(filepath, token_out_type=tf.string, lower_case=True)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "CPO8xc9hI_uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
        "sp_model = requests.get(url).content"
      ],
      "metadata": {
        "id": "4H-bFoFeJB7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.SentencepieceTokenizer(sp_model, out_type=tf.string)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "5w1DhPe5JEGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "mhtbqywXJFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")\n",
        "bigrams = tf_text.ngrams(characters, 2, reduction_type=tf_text.Reduction.STRING_JOIN, string_separator='')\n",
        "print(bigrams.to_list())"
      ],
      "metadata": {
        "id": "oDEK2gYgJGya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_HANDLE = \"https://tfhub.dev/google/zh_segmentation/1\"\n",
        "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
        "tokens = segmenter.tokenize([\"新华社北京\"])\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "-oh4pDTMJIML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_list(x):\n",
        "  if type(x) is list:\n",
        "    return list(map(decode_list, x))\n",
        "  return x.decode(\"UTF-8\")"
      ],
      "metadata": {
        "id": "3GLo9qh9JJI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_utf8_tensor(x):\n",
        "  return list(map(decode_list, x.to_list()))"
      ],
      "metadata": {
        "id": "hNTzw-D-JKkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode_utf8_tensor(tokens))"
      ],
      "metadata": {
        "id": "4jwr4ZBuJLov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strings = [\"新华社北京\"]\n",
        "labels = [[0, 1, 1, 0, 1]]\n",
        "tokenizer = tf_text.SplitMergeTokenizer()\n",
        "tokens = tokenizer.tokenize(strings, labels)\n",
        "print(decode_utf8_tensor(tokens))"
      ],
      "metadata": {
        "id": "HzbWFPz3JMqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strings = [[\"新华社北京\"]]\n",
        "labels = [[[5.0, -3.2], [0.2, 12.0], [0.0, 11.0], [2.2, -1.0], [-3.0, 3.0]]]\n",
        "tokenizer = tf_text.SplitMergeFromLogitsTokenizer()\n",
        "tokenizer.tokenize(strings, labels)\n",
        "print(decode_utf8_tensor(tokens))"
      ],
      "metadata": {
        "id": "YklQTmR4JOPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = tf_text.RegexSplitter(\"\\s\")\n",
        "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
        "print(tokens.to_list())"
      ],
      "metadata": {
        "id": "Na-ttPsWJUkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
        "print(tokens.to_list())\n",
        "print(start_offsets.to_list())\n",
        "print(end_offsets.to_list())"
      ],
      "metadata": {
        "id": "PoA_j39yJUr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())\n",
        "strings = tokenizer.detokenize(tokens)\n",
        "print(strings.numpy())"
      ],
      "metadata": {
        "id": "pjYw-cBCJVwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "iterator = iter(tokenized_docs)\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ],
      "metadata": {
        "id": "eCaJoD9OJXEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCJAoXbyJYWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}