{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c70e6aa",
      "metadata": {
        "id": "8c70e6aa"
      },
      "source": [
        "\n",
        "# Library Imports\n",
        "In this section, we import the necessary libraries for our code.\n",
        "\n",
        "- `os`: Library for operating system-related tasks.\n",
        "- `pandas`: Library for data manipulation and analysis.\n",
        "- `numpy`: Library for numerical computations.\n",
        "- `glob`: Library for file manipulation using patterns.\n",
        "- `nltk`: Python library for natural language processing (NLP).\n",
        "- `string`: Library for string operations.\n",
        "\n",
        "We also import specific modules from the NLTK library:\n",
        "- `stopwords`: Module for stop words that come with NLTK.\n",
        "- `PorterStemmer`: Module for word stemming.\n",
        "- `TweetTokenizer`: Module for tokenizing strings.\n",
        "- `re`: Regular expression library for pattern matching.\n",
        "- `random`: Library for random number generation.\n",
        "- `math`: Library for mathematical operations.\n",
        "\n",
        "## NLTK Resources Download\n",
        "We download NLTK resources using the `nltk.download('stopwords')` command. This ensures that we have access to the required stopwords for text preprocessing.\n",
        "\n",
        "Please note that detailed explanations for each library/module can be found in their respective documentation.\n",
        "\n",
        "Feel free to reach out if you have any questions or need further assistance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8803c2c6",
      "metadata": {
        "id": "8803c2c6",
        "outputId": "5e91d968-9920-4b81-9448-d9c982643d9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\abdel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os                                  # Library for operating system-related tasks\n",
        "import pandas as pd                        # Library for data manipulation and analysis\n",
        "import numpy as np                         # Library for numerical computations\n",
        "import glob                                # Library for file manipulation using patterns\n",
        "import nltk                                # Python library for natural language processing (NLP)\n",
        "import string                              # Library for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # Module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # Module for word stemming\n",
        "from nltk.tokenize import TweetTokenizer   # Module for tokenizing strings\n",
        "import re                                  # Regular expression library for pattern matching\n",
        "import random                              # Library for random number generation\n",
        "import math                                # Library for mathematical operations\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3292d537",
      "metadata": {
        "id": "3292d537"
      },
      "source": [
        "# Code Explanation\n",
        "\n",
        "## Tweet Tokenizer Instantiation\n",
        "In this section, we instantiate the `TweetTokenizer` class from NLTK for tokenizing strings that often contain characteristics unique to social media platforms like Twitter.\n",
        "\n",
        "- `preserve_case=False`: All text will be converted to lowercase for uniformity and easier analysis.\n",
        "- `strip_handles=True`: Twitter handles (user mentions) will be removed to remove user-specific information.\n",
        "- `reduce_len=True`: Repeated characters are normalized to avoid excessive elongation of words.\n",
        "\n",
        "## Stopwords and Stemming Setup\n",
        "We prepare for text preprocessing by setting up stopwords and a stemmer.\n",
        "\n",
        "- `stopwords_english`: We import the list of English stopwords from the NLTK corpus. These are common words that are often removed from text as they do not carry significant meaning.\n",
        "- `stemmer`: An instance of the PorterStemmer class is created. It is used to perform stemming, which involves reducing words to their base or root form. This aids in removing grammatical variations and simplifying analysis.\n",
        "\n",
        "Please note that these comments provide an overview of the code's functionality. For detailed information, refer to the documentation of the NLTK library.\n",
        "\n",
        "Feel free to reach out if you have any questions or need further assistance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62bbd56e",
      "metadata": {
        "id": "62bbd56e"
      },
      "outputs": [],
      "source": [
        "# Instantiate the TweetTokenizer class\n",
        "tokenizer = TweetTokenizer(\n",
        "    preserve_case=False,  # Convert all text to lowercase\n",
        "    strip_handles=True,    # Remove Twitter handles (user mentions)\n",
        "    reduce_len=True        # Normalize repeated characters\n",
        ")\n",
        "\n",
        "# Import the English stop words list from NLTK\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "# Instantiate the PorterStemmer for word stemming\n",
        "stemmer = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f054d090",
      "metadata": {
        "id": "f054d090",
        "outputId": "2d479310-d39c-4cde-cb10-7ad0ff9929a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Stop words\\n')\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb060e8",
      "metadata": {
        "id": "9cb060e8"
      },
      "source": [
        "# Function Explanation\n",
        "\n",
        "## Text Preprocessing Function\n",
        "Here, we define a function called `preprocess` that performs various text preprocessing steps on an input sample.\n",
        "\n",
        "### Arguments\n",
        "- `sample` (str): The input text to be preprocessed.\n",
        "\n",
        "### Steps\n",
        "1. Convert the input sample to lowercase for uniformity.\n",
        "2. Remove URLs using regular expression patterns.\n",
        "3. Replace certain characters like '#' and '-' with spaces.\n",
        "4. Tokenize the sample using the previously instantiated `TweetTokenizer`.\n",
        "5. Initialize an empty list, `output`, to store preprocessed tokens.\n",
        "\n",
        "### Token Processing Loop\n",
        "For each token in the tokenized sample:\n",
        "- Check conditions for including the token:\n",
        "    - Not in NLTK stopwords.\n",
        "    - Not a punctuation symbol.\n",
        "    - Not entirely numeric.\n",
        "    - Longer than one character.\n",
        "- Apply stemming using the `PorterStemmer` instance.\n",
        "- Add the stemmed token to the `output` list.\n",
        "\n",
        "### Returns\n",
        "- A list of preprocessed tokens.\n",
        "\n",
        "Please note that these comments provide an overview of the function's functionality. Refer to the documentation of the `TweetTokenizer` and `PorterStemmer` classes for more details on their specific operations.\n",
        "\n",
        "Feel free to adapt this explanation to your needs and preferences!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96aeacc7",
      "metadata": {
        "id": "96aeacc7"
      },
      "outputs": [],
      "source": [
        "def preprocess(sample):\n",
        "    \"\"\"\n",
        "    Preprocesses the input sample by performing several text transformation steps.\n",
        "\n",
        "    Args:\n",
        "        sample (str): The input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    # Convert sample to lowercase\n",
        "    sample = str(sample)\n",
        "    sample = sample.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    sample = re.sub('https?:\\/\\/.*[\\r\\n]*', ' ', sample)\n",
        "\n",
        "    # Replace certain characters with spaces\n",
        "    sample = re.sub(r'[#:-]', ' ', sample)\n",
        "\n",
        "    # Tokenize the sample\n",
        "    sample = tokenizer.tokenize(sample)\n",
        "\n",
        "    # Initialize an empty list to store preprocessed tokens\n",
        "    output = []\n",
        "\n",
        "    # Iterate through each token\n",
        "    for i in sample:\n",
        "        # Check conditions for including the token\n",
        "        if i not in stopwords_english and i not in string.punctuation and i.isnumeric() == False and len(i) > 1:\n",
        "            output.append(stemmer.stem(i))  # Perform stemming and add to output\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8c07f7",
      "metadata": {
        "id": "da8c07f7"
      },
      "source": [
        "# Code Explanation\n",
        "\n",
        "## Category Mapping Dictionary\n",
        "We start with a dictionary called `dictionary2` that maps category names to numerical codes. This dictionary is used for quick lookup and mapping between categories and their corresponding codes.\n",
        "\n",
        "- `'Household': 0`: Category \"Household\" is assigned the code 0.\n",
        "- `'Books': 1`: Category \"Books\" is assigned the code 1.\n",
        "- `'Clothing & Accessories': 2`: Category \"Clothing & Accessories\" is assigned the code 2.\n",
        "- `'Electronics': 3`: Category \"Electronics\" is assigned the code 3.\n",
        "\n",
        "## Function for Code to Category Conversion\n",
        "We define a function named `get_code2` that helps convert a numerical code back into its corresponding category name.\n",
        "\n",
        "### Arguments\n",
        "- `e` (int): The numerical code of the category.\n",
        "\n",
        "### Steps\n",
        "- Iterate through the items in the `dictionary2` using a `for` loop.\n",
        "- If the provided numerical code (`e`) matches the code in the dictionary, return the corresponding category name (`x`).\n",
        "\n",
        "### Returns\n",
        "- The name of the category corresponding to the provided numerical code.\n",
        "\n",
        "Please keep in mind that these comments offer an overview of the code's functionality. You can adapt and extend this explanation as per your specific requirements.\n",
        "\n",
        "Feel free to reach out if you have any questions or need further assistance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae2a0f2",
      "metadata": {
        "id": "9ae2a0f2"
      },
      "outputs": [],
      "source": [
        "# Mapping Dictionary\n",
        "# This dictionary maps categories to numerical codes\n",
        "dictionary = {'Household': 0, 'Books': 1, 'Clothing & Accessories': 2, 'Electronics': 3}\n",
        "\n",
        "def get_code(e):\n",
        "    \"\"\"\n",
        "    Retrieves the category name corresponding to a given numerical code.\n",
        "\n",
        "    Args:\n",
        "        e (int): The numerical code of the category.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the category.\n",
        "    \"\"\"\n",
        "    # Iterate through dictionary items to find the matching code\n",
        "    for x, y in dictionary.items():\n",
        "        if e == y:\n",
        "            return x  # Return the corresponding category name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59ac484",
      "metadata": {
        "id": "a59ac484"
      },
      "source": [
        "## Read CSV File into DataFrame\n",
        "In this section, we read a CSV file into a DataFrame using the `pd.read_csv()` function:\n",
        "\n",
        "- `r'C:\\Users\\abdel\\Downloads\\archive (2)\\ecommerceDataset.csv'`: The raw file path to the CSV file.\n",
        "- `header=None`: Indicating that there is no header row in the CSV file.\n",
        "- `names=['Y', 'X']`: Providing custom column names as 'Y' and 'X'.\n",
        "\n",
        "## Display First Few Rows\n",
        "We utilize the `.head()` method to display the initial rows of the DataFrame for a quick preview. This is a common practice to understand the data's structure and content.\n",
        "\n",
        "Please note that the `r` prefix before the file path ensures proper handling of backslashes.\n",
        "\n",
        "Feel free to adapt this explanation to your preferences and any additional details you may want to include.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aec0c66",
      "metadata": {
        "id": "7aec0c66",
        "outputId": "a4700376-90e2-46e5-8f5f-6c0a4c443202"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Household</td>\n",
              "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Household</td>\n",
              "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Household</td>\n",
              "      <td>SAF 'UV Textured Modern Art Print Framed' Pain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Household</td>\n",
              "      <td>SAF Flower Print Framed Painting (Synthetic, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Household</td>\n",
              "      <td>Incredible Gifts India Wooden Happy Birthday U...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Y                                                  X\n",
              "0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
              "1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n",
              "2  Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n",
              "3  Household  SAF Flower Print Framed Painting (Synthetic, 1...\n",
              "4  Household  Incredible Gifts India Wooden Happy Birthday U..."
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Read a CSV file into a DataFrame\n",
        "# Provide column names 'Y' and 'X' using the 'names' parameter\n",
        "df = pd.read_csv(\"/kaggle/input/ecommerce-text-classification/ecommerceDataset.csv\", header=None, names=['Y', 'X'])\n",
        "\n",
        "# Display the first few rows of the DataFrame using the .head() method\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dcaf56",
      "metadata": {
        "id": "e0dcaf56"
      },
      "source": [
        "# Data Extraction and Preparation for Training and Testing\n",
        "\n",
        "## Data Initialization\n",
        "In this section, we extract and prepare the data for training and testing.\n",
        "\n",
        "### Data Lists Initialization\n",
        "- `original_texts`: An empty list to store the original texts.\n",
        "- `original_labels`: An empty list to store the original category labels.\n",
        "\n",
        "### Data Iteration and Processing\n",
        "We iterate through each row in the DataFrame using the `iterrows()` function:\n",
        "- Extract the text value from the row as `text_value`.\n",
        "- Append the `text_value` to the `original_texts` list.\n",
        "- Get the numerical category label from the dictionary and append it to the `original_labels` list.\n",
        "\n",
        "### Data Combination and Shuffling\n",
        "- Combine the `original_texts` and `original_labels` using `zip()` to create `data_combined`.\n",
        "- Randomly shuffle the `data_combined` list to ensure a balanced distribution of data.\n",
        "\n",
        "### Data Separation\n",
        "- Unpack the shuffled `data_combined` list into separate lists for original texts and original labels.\n",
        "\n",
        "### Data Splitting for Training and Testing\n",
        "- Split the data into training and testing sets:\n",
        "  - `train_x`: An array containing 90% of the original texts for training.\n",
        "  - `train_y`: An array containing 90% of the original labels for training.\n",
        "  - `test_x`: An array containing the remaining 10% of original texts for testing.\n",
        "  - `test_y`: An array containing the remaining 10% of original labels for testing.\n",
        "\n",
        "Please adjust this explanation according to your preferences and any additional details you wish to include.\n",
        "\n",
        "If you have any questions or need further assistance, feel free to ask!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cec5325",
      "metadata": {
        "id": "0cec5325"
      },
      "outputs": [],
      "source": [
        "# Extracting and Preparing Data for Training and Testing\n",
        "\n",
        "# Initialize lists to store data\n",
        "original_texts = []\n",
        "original_labels = []\n",
        "\n",
        "# Iterate through each row in the DataFrame using iterrows()\n",
        "for index, row in df.iterrows():\n",
        "    # Extract the text value from the row\n",
        "    text_value = row[1]\n",
        "\n",
        "    # Append the text value to the original texts list\n",
        "    original_texts.append(text_value)\n",
        "\n",
        "    # Get the numerical category label from the dictionary and append to the original labels list\n",
        "    original_labels.append(dictionary2[row[0]])\n",
        "\n",
        "# Combine the original texts and labels using zip()\n",
        "data_combined = list(zip(original_texts, original_labels))\n",
        "# The combined list looks like: [('apples', 50), ('grapes', 40), ...]\n",
        "\n",
        "# Randomly shuffle the combined list\n",
        "random.shuffle(data_combined)\n",
        "\n",
        "# Unpack the shuffled combined list into separate lists\n",
        "original_texts = [item[0] for item in data_combined]\n",
        "original_labels = [item[1] for item in data_combined]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_x = np.array(original_texts[:int(len(original_texts) * 0.9)])\n",
        "train_y = np.array(original_labels[:int(len(original_labels) * 0.9)])\n",
        "\n",
        "test_x = np.array(original_texts[int(len(original_texts) * 0.9):])\n",
        "test_y = np.array(original_labels[int(len(original_labels) * 0.9):])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003088c3",
      "metadata": {
        "id": "003088c3"
      },
      "source": [
        "# Function Explanation\n",
        "\n",
        "## Frequency Dictionary Builder\n",
        "Here, we define a function named `build_freqs` that constructs a frequency dictionary for words in tweets categorized by sentiment labels.\n",
        "\n",
        "### Arguments\n",
        "- `tweets` (list): A list of preprocessed tweets.\n",
        "- `ys` (numpy.ndarray or list): An array of sentiment labels corresponding to the tweets.\n",
        "\n",
        "### Steps\n",
        "1. Convert the sentiment labels array `ys` to a list named `yslist` using `np.squeeze(ys).tolist()`. This step ensures compatibility with the subsequent use of `zip()`.\n",
        "2. Initialize an empty dictionary named `freqs` to store word-sentiment frequency pairs.\n",
        "\n",
        "### Loop through Data\n",
        "- Iterate through each sentiment label `y` and its corresponding tweet using `zip(yslist, tweets)`:\n",
        "    - Preprocess the current tweet using the `preprocess()` function.\n",
        "    - Loop through the processed words in the tweet:\n",
        "        - Create a pair comprising the word and its sentiment label.\n",
        "        - Increment the frequency count for the pair in the `freqs` dictionary.\n",
        "\n",
        "### Returns\n",
        "- A dictionary containing word-sentiment label pairs as keys and their respective frequencies as values.\n",
        "\n",
        "Please adjust this explanation according to your preferences and any additional details you wish to include.\n",
        "\n",
        "If you have any questions or need further assistance, feel free to ask!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d382a27a",
      "metadata": {
        "id": "d382a27a"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"\n",
        "    Builds a frequency dictionary for words in tweets categorized by sentiment labels.\n",
        "\n",
        "    Args:\n",
        "        tweets (list): List of preprocessed tweets.\n",
        "        ys (numpy.ndarray or list): Sentiment labels corresponding to the tweets.\n",
        "\n",
        "    Returns:\n",
        "        dict: A frequency dictionary with word-sentiment label pairs as keys and frequencies as values.\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Initialize an empty dictionary to store word-sentiment frequency pairs\n",
        "    freqs = {}\n",
        "\n",
        "    # Loop over sentiment labels and their corresponding tweets using zip\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        # Preprocess the current tweet and loop over processed words\n",
        "        for word in preprocess(tweet):\n",
        "            # Create a pair of the word and its sentiment label\n",
        "            pair = (word, y)\n",
        "            # Increment the frequency count for the pair\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a48c36",
      "metadata": {
        "id": "38a48c36"
      },
      "outputs": [],
      "source": [
        "freq = build_freqs(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca2314b",
      "metadata": {
        "id": "cca2314b"
      },
      "source": [
        "# Naive Bayes Classifier Training\n",
        "\n",
        "## Naive Bayes Classifier Training Function\n",
        "In this section, we define a function named `train_naive_bayes` that trains a Naive Bayes classifier using frequency-based features.\n",
        "\n",
        "### Arguments\n",
        "- `freqs` (dict): A frequency dictionary of word-sentiment label pairs.\n",
        "- `train_x` (numpy.ndarray): Training data containing preprocessed texts.\n",
        "- `train_y` (numpy.ndarray): Training data containing sentiment labels.\n",
        "\n",
        "### Initialization\n",
        "We start by initializing dictionaries to store log-likelihoods and prior probabilities for each category:\n",
        "- `loglikelihood_Household`, `loglikelihood_Books`, `loglikelihood_Cloth_Access`, and `loglikelihood_Electronics`.\n",
        "\n",
        "### Vocabulary and Data Statistics\n",
        "- Create a vocabulary set `vocab` from the keys of the frequency dictionary.\n",
        "- Calculate the total number of documents `D` in the training data.\n",
        "- Calculate the number of documents for each category `D_i` using `sum(train_y == i)`.\n",
        "\n",
        "### Word Count Calculation\n",
        "We iterate through each word-sentiment label pair in the frequency dictionary and calculate the word count for each category:\n",
        "- Increment the word count variables `N_i` for each category based on the sentiment label.\n",
        "\n",
        "### Log-Prior Probability Calculation\n",
        "- Calculate log-prior probabilities for each category using the formula:\n",
        "    - `logprior_i = log(D_i) - log(D_sum_except_i)`\n",
        "\n",
        "### Word Probability Calculation\n",
        "- For each word in the vocabulary, calculate the probability of the word occurring in each category:\n",
        "    - Calculate `p_w_i` as `(freq_i + 1) / (N_i + V)` and `p_w_neg_i` as `(freq_sum_except_i + 1) / (N_sum_except_i + V)`.\n",
        "\n",
        "### Log-Likelihood Calculation\n",
        "- Calculate log-likelihoods for each word and category using the formula:\n",
        "    - `loglikelihood_i[word] = log(p_w_i) - log(p_w_neg_i)`\n",
        "\n",
        "### Return\n",
        "- The function returns the calculated log-prior probabilities and log-likelihoods for each category.\n",
        "\n",
        "Please adapt this explanation according to your preferences and any additional details you wish to include.\n",
        "\n",
        "If you have any questions or require further clarification, feel free to ask!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466a7218",
      "metadata": {
        "id": "466a7218"
      },
      "outputs": [],
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    \"\"\"\n",
        "    Train a Naive Bayes classifier using frequency-based features.\n",
        "\n",
        "    Args:\n",
        "        freqs (dict): A frequency dictionary of word-sentiment label pairs.\n",
        "        train_x (numpy.ndarray): Training data containing preprocessed texts.\n",
        "        train_y (numpy.ndarray): Training data containing sentiment labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing calculated prior probabilities and log-likelihoods for each category.\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries for log-likelihoods and prior probabilities\n",
        "    loglikelihood_Household = {}\n",
        "    loglikelihood_Books = {}\n",
        "    loglikelihood_Cloth_Access = {}\n",
        "    loglikelihood_Electronics = {}\n",
        "\n",
        "    # Create a vocabulary set from the frequency dictionary keys\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "\n",
        "    # Calculate the total number of documents (D)\n",
        "    D = len(train_y)\n",
        "\n",
        "    # Calculate the number of documents for each category (D_i)\n",
        "    D_Household = sum(train_y == 0)\n",
        "    D_Books = sum(train_y == 1)\n",
        "    D_Cloth_Access = sum(train_y == 2)\n",
        "    D_Electronics = sum(train_y == 3)\n",
        "\n",
        "    # Initialize word count variables for each category\n",
        "    N_Household = N_Books = N_Cloth_Access = N_Electronics = 0\n",
        "\n",
        "    # Loop through each word-sentiment label pair in the frequency dictionary\n",
        "    for pair in freqs.keys():\n",
        "        # If the sentiment label is for the \"Household\" category (positive)\n",
        "        if pair[1] == 0:\n",
        "            # Increment the word count for the \"Household\" category\n",
        "            N_Household += freqs.get(pair)\n",
        "        # Else if the sentiment label is for the \"Books\" category (negative)\n",
        "        elif pair[1] == 1:\n",
        "            # Increment the word count for the \"Books\" category\n",
        "            N_Books += freqs.get(pair)\n",
        "        # Else if the sentiment label is for the \"Clothing & Accessories\" category\n",
        "        elif pair[1] == 2:\n",
        "            # Increment the word count for the \"Clothing & Accessories\" category\n",
        "            N_Cloth_Access += freqs.get(pair)\n",
        "        # Else, the sentiment label is for the \"Electronics\" category\n",
        "        else:\n",
        "            # Increment the word count for the \"Electronics\" category\n",
        "            N_Electronics += freqs.get(pair)\n",
        "\n",
        "    # Calculate log-prior probabilities for each category\n",
        "    logprior_Household = np.log(D_Household) - np.log((D_Books + D_Cloth_Access + D_Electronics))\n",
        "    logprior_Books = np.log(D_Books) - np.log((D_Household + D_Cloth_Access + D_Electronics))\n",
        "    logprior_Cloth_Access = np.log(D_Cloth_Access) - np.log((D_Household + D_Books + D_Electronics))\n",
        "    logprior_Electronics = np.log(D_Electronics) - np.log((D_Household + D_Books + D_Cloth_Access))\n",
        "\n",
        "    # Loop through each word in the vocabulary\n",
        "    for word in vocab:\n",
        "        # Get the frequency of the word for each category\n",
        "        freq_Household = freqs.get((word, 0), 0)\n",
        "        freq_Books = freqs.get((word, 1), 0)\n",
        "        freq_Cloth_Access = freqs.get((word, 2), 0)\n",
        "        freq_Electronics = freqs.get((word, 3), 0)\n",
        "\n",
        "        # Calculate probabilities for each word being in its respective category\n",
        "        p_w_Household = (freq_Household + 1) / (N_Household + V)\n",
        "        p_w_neg_Household = (freq_Books + freq_Cloth_Access + freq_Electronics + 1) / (N_Books + N_Cloth_Access + N_Electronics + V)\n",
        "\n",
        "        p_w_Books = (freq_Books + 1) / (N_Books + V)\n",
        "        p_w_neg_Books = (freq_Household + freq_Cloth_Access + freq_Electronics + 1) / (N_Household + N_Cloth_Access + N_Electronics + V)\n",
        "\n",
        "        p_w_Cloth_Access = (freq_Cloth_Access + 1) / (N_Cloth_Access + V)\n",
        "        p_w_neg_Cloth_Access = (freq_Household + freq_Books + freq_Electronics + 1) / (N_Household + N_Books + N_Electronics + V)\n",
        "\n",
        "        p_w_Electronics = (freq_Electronics + 1) / (N_Electronics + V)\n",
        "        p_w_neg_Electronics = (freq_Household + freq_Books + freq_Cloth_Access + 1) / (N_Household + N_Books + N_Cloth_Access + V)\n",
        "\n",
        "        # Calculate log-likelihoods for each word and category\n",
        "        loglikelihood_Household[word] = np.log(p_w_Household) - np.log(p_w_neg_Household)\n",
        "        loglikelihood_Books[word] = np.log(p_w_Books) - np.log(p_w_neg_Books)\n",
        "        loglikelihood_Cloth_Access[word] = np.log(p_w_Cloth_Access) - np.log(p_w_neg_Cloth_Access)\n",
        "        loglikelihood_Electronics[word] = np.log(p_w_Electronics) - np.log(p_w_neg_Electronics)\n",
        "\n",
        "    # Return calculated log-prior probabilities and log-likelihoods for each category\n",
        "    return logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56a1267",
      "metadata": {
        "id": "c56a1267"
      },
      "outputs": [],
      "source": [
        "logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics =  train_naive_bayes(freq, train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146fc8ae",
      "metadata": {
        "id": "146fc8ae",
        "outputId": "178a5b8b-1e71-4135-e24f-1f5707b6b79d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.47648959611803043,\n",
              " -1.1865235391083715,\n",
              " -1.571627607134717,\n",
              " -1.3185908170882747)"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3eac324",
      "metadata": {
        "id": "c3eac324"
      },
      "source": [
        "# Naive Bayes Prediction Function\n",
        "\n",
        "## Naive Bayes Prediction Function\n",
        "This section defines a function named `naive_bayes_predict` that predicts the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.\n",
        "\n",
        "### Arguments\n",
        "- `tweet` (str): The preprocessed tweet for which sentiment label probabilities are predicted.\n",
        "- `logprior_Household` (float): Log-prior probability for the \"Household\" category.\n",
        "- `logprior_Books` (float): Log-prior probability for the \"Books\" category.\n",
        "- `logprior_Cloth_Access` (float): Log-prior probability for the \"Clothing & Accessories\" category.\n",
        "- `logprior_Electronics` (float): Log-prior probability for the \"Electronics\" category.\n",
        "- `loglikelihood_Household` (dict): Log-likelihoods for words in the \"Household\" category.\n",
        "- `loglikelihood_Books` (dict): Log-likelihoods for words in the \"Books\" category.\n",
        "- `loglikelihood_Cloth_Access` (dict): Log-likelihoods for words in the \"Clothing & Accessories\" category.\n",
        "- `loglikelihood_Electronics` (dict): Log-likelihoods for words in the \"Electronics\" category.\n",
        "\n",
        "### Steps\n",
        "- Preprocess the input `tweet` to obtain a list of words.\n",
        "- Initialize probabilities for each category to zero:\n",
        "    - `p_Household`, `p_Books`, `p_Cloth_Access`, `p_Electronics`.\n",
        "- Add the corresponding log-priors to the probabilities.\n",
        "- Loop through each word in the preprocessed tweet:\n",
        "    - Check if the word exists in the log-likelihood dictionaries.\n",
        "    - If found, add the log-likelihood of that word to the corresponding probability.\n",
        "\n",
        "### Return\n",
        "- The function returns a tuple containing the calculated sum of log-likelihoods plus the corresponding log-prior for each category:\n",
        "    - `p_Household`, `p_Books`, `p_Cloth_Access`, `p_Electronics`.\n",
        "\n",
        "Please adapt this explanation according to your preferences and any additional details you wish to include.\n",
        "\n",
        "If you have any questions or need further assistance, feel free to ask!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e083616a",
      "metadata": {
        "id": "e083616a"
      },
      "outputs": [],
      "source": [
        "def naive_bayes_predict(tweet, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics,\n",
        "                        loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics):\n",
        "    \"\"\"\n",
        "    Predict the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The preprocessed tweet to predict the sentiment label probabilities for.\n",
        "        logprior_Household (float): Log-prior probability for the \"Household\" category.\n",
        "        logprior_Books (float): Log-prior probability for the \"Books\" category.\n",
        "        logprior_Cloth_Access (float): Log-prior probability for the \"Clothing & Accessories\" category.\n",
        "        logprior_Electronics (float): Log-prior probability for the \"Electronics\" category.\n",
        "        loglikelihood_Household (dict): Log-likelihoods for words in the \"Household\" category.\n",
        "        loglikelihood_Books (dict): Log-likelihoods for words in the \"Books\" category.\n",
        "        loglikelihood_Cloth_Access (dict): Log-likelihoods for words in the \"Clothing & Accessories\" category.\n",
        "        loglikelihood_Electronics (dict): Log-likelihoods for words in the \"Electronics\" category.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The calculated sum of log-likelihoods for each category plus the corresponding log-priors.\n",
        "    \"\"\"\n",
        "    # Process the tweet to get a list of words\n",
        "    word_l = preprocess(tweet)\n",
        "\n",
        "    # Initialize probabilities for each category\n",
        "    p_Household = 0\n",
        "    p_Books = 0\n",
        "    p_Cloth_Access = 0\n",
        "    p_Electronics = 0\n",
        "\n",
        "    # Add the log-priors to the probabilities\n",
        "    p_Household += logprior_Household\n",
        "    p_Books += logprior_Books\n",
        "    p_Cloth_Access += logprior_Cloth_Access\n",
        "    p_Electronics += logprior_Electronics\n",
        "\n",
        "    # Loop through each word in the processed tweet\n",
        "    for word in word_l:\n",
        "\n",
        "        # Check if the word exists in the loglikelihood dictionaries\n",
        "        if word in loglikelihood_Household:\n",
        "            # Add the log-likelihood of that word to the corresponding probability\n",
        "            p_Household += loglikelihood_Household[word]\n",
        "            p_Books += loglikelihood_Books[word]\n",
        "            p_Cloth_Access += loglikelihood_Cloth_Access[word]\n",
        "            p_Electronics += loglikelihood_Electronics[word]\n",
        "\n",
        "    # Return the calculated log-likelihood sums plus their respective log-priors\n",
        "    return p_Household, p_Books, p_Cloth_Access, p_Electronics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a64f96",
      "metadata": {
        "id": "81a64f96"
      },
      "source": [
        "# Naive Bayes Classifier Testing\n",
        "\n",
        "## Naive Bayes Classifier Testing Function\n",
        "In this section, we define a function named `test_naive_bayes` that evaluates the performance of a trained Naive Bayes classifier on a test dataset.\n",
        "\n",
        "### Arguments\n",
        "- `test_x` (list): A list of preprocessed tweets for testing.\n",
        "- `test_y` (list): The corresponding true labels for the test tweets.\n",
        "- `logprior_Household` (float): Log-prior probability for the \"Household\" category.\n",
        "- `logprior_Books` (float): Log-prior probability for the \"Books\" category.\n",
        "- `logprior_Cloth_Access` (float): Log-prior probability for the \"Clothing & Accessories\" category.\n",
        "- `logprior_Electronics` (float): Log-prior probability for the \"Electronics\" category.\n",
        "- `loglikelihood_Household` (dict): Log-likelihoods for words in the \"Household\" category.\n",
        "- `loglikelihood_Books` (dict): Log-likelihoods for words in the \"Books\" category.\n",
        "- `loglikelihood_Cloth_Access` (dict): Log-likelihoods for words in the \"Clothing & Accessories\" category.\n",
        "- `loglikelihood_Electronics` (dict): Log-likelihoods for words in the \"Electronics\" category.\n",
        "\n",
        "### Steps\n",
        "- Initialize an accuracy variable to 0.\n",
        "- Create an empty list `y_hats` to store predicted labels for each test tweet.\n",
        "- Initialize a counter `o` for tracking the tweet index.\n",
        "- Iterate through each tweet in the test data:\n",
        "    - Predict the sentiment label using the trained Naive Bayes classifier for each category.\n",
        "    - Create a list of predicted class probabilities.\n",
        "    - Append the index of the predicted class with the highest probability to the `y_hats` list.\n",
        "    - Optionally, print the predicted class, real class, and probabilities for debugging purposes.\n",
        "    - Increment the tweet index counter `o`.\n",
        "- Calculate the error as the average absolute difference between predicted and true labels.\n",
        "- Calculate the accuracy as 1 minus the error.\n",
        "\n",
        "### Return\n",
        "- The function returns the calculated accuracy of the Naive Bayes classifier on the test dataset.\n",
        "\n",
        "Feel free to customize and expand upon this Markdown structure as needed to align with your requirements and style.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e87d49b",
      "metadata": {
        "id": "5e87d49b"
      },
      "outputs": [],
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics, loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics):\n",
        "    \"\"\"\n",
        "    Test the performance of a trained Naive Bayes classifier on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        test_x (list): A list of preprocessed tweets for testing.\n",
        "        test_y (list): The corresponding true labels for the test tweets.\n",
        "        logprior_Household (float): Log-prior probability for the \"Household\" category.\n",
        "        logprior_Books (float): Log-prior probability for the \"Books\" category.\n",
        "        logprior_Cloth_Access (float): Log-prior probability for the \"Clothing & Accessories\" category.\n",
        "        logprior_Electronics (float): Log-prior probability for the \"Electronics\" category.\n",
        "        loglikelihood_Household (dict): Log-likelihoods for words in the \"Household\" category.\n",
        "        loglikelihood_Books (dict): Log-likelihoods for words in the \"Books\" category.\n",
        "        loglikelihood_Cloth_Access (dict): Log-likelihoods for words in the \"Clothing & Accessories\" category.\n",
        "        loglikelihood_Electronics (dict): Log-likelihoods for words in the \"Electronics\" category.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the Naive Bayes classifier on the test dataset.\n",
        "    \"\"\"\n",
        "    accuracy = 0  # Initialize accuracy variable\n",
        "    y_hats = []  # Initialize a list to store predicted labels for each test tweet\n",
        "    for tweet in test_x:\n",
        "        # Predict the sentiment label using the trained Naive Bayes classifier\n",
        "        p_Household, p_Books, p_Cloth_Access, p_Electronics = naive_bayes_predict(tweet, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics, loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics)\n",
        "\n",
        "        # Create a list of predicted class probabilities\n",
        "        l = list([p_Household, p_Books, p_Cloth_Access, p_Electronics])\n",
        "\n",
        "        # Append the index of the predicted class with the highest probability to y_hats\n",
        "        y_hats.append(np.argmax(l))\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate the error as the average absolute difference between predicted and true labels\n",
        "    error = sum(np.abs(y_hats - np.squeeze(test_y))) / len(y_hats)\n",
        "\n",
        "    # Calculate accuracy as 1 minus the error\n",
        "    accuracy = 1 - error\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55379fb",
      "metadata": {
        "id": "d55379fb",
        "outputId": "2730fb75-5077-49ea-c7e8-6286e7428c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9160\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(train_x,train_y,logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e926f53a",
      "metadata": {
        "id": "e926f53a",
        "outputId": "db64712d-e17c-4f77-9c21-c1a5bb722218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9070\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y,logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a248aa2",
      "metadata": {
        "id": "4a248aa2"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "\n",
        "## Class: NaiveBayesClassifier\n",
        "This class implements a Naive Bayes classifier for sentiment analysis using frequency-based features.\n",
        "\n",
        "### Method: train\n",
        "Train the Naive Bayes classifier using frequency-based features.\n",
        "\n",
        "**Arguments:**\n",
        "- `freqs` (dict): A frequency dictionary of word-sentiment label pairs.\n",
        "- `train_x` (numpy.ndarray): Training data containing preprocessed texts.\n",
        "- `train_y` (numpy.ndarray): Training data containing sentiment labels.\n",
        "\n",
        "**Returns:**\n",
        "- A tuple containing calculated prior probabilities and log-likelihoods for each category.\n",
        "\n",
        "### Method: predict\n",
        "Predict the sentiment label probabilities of a given tweet using the trained Naive Bayes classifier.\n",
        "\n",
        "**Arguments:**\n",
        "- `tweet` (str): The preprocessed tweet to predict the sentiment label probabilities for.\n",
        "\n",
        "**Returns:**\n",
        "- A list containing the calculated log-likelihood sums for each category.\n",
        "\n",
        "### Method: test\n",
        "Test the performance of the trained Naive Bayes classifier on the test dataset.\n",
        "\n",
        "**Arguments:**\n",
        "- `test_x` (list): A list of preprocessed tweets for testing.\n",
        "- `test_y` (list): The corresponding true labels for the test tweets.\n",
        "\n",
        "**Returns:**\n",
        "- Accuracy of the Naive Bayes classifier on the test dataset.\n",
        "\n",
        "### Workflow\n",
        "1. In the `train` method:\n",
        "   - Create vocabulary set and calculate the total number of documents (D).\n",
        "   - For each sentiment label:\n",
        "     - Calculate the number of documents for that label (D_label).\n",
        "     - Calculate log-prior probability for that label.\n",
        "     - Loop through each word-sentiment label pair:\n",
        "       - Increment word counts for each label.\n",
        "       - Calculate and store log-likelihood for each word and label.\n",
        "   - Return calculated log-prior probabilities and log-likelihoods.\n",
        "\n",
        "2. In the `predict` method:\n",
        "   - Process input tweet to get list of words.\n",
        "   - Loop through each label:\n",
        "     - Initialize probability for that label.\n",
        "     - Add log-prior probability.\n",
        "     - Loop through each word in the tweet:\n",
        "       - If word exists in log-likelihood, add its log-likelihood to probability.\n",
        "     - Append probability to label_probs list.\n",
        "   - Return label_probs list.\n",
        "\n",
        "3. In the `test` method:\n",
        "   - Initialize accuracy and y_hats lists.\n",
        "   - For each test tweet:\n",
        "     - Predict label probabilities using the `predict` method.\n",
        "     - Get the index of the predicted label with highest probability.\n",
        "     - Append predicted label to y_hats.\n",
        "   - Calculate accuracy as 1 minus the average absolute difference between predicted and true labels.\n",
        "   - Return accuracy.\n",
        "\n",
        "Please adapt this explanation according to your preferences and any additional details you wish to include.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ed1d39",
      "metadata": {
        "id": "08ed1d39"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def train(self, freqs, train_x, train_y):\n",
        "        \"\"\"\n",
        "        Train a Naive Bayes classifier using frequency-based features.\n",
        "\n",
        "        Args:\n",
        "            freqs (dict): A frequency dictionary of word-sentiment label pairs.\n",
        "            train_x (numpy.ndarray): Training data containing preprocessed texts.\n",
        "            train_y (numpy.ndarray): Training data containing sentiment labels.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing calculated prior probabilities and log-likelihoods for each category.\n",
        "        \"\"\"\n",
        "        self.logprior = {}\n",
        "        self.loglikelihood = {}\n",
        "\n",
        "        # Create a vocabulary set from the frequency dictionary keys\n",
        "        vocab = set([pair[0] for pair in freqs.keys()])\n",
        "        V = len(vocab)\n",
        "\n",
        "        # Calculate the total number of documents (D)\n",
        "        D = len(train_y)\n",
        "\n",
        "        # Calculate the number of documents for each category (D_i)\n",
        "        for label in np.unique(train_y):\n",
        "            D_label = sum(train_y == label)\n",
        "            self.logprior[label] = np.log(D_label) - np.log(D - D_label)\n",
        "\n",
        "            # Initialize word count variables for each category\n",
        "            N_label = 0\n",
        "            N_nonlabel=0\n",
        "            # Loop through each word-sentiment label pair in the frequency dictionary\n",
        "            for pair in freqs.keys():\n",
        "                if pair[1] == label:\n",
        "                    N_label += freqs.get(pair)\n",
        "                else:\n",
        "                    N_nonlabel+=freqs.get(pair)\n",
        "\n",
        "\n",
        "\n",
        "            self.loglikelihood[label] = {}\n",
        "            for word in vocab:\n",
        "                freq_label = freqs.get((word, label), 0)\n",
        "                freq_nonlabel = sum([freqs.get((word, other_label), 0) for other_label in np.unique(train_y) if other_label != label])\n",
        "                p_w_label = (freq_label + 1) / (N_label + V)\n",
        "                p_w_nonlabel = (freq_nonlabel + 1) / (N_nonlabel + V)\n",
        "                self.loglikelihood[label][word] = np.log(p_w_label) - np.log(p_w_nonlabel)\n",
        "\n",
        "        return self.logprior, self.loglikelihood\n",
        "\n",
        "    def predict(self, tweet):\n",
        "        \"\"\"\n",
        "        Predict the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.\n",
        "\n",
        "        Args:\n",
        "            tweet (str): The preprocessed tweet to predict the sentiment label probabilities for.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the calculated log-likelihood sums for each category.\n",
        "        \"\"\"\n",
        "        word_l = preprocess(tweet)\n",
        "        label_probs = []\n",
        "\n",
        "        for label in self.logprior.keys():\n",
        "            p_label = 0\n",
        "            p_label += self.logprior[label]\n",
        "\n",
        "            for word in word_l:\n",
        "                if word in self.loglikelihood[label]:\n",
        "                    p_label += self.loglikelihood[label][word]\n",
        "\n",
        "            label_probs.append(p_label)\n",
        "\n",
        "        return label_probs\n",
        "\n",
        "    def test(self, test_x, test_y):\n",
        "        \"\"\"\n",
        "        Test the performance of a trained Naive Bayes classifier on the test dataset.\n",
        "\n",
        "        Args:\n",
        "            test_x (list): A list of preprocessed tweets for testing.\n",
        "            test_y (list): The corresponding true labels for the test tweets.\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy of the Naive Bayes classifier on the test dataset.\n",
        "        \"\"\"\n",
        "        accuracy = 0\n",
        "        y_hats = []\n",
        "\n",
        "        for tweet in test_x:\n",
        "            label_probs = self.predict(tweet)\n",
        "            predicted_label = np.argmax(label_probs)\n",
        "\n",
        "            y_hats.append(predicted_label)\n",
        "\n",
        "        error = sum(np.abs(y_hats - np.squeeze(test_y))) / len(y_hats)\n",
        "        accuracy = 1 - error\n",
        "\n",
        "        return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba90a920",
      "metadata": {
        "id": "ba90a920"
      },
      "outputs": [],
      "source": [
        "# Instantiate the NaiveBayesClassifier class\n",
        "nb_classifier = NaiveBayesClassifier()\n",
        "\n",
        "# Train the classifier using your training data and frequency dictionary\n",
        "logprior, loglikelihood = nb_classifier.train(freq, train_x, train_y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb4cc742",
      "metadata": {
        "id": "cb4cc742",
        "outputId": "465ba6e1-2eed-488f-9d9f-2cbbb18ac64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier accuracy on test data: 0.9160019390947953\n"
          ]
        }
      ],
      "source": [
        "# Example test data for testing the classifier's performance\n",
        "test_accuracy = nb_classifier.test(train_x, train_y)\n",
        "print(\"Classifier accuracy on test data:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb09b52a",
      "metadata": {
        "id": "eb09b52a",
        "outputId": "fa608953-87d2-45bc-fbbc-a76242fa9b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier accuracy on test data: 0.9069998017053341\n"
          ]
        }
      ],
      "source": [
        "# Example test data for testing the classifier's performance\n",
        "test_accuracy = nb_classifier.test(test_x,test_y)\n",
        "print(\"Classifier accuracy on test data:\", test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}