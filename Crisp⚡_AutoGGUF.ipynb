{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD24jJxq7t3k"
      },
      "outputs": [],
      "source": [
        "# @title # ‚ö° AutoQuant\n",
        "\n",
        "# @markdown > üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "# @markdown ‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne) with a few minor adaptions. (I only kept the GGUF part, because i only use that. See his versions for more functionality.)\n",
        "\n",
        "# @markdown **Usage:** Download the model by **running this cell** and then run the cells corresponding to your quantization methods of interest.\n",
        "\n",
        "# @markdown To quantize a 7B model, GGUF only needs a T4 GPU.\n",
        "\n",
        "# @markdown *See also the [AutoQuantize](https://colab.research.google.com/drive/1Li3USnl3yoYctqJLtYux3LAIy4Bnnv3J) notebook from zainulabideen.*\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## ü§ó Download model (required)\n",
        "# @markdown `HF_TOKEN` corresponds to the name of the secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens) in Colab.\n",
        "\n",
        "MODEL_ID = \"cstr/phi3-mini-4k-llamafied-sft-v5\" # @param {type:\"string\"}\n",
        "USERNAME = \"cstr\" # @param {type:\"string\"}\n",
        "HF_TOKEN = \"HF_TOK\" # @param {type:\"string\"}\n",
        "\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "\n",
        "# Download model\n",
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import create_repo, HfApi, ModelCard, snapshot_download\n",
        "from google.colab import userdata, runtime\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = userdata.get(HF_TOKEN)\n",
        "\n",
        "#import subprocess\n",
        "#subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_token])\n",
        "\n",
        "savepath = f'/content/{MODEL_NAME}'\n",
        "outpath = snapshot_download(repo_id=MODEL_ID, revision=\"main\", cache_dir=savepath, token=hf_token)\n",
        "print(f\"Model downloaded to: {outpath}\")\n",
        "\n",
        "#!git lfs install\n",
        "#!git clone --depth 1 https://{USERNAME}:{hf_token}@huggingface.co/{MODEL_ID}\n",
        "\n",
        "api = HfApi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL0yGhbe3EFk"
      },
      "outputs": [],
      "source": [
        "# @title ## üß© GGUF\n",
        "\n",
        "# @markdown Quantization methods: `q2_k`, `q3_k_l`, `q3_k_m`, `q3_k_s`, `q4_0`, `q4_1`, `q4_k_m`, `q4_k_s`, `q5_0`, `q5_1`, `q5_k_m`, `q5_k_s`, `q6_k`, `q8_0`\n",
        "\n",
        "# @markdown Learn more about GGUF and quantization methods in [this article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html).\n",
        "# @markdown For Llama3, you must select PAD_VOCAB and set VOCAB_TYPE to bpe\n",
        "\n",
        "QUANTIZATION_FORMAT = \"q4_k_m\" # @param {type:\"string\"}\n",
        "PAD_VOCAB = False # @param {type: \"boolean\"}\n",
        "VOCAB_TYPE = \"\" # @param [\"\", \"bpe\", \"\"]\n",
        "QUANTIZATION_METHODS = QUANTIZATION_FORMAT.replace(\" \", \"\").split(\",\")\n",
        "\n",
        "PAD_V = \"\"\n",
        "if PAD_VOCAB:\n",
        "  PAD_V = \"--pad-vocab\"\n",
        "\n",
        "V_TYPE = \"\"\n",
        "if VOCAB_TYPE != \"\":\n",
        "  V_TYPE = \"--vocab-type \" + VOCAB_TYPE\n",
        "\n",
        "# Install llama.cpp\n",
        "#!git clone https://github.com/ggerganov/llama.cpp\n",
        "#!cd llama.cpp && make\n",
        "# !LLAMA_CUBLAS=1 make\n",
        "#!pip install -r llama.cpp/requirements.txt\n",
        "\n",
        "# Convert to fp16\n",
        "#fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "#!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16} {V_TYPE} {PAD_V}\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "!apt update && apt install gh -y\n",
        "\n",
        "# Function to download and unzip binaries\n",
        "def download_and_prepare_binaries(github_token, repo, release_tag, asset_name, target_path):\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path, exist_ok=True)\n",
        "    # Login to GitHub CLI\n",
        "    !echo {github_token} | gh auth login --with-token\n",
        "    # Download release assets using GitHub CLI\n",
        "    !gh release download {release_tag} --repo {repo} --pattern {asset_name} --dir {target_path}\n",
        "    # Unzip the downloaded assets\n",
        "    !unzip {target_path}/{asset_name} -d {target_path}\n",
        "\n",
        "# GitHub repository and release information\n",
        "github_token = userdata.get('github_repo')  # Get the GitHub token from Colab's secrets\n",
        "repo = 'CrispStrobe/llama_cpp'  # GitHub repo where binaries are stored\n",
        "release_tag = 'b2749'  # Tag of the release containing the binaries\n",
        "asset_name = 'llama_cpp_binaries.zip'  # Name of the asset to download\n",
        "binaries_path = '/content/llama.cpp'  # Local path to store binaries\n",
        "\n",
        "# Download and prepare binaries if not already present\n",
        "download_and_prepare_binaries(github_token, repo, release_tag, asset_name, binaries_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Proceed with using the binaries\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!{binaries_path}/convert.py {outpath}/ --outtype f16 --outfile {fp16} {V_TYPE} {PAD_V}\n",
        "\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !{binaries_path}/quantize {fp16} {qtype} {method}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}\n",
        "\n",
        "# Create model card\n",
        "card = ModelCard.load(MODEL_ID)\n",
        "card.data.tags.append(\"autoquant\")\n",
        "card.data.tags.append(\"gguf\")\n",
        "card.save(f'{MODEL_NAME}/README.md')\n",
        "\n",
        "# Upload model\n",
        "create_repo(\n",
        "    repo_id = f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=[\"*.gguf\",\"$.md\"],\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "iotaaL8yzBrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZe6zHw2f4ac"
      },
      "outputs": [],
      "source": [
        "#!cd /content/llama.cpp && zip -r llama_cpp_binaries.zip . # adjust path to include only necessary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2_ft-ELgO3s"
      },
      "outputs": [],
      "source": [
        "#!apt update && apt install gh\n",
        "#from google.colab import userdata\n",
        "#GITHUB_TOKEN = userdata.get('github_repo')  # Get the GitHub token from Colab's secrets\n",
        "#!echo $GITHUB_TOKEN | gh auth login --with-token\n",
        "#!gh release create b2749 /content/llama.cpp/llama_cpp_binaries.zip --repo CrispStrobe/llama_cpp --notes \"Compiled binaries for llama_cpp\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}