# -*- coding: utf-8 -*-
"""Naive Bayes Text Multi Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w15VYm_IF8O1RbjhVVHs_0iA_4WUFrBf

# Library Imports
In this section, we import the necessary libraries for our code.

- `os`: Library for operating system-related tasks.
- `pandas`: Library for data manipulation and analysis.
- `numpy`: Library for numerical computations.
- `glob`: Library for file manipulation using patterns.
- `nltk`: Python library for natural language processing (NLP).
- `string`: Library for string operations.

We also import specific modules from the NLTK library:
- `stopwords`: Module for stop words that come with NLTK.
- `PorterStemmer`: Module for word stemming.
- `TweetTokenizer`: Module for tokenizing strings.
- `re`: Regular expression library for pattern matching.
- `random`: Library for random number generation.
- `math`: Library for mathematical operations.

## NLTK Resources Download
We download NLTK resources using the `nltk.download('stopwords')` command. This ensures that we have access to the required stopwords for text preprocessing.

Please note that detailed explanations for each library/module can be found in their respective documentation.

Feel free to reach out if you have any questions or need further assistance!
"""

# Import necessary libraries
import os                                  # Library for operating system-related tasks
import pandas as pd                        # Library for data manipulation and analysis
import numpy as np                         # Library for numerical computations
import glob                                # Library for file manipulation using patterns
import nltk                                # Python library for natural language processing (NLP)
import string                              # Library for string operations

from nltk.corpus import stopwords          # Module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # Module for word stemming
from nltk.tokenize import TweetTokenizer   # Module for tokenizing strings
import re                                  # Regular expression library for pattern matching
import random                              # Library for random number generation
import math                                # Library for mathematical operations

# Download NLTK resources if not already downloaded
nltk.download('stopwords')

"""# Code Explanation

## Tweet Tokenizer Instantiation
In this section, we instantiate the `TweetTokenizer` class from NLTK for tokenizing strings that often contain characteristics unique to social media platforms like Twitter.

- `preserve_case=False`: All text will be converted to lowercase for uniformity and easier analysis.
- `strip_handles=True`: Twitter handles (user mentions) will be removed to remove user-specific information.
- `reduce_len=True`: Repeated characters are normalized to avoid excessive elongation of words.

## Stopwords and Stemming Setup
We prepare for text preprocessing by setting up stopwords and a stemmer.

- `stopwords_english`: We import the list of English stopwords from the NLTK corpus. These are common words that are often removed from text as they do not carry significant meaning.
- `stemmer`: An instance of the PorterStemmer class is created. It is used to perform stemming, which involves reducing words to their base or root form. This aids in removing grammatical variations and simplifying analysis.

Please note that these comments provide an overview of the code's functionality. For detailed information, refer to the documentation of the NLTK library.

Feel free to reach out if you have any questions or need further assistance!

"""

# Instantiate the TweetTokenizer class
tokenizer = TweetTokenizer(
    preserve_case=False,  # Convert all text to lowercase
    strip_handles=True,    # Remove Twitter handles (user mentions)
    reduce_len=True        # Normalize repeated characters
)

# Import the English stop words list from NLTK
stopwords_english = stopwords.words('english')

# Instantiate the PorterStemmer for word stemming
stemmer = PorterStemmer()

print('Stop words\n')
print(stopwords_english)

print('\nPunctuation\n')
print(string.punctuation)

"""# Function Explanation

## Text Preprocessing Function
Here, we define a function called `preprocess` that performs various text preprocessing steps on an input sample.

### Arguments
- `sample` (str): The input text to be preprocessed.

### Steps
1. Convert the input sample to lowercase for uniformity.
2. Remove URLs using regular expression patterns.
3. Replace certain characters like '#' and '-' with spaces.
4. Tokenize the sample using the previously instantiated `TweetTokenizer`.
5. Initialize an empty list, `output`, to store preprocessed tokens.

### Token Processing Loop
For each token in the tokenized sample:
- Check conditions for including the token:
    - Not in NLTK stopwords.
    - Not a punctuation symbol.
    - Not entirely numeric.
    - Longer than one character.
- Apply stemming using the `PorterStemmer` instance.
- Add the stemmed token to the `output` list.

### Returns
- A list of preprocessed tokens.

Please note that these comments provide an overview of the function's functionality. Refer to the documentation of the `TweetTokenizer` and `PorterStemmer` classes for more details on their specific operations.

Feel free to adapt this explanation to your needs and preferences!

"""

def preprocess(sample):
    """
    Preprocesses the input sample by performing several text transformation steps.

    Args:
        sample (str): The input text to be preprocessed.

    Returns:
        list: A list of preprocessed tokens.
    """
    # Convert sample to lowercase
    sample = str(sample)
    sample = sample.lower()

    # Remove URLs
    sample = re.sub('https?:\/\/.*[\r\n]*', ' ', sample)

    # Replace certain characters with spaces
    sample = re.sub(r'[#:-]', ' ', sample)

    # Tokenize the sample
    sample = tokenizer.tokenize(sample)

    # Initialize an empty list to store preprocessed tokens
    output = []

    # Iterate through each token
    for i in sample:
        # Check conditions for including the token
        if i not in stopwords_english and i not in string.punctuation and i.isnumeric() == False and len(i) > 1:
            output.append(stemmer.stem(i))  # Perform stemming and add to output

    return output

"""# Code Explanation

## Category Mapping Dictionary
We start with a dictionary called `dictionary2` that maps category names to numerical codes. This dictionary is used for quick lookup and mapping between categories and their corresponding codes.

- `'Household': 0`: Category "Household" is assigned the code 0.
- `'Books': 1`: Category "Books" is assigned the code 1.
- `'Clothing & Accessories': 2`: Category "Clothing & Accessories" is assigned the code 2.
- `'Electronics': 3`: Category "Electronics" is assigned the code 3.

## Function for Code to Category Conversion
We define a function named `get_code2` that helps convert a numerical code back into its corresponding category name.

### Arguments
- `e` (int): The numerical code of the category.

### Steps
- Iterate through the items in the `dictionary2` using a `for` loop.
- If the provided numerical code (`e`) matches the code in the dictionary, return the corresponding category name (`x`).

### Returns
- The name of the category corresponding to the provided numerical code.

Please keep in mind that these comments offer an overview of the code's functionality. You can adapt and extend this explanation as per your specific requirements.

Feel free to reach out if you have any questions or need further assistance!

"""

# Mapping Dictionary
# This dictionary maps categories to numerical codes
dictionary = {'Household': 0, 'Books': 1, 'Clothing & Accessories': 2, 'Electronics': 3}

def get_code(e):
    """
    Retrieves the category name corresponding to a given numerical code.

    Args:
        e (int): The numerical code of the category.

    Returns:
        str: The name of the category.
    """
    # Iterate through dictionary items to find the matching code
    for x, y in dictionary.items():
        if e == y:
            return x  # Return the corresponding category name

"""## Read CSV File into DataFrame
In this section, we read a CSV file into a DataFrame using the `pd.read_csv()` function:

- `r'C:\Users\abdel\Downloads\archive (2)\ecommerceDataset.csv'`: The raw file path to the CSV file.
- `header=None`: Indicating that there is no header row in the CSV file.
- `names=['Y', 'X']`: Providing custom column names as 'Y' and 'X'.

## Display First Few Rows
We utilize the `.head()` method to display the initial rows of the DataFrame for a quick preview. This is a common practice to understand the data's structure and content.

Please note that the `r` prefix before the file path ensures proper handling of backslashes.

Feel free to adapt this explanation to your preferences and any additional details you may want to include.

"""

# Read a CSV file into a DataFrame
# Provide column names 'Y' and 'X' using the 'names' parameter
df = pd.read_csv("/kaggle/input/ecommerce-text-classification/ecommerceDataset.csv", header=None, names=['Y', 'X'])

# Display the first few rows of the DataFrame using the .head() method
df.head()

"""# Data Extraction and Preparation for Training and Testing

## Data Initialization
In this section, we extract and prepare the data for training and testing.

### Data Lists Initialization
- `original_texts`: An empty list to store the original texts.
- `original_labels`: An empty list to store the original category labels.

### Data Iteration and Processing
We iterate through each row in the DataFrame using the `iterrows()` function:
- Extract the text value from the row as `text_value`.
- Append the `text_value` to the `original_texts` list.
- Get the numerical category label from the dictionary and append it to the `original_labels` list.

### Data Combination and Shuffling
- Combine the `original_texts` and `original_labels` using `zip()` to create `data_combined`.
- Randomly shuffle the `data_combined` list to ensure a balanced distribution of data.

### Data Separation
- Unpack the shuffled `data_combined` list into separate lists for original texts and original labels.

### Data Splitting for Training and Testing
- Split the data into training and testing sets:
  - `train_x`: An array containing 90% of the original texts for training.
  - `train_y`: An array containing 90% of the original labels for training.
  - `test_x`: An array containing the remaining 10% of original texts for testing.
  - `test_y`: An array containing the remaining 10% of original labels for testing.

Please adjust this explanation according to your preferences and any additional details you wish to include.

If you have any questions or need further assistance, feel free to ask!

"""

# Extracting and Preparing Data for Training and Testing

# Initialize lists to store data
original_texts = []
original_labels = []

# Iterate through each row in the DataFrame using iterrows()
for index, row in df.iterrows():
    # Extract the text value from the row
    text_value = row[1]

    # Append the text value to the original texts list
    original_texts.append(text_value)

    # Get the numerical category label from the dictionary and append to the original labels list
    original_labels.append(dictionary2[row[0]])

# Combine the original texts and labels using zip()
data_combined = list(zip(original_texts, original_labels))
# The combined list looks like: [('apples', 50), ('grapes', 40), ...]

# Randomly shuffle the combined list
random.shuffle(data_combined)

# Unpack the shuffled combined list into separate lists
original_texts = [item[0] for item in data_combined]
original_labels = [item[1] for item in data_combined]

# Split data into training and testing sets
train_x = np.array(original_texts[:int(len(original_texts) * 0.9)])
train_y = np.array(original_labels[:int(len(original_labels) * 0.9)])

test_x = np.array(original_texts[int(len(original_texts) * 0.9):])
test_y = np.array(original_labels[int(len(original_labels) * 0.9):])

"""# Function Explanation

## Frequency Dictionary Builder
Here, we define a function named `build_freqs` that constructs a frequency dictionary for words in tweets categorized by sentiment labels.

### Arguments
- `tweets` (list): A list of preprocessed tweets.
- `ys` (numpy.ndarray or list): An array of sentiment labels corresponding to the tweets.

### Steps
1. Convert the sentiment labels array `ys` to a list named `yslist` using `np.squeeze(ys).tolist()`. This step ensures compatibility with the subsequent use of `zip()`.
2. Initialize an empty dictionary named `freqs` to store word-sentiment frequency pairs.

### Loop through Data
- Iterate through each sentiment label `y` and its corresponding tweet using `zip(yslist, tweets)`:
    - Preprocess the current tweet using the `preprocess()` function.
    - Loop through the processed words in the tweet:
        - Create a pair comprising the word and its sentiment label.
        - Increment the frequency count for the pair in the `freqs` dictionary.

### Returns
- A dictionary containing word-sentiment label pairs as keys and their respective frequencies as values.

Please adjust this explanation according to your preferences and any additional details you wish to include.

If you have any questions or need further assistance, feel free to ask!

"""

def build_freqs(tweets, ys):
    """
    Builds a frequency dictionary for words in tweets categorized by sentiment labels.

    Args:
        tweets (list): List of preprocessed tweets.
        ys (numpy.ndarray or list): Sentiment labels corresponding to the tweets.

    Returns:
        dict: A frequency dictionary with word-sentiment label pairs as keys and frequencies as values.
    """
    # Convert np array to list since zip needs an iterable.
    # The squeeze is necessary or the list ends up with one element.
    # Also note that this is just a NOP if ys is already a list.
    yslist = np.squeeze(ys).tolist()

    # Initialize an empty dictionary to store word-sentiment frequency pairs
    freqs = {}

    # Loop over sentiment labels and their corresponding tweets using zip
    for y, tweet in zip(yslist, tweets):
        # Preprocess the current tweet and loop over processed words
        for word in preprocess(tweet):
            # Create a pair of the word and its sentiment label
            pair = (word, y)
            # Increment the frequency count for the pair
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs

freq = build_freqs(train_x, train_y)

"""# Naive Bayes Classifier Training

## Naive Bayes Classifier Training Function
In this section, we define a function named `train_naive_bayes` that trains a Naive Bayes classifier using frequency-based features.

### Arguments
- `freqs` (dict): A frequency dictionary of word-sentiment label pairs.
- `train_x` (numpy.ndarray): Training data containing preprocessed texts.
- `train_y` (numpy.ndarray): Training data containing sentiment labels.

### Initialization
We start by initializing dictionaries to store log-likelihoods and prior probabilities for each category:
- `loglikelihood_Household`, `loglikelihood_Books`, `loglikelihood_Cloth_Access`, and `loglikelihood_Electronics`.

### Vocabulary and Data Statistics
- Create a vocabulary set `vocab` from the keys of the frequency dictionary.
- Calculate the total number of documents `D` in the training data.
- Calculate the number of documents for each category `D_i` using `sum(train_y == i)`.

### Word Count Calculation
We iterate through each word-sentiment label pair in the frequency dictionary and calculate the word count for each category:
- Increment the word count variables `N_i` for each category based on the sentiment label.

### Log-Prior Probability Calculation
- Calculate log-prior probabilities for each category using the formula:
    - `logprior_i = log(D_i) - log(D_sum_except_i)`

### Word Probability Calculation
- For each word in the vocabulary, calculate the probability of the word occurring in each category:
    - Calculate `p_w_i` as `(freq_i + 1) / (N_i + V)` and `p_w_neg_i` as `(freq_sum_except_i + 1) / (N_sum_except_i + V)`.

### Log-Likelihood Calculation
- Calculate log-likelihoods for each word and category using the formula:
    - `loglikelihood_i[word] = log(p_w_i) - log(p_w_neg_i)`

### Return
- The function returns the calculated log-prior probabilities and log-likelihoods for each category.

Please adapt this explanation according to your preferences and any additional details you wish to include.

If you have any questions or require further clarification, feel free to ask!

"""

def train_naive_bayes(freqs, train_x, train_y):
    """
    Train a Naive Bayes classifier using frequency-based features.

    Args:
        freqs (dict): A frequency dictionary of word-sentiment label pairs.
        train_x (numpy.ndarray): Training data containing preprocessed texts.
        train_y (numpy.ndarray): Training data containing sentiment labels.

    Returns:
        tuple: A tuple containing calculated prior probabilities and log-likelihoods for each category.
    """
    # Initialize dictionaries for log-likelihoods and prior probabilities
    loglikelihood_Household = {}
    loglikelihood_Books = {}
    loglikelihood_Cloth_Access = {}
    loglikelihood_Electronics = {}

    # Create a vocabulary set from the frequency dictionary keys
    vocab = set([pair[0] for pair in freqs.keys()])
    V = len(vocab)

    # Calculate the total number of documents (D)
    D = len(train_y)

    # Calculate the number of documents for each category (D_i)
    D_Household = sum(train_y == 0)
    D_Books = sum(train_y == 1)
    D_Cloth_Access = sum(train_y == 2)
    D_Electronics = sum(train_y == 3)

    # Initialize word count variables for each category
    N_Household = N_Books = N_Cloth_Access = N_Electronics = 0

    # Loop through each word-sentiment label pair in the frequency dictionary
    for pair in freqs.keys():
        # If the sentiment label is for the "Household" category (positive)
        if pair[1] == 0:
            # Increment the word count for the "Household" category
            N_Household += freqs.get(pair)
        # Else if the sentiment label is for the "Books" category (negative)
        elif pair[1] == 1:
            # Increment the word count for the "Books" category
            N_Books += freqs.get(pair)
        # Else if the sentiment label is for the "Clothing & Accessories" category
        elif pair[1] == 2:
            # Increment the word count for the "Clothing & Accessories" category
            N_Cloth_Access += freqs.get(pair)
        # Else, the sentiment label is for the "Electronics" category
        else:
            # Increment the word count for the "Electronics" category
            N_Electronics += freqs.get(pair)

    # Calculate log-prior probabilities for each category
    logprior_Household = np.log(D_Household) - np.log((D_Books + D_Cloth_Access + D_Electronics))
    logprior_Books = np.log(D_Books) - np.log((D_Household + D_Cloth_Access + D_Electronics))
    logprior_Cloth_Access = np.log(D_Cloth_Access) - np.log((D_Household + D_Books + D_Electronics))
    logprior_Electronics = np.log(D_Electronics) - np.log((D_Household + D_Books + D_Cloth_Access))

    # Loop through each word in the vocabulary
    for word in vocab:
        # Get the frequency of the word for each category
        freq_Household = freqs.get((word, 0), 0)
        freq_Books = freqs.get((word, 1), 0)
        freq_Cloth_Access = freqs.get((word, 2), 0)
        freq_Electronics = freqs.get((word, 3), 0)

        # Calculate probabilities for each word being in its respective category
        p_w_Household = (freq_Household + 1) / (N_Household + V)
        p_w_neg_Household = (freq_Books + freq_Cloth_Access + freq_Electronics + 1) / (N_Books + N_Cloth_Access + N_Electronics + V)

        p_w_Books = (freq_Books + 1) / (N_Books + V)
        p_w_neg_Books = (freq_Household + freq_Cloth_Access + freq_Electronics + 1) / (N_Household + N_Cloth_Access + N_Electronics + V)

        p_w_Cloth_Access = (freq_Cloth_Access + 1) / (N_Cloth_Access + V)
        p_w_neg_Cloth_Access = (freq_Household + freq_Books + freq_Electronics + 1) / (N_Household + N_Books + N_Electronics + V)

        p_w_Electronics = (freq_Electronics + 1) / (N_Electronics + V)
        p_w_neg_Electronics = (freq_Household + freq_Books + freq_Cloth_Access + 1) / (N_Household + N_Books + N_Cloth_Access + V)

        # Calculate log-likelihoods for each word and category
        loglikelihood_Household[word] = np.log(p_w_Household) - np.log(p_w_neg_Household)
        loglikelihood_Books[word] = np.log(p_w_Books) - np.log(p_w_neg_Books)
        loglikelihood_Cloth_Access[word] = np.log(p_w_Cloth_Access) - np.log(p_w_neg_Cloth_Access)
        loglikelihood_Electronics[word] = np.log(p_w_Electronics) - np.log(p_w_neg_Electronics)

    # Return calculated log-prior probabilities and log-likelihoods for each category
    return logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics

logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics =  train_naive_bayes(freq, train_x, train_y)

logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics

"""# Naive Bayes Prediction Function

## Naive Bayes Prediction Function
This section defines a function named `naive_bayes_predict` that predicts the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.

### Arguments
- `tweet` (str): The preprocessed tweet for which sentiment label probabilities are predicted.
- `logprior_Household` (float): Log-prior probability for the "Household" category.
- `logprior_Books` (float): Log-prior probability for the "Books" category.
- `logprior_Cloth_Access` (float): Log-prior probability for the "Clothing & Accessories" category.
- `logprior_Electronics` (float): Log-prior probability for the "Electronics" category.
- `loglikelihood_Household` (dict): Log-likelihoods for words in the "Household" category.
- `loglikelihood_Books` (dict): Log-likelihoods for words in the "Books" category.
- `loglikelihood_Cloth_Access` (dict): Log-likelihoods for words in the "Clothing & Accessories" category.
- `loglikelihood_Electronics` (dict): Log-likelihoods for words in the "Electronics" category.

### Steps
- Preprocess the input `tweet` to obtain a list of words.
- Initialize probabilities for each category to zero:
    - `p_Household`, `p_Books`, `p_Cloth_Access`, `p_Electronics`.
- Add the corresponding log-priors to the probabilities.
- Loop through each word in the preprocessed tweet:
    - Check if the word exists in the log-likelihood dictionaries.
    - If found, add the log-likelihood of that word to the corresponding probability.

### Return
- The function returns a tuple containing the calculated sum of log-likelihoods plus the corresponding log-prior for each category:
    - `p_Household`, `p_Books`, `p_Cloth_Access`, `p_Electronics`.

Please adapt this explanation according to your preferences and any additional details you wish to include.

If you have any questions or need further assistance, feel free to ask!

"""

def naive_bayes_predict(tweet, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics,
                        loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics):
    """
    Predict the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.

    Args:
        tweet (str): The preprocessed tweet to predict the sentiment label probabilities for.
        logprior_Household (float): Log-prior probability for the "Household" category.
        logprior_Books (float): Log-prior probability for the "Books" category.
        logprior_Cloth_Access (float): Log-prior probability for the "Clothing & Accessories" category.
        logprior_Electronics (float): Log-prior probability for the "Electronics" category.
        loglikelihood_Household (dict): Log-likelihoods for words in the "Household" category.
        loglikelihood_Books (dict): Log-likelihoods for words in the "Books" category.
        loglikelihood_Cloth_Access (dict): Log-likelihoods for words in the "Clothing & Accessories" category.
        loglikelihood_Electronics (dict): Log-likelihoods for words in the "Electronics" category.

    Returns:
        tuple: The calculated sum of log-likelihoods for each category plus the corresponding log-priors.
    """
    # Process the tweet to get a list of words
    word_l = preprocess(tweet)

    # Initialize probabilities for each category
    p_Household = 0
    p_Books = 0
    p_Cloth_Access = 0
    p_Electronics = 0

    # Add the log-priors to the probabilities
    p_Household += logprior_Household
    p_Books += logprior_Books
    p_Cloth_Access += logprior_Cloth_Access
    p_Electronics += logprior_Electronics

    # Loop through each word in the processed tweet
    for word in word_l:

        # Check if the word exists in the loglikelihood dictionaries
        if word in loglikelihood_Household:
            # Add the log-likelihood of that word to the corresponding probability
            p_Household += loglikelihood_Household[word]
            p_Books += loglikelihood_Books[word]
            p_Cloth_Access += loglikelihood_Cloth_Access[word]
            p_Electronics += loglikelihood_Electronics[word]

    # Return the calculated log-likelihood sums plus their respective log-priors
    return p_Household, p_Books, p_Cloth_Access, p_Electronics

"""# Naive Bayes Classifier Testing

## Naive Bayes Classifier Testing Function
In this section, we define a function named `test_naive_bayes` that evaluates the performance of a trained Naive Bayes classifier on a test dataset.

### Arguments
- `test_x` (list): A list of preprocessed tweets for testing.
- `test_y` (list): The corresponding true labels for the test tweets.
- `logprior_Household` (float): Log-prior probability for the "Household" category.
- `logprior_Books` (float): Log-prior probability for the "Books" category.
- `logprior_Cloth_Access` (float): Log-prior probability for the "Clothing & Accessories" category.
- `logprior_Electronics` (float): Log-prior probability for the "Electronics" category.
- `loglikelihood_Household` (dict): Log-likelihoods for words in the "Household" category.
- `loglikelihood_Books` (dict): Log-likelihoods for words in the "Books" category.
- `loglikelihood_Cloth_Access` (dict): Log-likelihoods for words in the "Clothing & Accessories" category.
- `loglikelihood_Electronics` (dict): Log-likelihoods for words in the "Electronics" category.

### Steps
- Initialize an accuracy variable to 0.
- Create an empty list `y_hats` to store predicted labels for each test tweet.
- Initialize a counter `o` for tracking the tweet index.
- Iterate through each tweet in the test data:
    - Predict the sentiment label using the trained Naive Bayes classifier for each category.
    - Create a list of predicted class probabilities.
    - Append the index of the predicted class with the highest probability to the `y_hats` list.
    - Optionally, print the predicted class, real class, and probabilities for debugging purposes.
    - Increment the tweet index counter `o`.
- Calculate the error as the average absolute difference between predicted and true labels.
- Calculate the accuracy as 1 minus the error.

### Return
- The function returns the calculated accuracy of the Naive Bayes classifier on the test dataset.

Feel free to customize and expand upon this Markdown structure as needed to align with your requirements and style.

"""

def test_naive_bayes(test_x, test_y, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics, loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics):
    """
    Test the performance of a trained Naive Bayes classifier on the test dataset.

    Args:
        test_x (list): A list of preprocessed tweets for testing.
        test_y (list): The corresponding true labels for the test tweets.
        logprior_Household (float): Log-prior probability for the "Household" category.
        logprior_Books (float): Log-prior probability for the "Books" category.
        logprior_Cloth_Access (float): Log-prior probability for the "Clothing & Accessories" category.
        logprior_Electronics (float): Log-prior probability for the "Electronics" category.
        loglikelihood_Household (dict): Log-likelihoods for words in the "Household" category.
        loglikelihood_Books (dict): Log-likelihoods for words in the "Books" category.
        loglikelihood_Cloth_Access (dict): Log-likelihoods for words in the "Clothing & Accessories" category.
        loglikelihood_Electronics (dict): Log-likelihoods for words in the "Electronics" category.

    Returns:
        float: Accuracy of the Naive Bayes classifier on the test dataset.
    """
    accuracy = 0  # Initialize accuracy variable
    y_hats = []  # Initialize a list to store predicted labels for each test tweet
    for tweet in test_x:
        # Predict the sentiment label using the trained Naive Bayes classifier
        p_Household, p_Books, p_Cloth_Access, p_Electronics = naive_bayes_predict(tweet, logprior_Household, logprior_Books, logprior_Cloth_Access, logprior_Electronics, loglikelihood_Household, loglikelihood_Books, loglikelihood_Cloth_Access, loglikelihood_Electronics)

        # Create a list of predicted class probabilities
        l = list([p_Household, p_Books, p_Cloth_Access, p_Electronics])

        # Append the index of the predicted class with the highest probability to y_hats
        y_hats.append(np.argmax(l))



    # Calculate the error as the average absolute difference between predicted and true labels
    error = sum(np.abs(y_hats - np.squeeze(test_y))) / len(y_hats)

    # Calculate accuracy as 1 minus the error
    accuracy = 1 - error

    return accuracy

print("Naive Bayes accuracy = %0.4f" %
      (test_naive_bayes(train_x,train_y,logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics)))

print("Naive Bayes accuracy = %0.4f" %
      (test_naive_bayes(test_x, test_y,logprior_Household,logprior_Books,logprior_Cloth_Access,logprior_Electronics, loglikelihood_Household,loglikelihood_Books,loglikelihood_Cloth_Access,loglikelihood_Electronics)))

"""# Naive Bayes Classifier

## Class: NaiveBayesClassifier
This class implements a Naive Bayes classifier for sentiment analysis using frequency-based features.

### Method: train
Train the Naive Bayes classifier using frequency-based features.

**Arguments:**
- `freqs` (dict): A frequency dictionary of word-sentiment label pairs.
- `train_x` (numpy.ndarray): Training data containing preprocessed texts.
- `train_y` (numpy.ndarray): Training data containing sentiment labels.

**Returns:**
- A tuple containing calculated prior probabilities and log-likelihoods for each category.

### Method: predict
Predict the sentiment label probabilities of a given tweet using the trained Naive Bayes classifier.

**Arguments:**
- `tweet` (str): The preprocessed tweet to predict the sentiment label probabilities for.

**Returns:**
- A list containing the calculated log-likelihood sums for each category.

### Method: test
Test the performance of the trained Naive Bayes classifier on the test dataset.

**Arguments:**
- `test_x` (list): A list of preprocessed tweets for testing.
- `test_y` (list): The corresponding true labels for the test tweets.

**Returns:**
- Accuracy of the Naive Bayes classifier on the test dataset.

### Workflow
1. In the `train` method:
   - Create vocabulary set and calculate the total number of documents (D).
   - For each sentiment label:
     - Calculate the number of documents for that label (D_label).
     - Calculate log-prior probability for that label.
     - Loop through each word-sentiment label pair:
       - Increment word counts for each label.
       - Calculate and store log-likelihood for each word and label.
   - Return calculated log-prior probabilities and log-likelihoods.

2. In the `predict` method:
   - Process input tweet to get list of words.
   - Loop through each label:
     - Initialize probability for that label.
     - Add log-prior probability.
     - Loop through each word in the tweet:
       - If word exists in log-likelihood, add its log-likelihood to probability.
     - Append probability to label_probs list.
   - Return label_probs list.

3. In the `test` method:
   - Initialize accuracy and y_hats lists.
   - For each test tweet:
     - Predict label probabilities using the `predict` method.
     - Get the index of the predicted label with highest probability.
     - Append predicted label to y_hats.
   - Calculate accuracy as 1 minus the average absolute difference between predicted and true labels.
   - Return accuracy.

Please adapt this explanation according to your preferences and any additional details you wish to include.

"""

class NaiveBayesClassifier:
    def train(self, freqs, train_x, train_y):
        """
        Train a Naive Bayes classifier using frequency-based features.

        Args:
            freqs (dict): A frequency dictionary of word-sentiment label pairs.
            train_x (numpy.ndarray): Training data containing preprocessed texts.
            train_y (numpy.ndarray): Training data containing sentiment labels.

        Returns:
            tuple: A tuple containing calculated prior probabilities and log-likelihoods for each category.
        """
        self.logprior = {}
        self.loglikelihood = {}

        # Create a vocabulary set from the frequency dictionary keys
        vocab = set([pair[0] for pair in freqs.keys()])
        V = len(vocab)

        # Calculate the total number of documents (D)
        D = len(train_y)

        # Calculate the number of documents for each category (D_i)
        for label in np.unique(train_y):
            D_label = sum(train_y == label)
            self.logprior[label] = np.log(D_label) - np.log(D - D_label)

            # Initialize word count variables for each category
            N_label = 0
            N_nonlabel=0
            # Loop through each word-sentiment label pair in the frequency dictionary
            for pair in freqs.keys():
                if pair[1] == label:
                    N_label += freqs.get(pair)
                else:
                    N_nonlabel+=freqs.get(pair)



            self.loglikelihood[label] = {}
            for word in vocab:
                freq_label = freqs.get((word, label), 0)
                freq_nonlabel = sum([freqs.get((word, other_label), 0) for other_label in np.unique(train_y) if other_label != label])
                p_w_label = (freq_label + 1) / (N_label + V)
                p_w_nonlabel = (freq_nonlabel + 1) / (N_nonlabel + V)
                self.loglikelihood[label][word] = np.log(p_w_label) - np.log(p_w_nonlabel)

        return self.logprior, self.loglikelihood

    def predict(self, tweet):
        """
        Predict the sentiment label probabilities of a given tweet using a trained Naive Bayes classifier.

        Args:
            tweet (str): The preprocessed tweet to predict the sentiment label probabilities for.

        Returns:
            dict: A dictionary containing the calculated log-likelihood sums for each category.
        """
        word_l = preprocess(tweet)
        label_probs = []

        for label in self.logprior.keys():
            p_label = 0
            p_label += self.logprior[label]

            for word in word_l:
                if word in self.loglikelihood[label]:
                    p_label += self.loglikelihood[label][word]

            label_probs.append(p_label)

        return label_probs

    def test(self, test_x, test_y):
        """
        Test the performance of a trained Naive Bayes classifier on the test dataset.

        Args:
            test_x (list): A list of preprocessed tweets for testing.
            test_y (list): The corresponding true labels for the test tweets.

        Returns:
            float: Accuracy of the Naive Bayes classifier on the test dataset.
        """
        accuracy = 0
        y_hats = []

        for tweet in test_x:
            label_probs = self.predict(tweet)
            predicted_label = np.argmax(label_probs)

            y_hats.append(predicted_label)

        error = sum(np.abs(y_hats - np.squeeze(test_y))) / len(y_hats)
        accuracy = 1 - error

        return accuracy

# Instantiate the NaiveBayesClassifier class
nb_classifier = NaiveBayesClassifier()

# Train the classifier using your training data and frequency dictionary
logprior, loglikelihood = nb_classifier.train(freq, train_x, train_y)

# Example test data for testing the classifier's performance
test_accuracy = nb_classifier.test(train_x, train_y)
print("Classifier accuracy on test data:", test_accuracy)

# Example test data for testing the classifier's performance
test_accuracy = nb_classifier.test(test_x,test_y)
print("Classifier accuracy on test data:", test_accuracy)