{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujmlV7qHEZIS",
        "outputId": "f3a0c3fa-cd2c-4c84-9583-7291d4fccb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6mB0bWiER8G",
        "outputId": "ab810d87-33f4-4850-e0fd-374b2b8e9c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-base were not used when initializing LukeModel: ['lm_head.layer_norm.bias', 'entity_predictions.transform.LayerNorm.bias', 'entity_predictions.transform.LayerNorm.weight', 'entity_predictions.transform.dense.weight', 'lm_head.dense.bias', 'entity_predictions.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'entity_predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing LukeModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: no_relation\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer, LukeModel, LukeForEntityPairClassification\n",
        "\n",
        "model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
        "# Example 1: Computing the contextualized entity representation corresponding to the entity mention\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "inputs = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "word_last_hidden_state = outputs.last_hidden_state\n",
        "entity_last_hidden_state = outputs.entity_last_hidden_state\n",
        "# Example 2: Inputting Wikipedia entities to obtain enriched contextualized representations\n",
        "\n",
        "entities = [\n",
        "    \"we\",\n",
        "    \"LM\",\n",
        "    \"LMS\",\n",
        "    \"Calgary\"\n",
        "]  # Wikipedia entity titles corresponding to the entity mentions \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "inputs = tokenizer(text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "word_last_hidden_state = outputs.last_hidden_state\n",
        "entity_last_hidden_state = outputs.entity_last_hidden_state\n",
        "# Example 3: Classifying the relationship between two entities using LukeForEntityPairClassification head model\n",
        "\n",
        "model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "entity_spans = [(25, 26), (47,48)]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = int(logits[0].argmax())\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r87NCHHKEVCW",
        "outputId": "28bd3d0d-6f49-4932-c693-33c5d26ce1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-base were not used when initializing LukeModel: ['lm_head.layer_norm.bias', 'entity_predictions.transform.LayerNorm.bias', 'entity_predictions.transform.LayerNorm.weight', 'entity_predictions.transform.dense.weight', 'lm_head.dense.bias', 'entity_predictions.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'entity_predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing LukeModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer, LukeModel\n",
        "\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
        "model = LukeModel.from_pretrained(\"studio-ousia/luke-base\")\n",
        "# Compute the contextualized entity representation corresponding to the entity mention \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "entity_spans = [(25, 26), (47,48), (96,98), (119,126)]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "\n",
        "encoding = tokenizer(text, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\")\n",
        "outputs = model(**encoding)\n",
        "word_last_hidden_state = outputs.last_hidden_state\n",
        "entity_last_hidden_state = outputs.entity_last_hidden_state\n",
        "# Input Wikipedia entities to obtain enriched contextualized representations of word tokens\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "entities = [\n",
        "    \"we\",\n",
        "    \"LM\",\n",
        "    \"LMS\",\n",
        "    \"Calgary\"\n",
        "]  # Wikipedia entity titles corresponding to the entity mentions \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "entity_spans = [\n",
        "    (25, 26), \n",
        "    (47,48), \n",
        "    (96,98), \n",
        "    (119,126)\n",
        "]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "\n",
        "encoding = tokenizer(\n",
        "    text, entities=entities, entity_spans=entity_spans, add_prefix_space=True, return_tensors=\"pt\"\n",
        ")\n",
        "outputs = model(**encoding)\n",
        "word_last_hidden_state = outputs.last_hidden_state\n",
        "entity_last_hidden_state = outputs.entity_last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS5DClziEmXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a552dc-786f-4603-856b-9e3c4b780129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-open-entity were not used when initializing LukeForEntityClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntityClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntityClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: object\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer, LukeForEntityClassification\n",
        "\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-open-entity\")\n",
        "model = LukeForEntityClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-open-entity\")\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "entity_spans = [(119,126)]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_SClpMcEqP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce75633-5c57-4454-b41a-a007db1ec2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: no_relation\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer, LukeForEntityPairClassification\n",
        "\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "entity_spans = [\n",
        "    (96,98), \n",
        "    (119,126)\n",
        "]  # character-based entity spans corresponding to \"we\", \"LM\", \"LMS\" and \"Calgary\"\n",
        "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmLBcPm9Esz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172994c9-d35d-4580-9901-f84ef31dea38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-conll-2003 were not used when initializing LukeForEntitySpanClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Speaker_10', ':', 'And', 'now', 'when', 'we', 'insert', 'it', 'into', 'the', 'LM', ',', 'apparently', 'the', 'one', 'with', 'the', 'least', 'number', 'of', 'LMS', 'be', 'improved', 'in', 'the', 'Calgary', 'and', 'I', 'think', 'that', 'is', 'expected', 'as', 'we', \"'re\", 'just', 'using', 'them', 'as', 'random', 'unigrams', 'with', 'probability', 'of', '1', 'within', 'scaling', 'one', '.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer, LukeForEntitySpanClassification\n",
        "\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
        "model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
        "\n",
        "text = \"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\"\n",
        "import spacy\n",
        "from spacy.pipeline import merge_entities    \n",
        "nlp = spacy.load(\"en_core_web_sm\")  # or any other model\n",
        "nlp.add_pipe(merge_entities)\n",
        "print([token.text for token in nlp(\"Speaker_10: And now when we insert it into the LM, apparently the one with the least number of LMS be improved in the Calgary and I think that is expected as we're just using them as random unigrams with probability of 1 within scaling one.\")])\n",
        "\n",
        "word_start_positions = [0, 8, 14, 17, 21]  # character-based start positions of word tokens\n",
        "word_end_positions = [7, 13, 16, 20, 28]  # character-based end positions of word tokens\n",
        "entity_spans = []\n",
        "for i, start_pos in enumerate(word_start_positions):\n",
        "    for end_pos in word_end_positions[i:]:\n",
        "        entity_spans.append((start_pos, end_pos))\n",
        "\n",
        "inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "predicted_class_indices = logits.argmax(-1).squeeze().tolist()\n",
        "for span, predicted_class_idx in zip(entity_spans, predicted_class_indices):\n",
        "    if predicted_class_idx != 0:\n",
        "        print(text[span[0] : span[1]], model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PxNvmft_1Ljl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LUKE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}