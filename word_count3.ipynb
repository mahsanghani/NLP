{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_count3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBJjmKw9j-Zr",
        "outputId": "549fb5c4-e7d4-4da4-c3d6-1a159ea828e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 45.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=de2c759d9309c13392cf137a2de8de4e30b907dcba0859f1a7ef8337c8b92633\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"word-counts\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "Jtz1GELHkCzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book = sc.textFile(\"/content/millions_from_waste.txt\")\n",
        "book.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJRLI107kGWj",
        "outputId": "b7ca5c1f-0a39-4332-e4f4-21cf8c5a7b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Project Gutenberg eBook of Millions from Waste, by Frederick A.',\n",
              " 'Talbot',\n",
              " '',\n",
              " 'This eBook is for the use of anyone anywhere in the United States and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
              " 'of the Project Gutenberg License included with this eBook or online at',\n",
              " 'www.gutenberg.org. If you are not located in the United States, you',\n",
              " 'will have to check the laws of the country where you are located before',\n",
              " 'using this eBook.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = book.flatMap(lambda x: x.split()).countByValue()\n",
        "\n",
        "for i, (word, count) in enumerate(word_counts.items()):\n",
        "    if i == 15: break\n",
        "    print(word, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPexoQqwkJwj",
        "outputId": "f9ed0679-75f0-4970-b3c1-721a0f16c566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 995\n",
            "Project 79\n",
            "Gutenberg 22\n",
            "eBook 6\n",
            "of 4980\n",
            "Millions 3\n",
            "from 923\n",
            "Waste, 1\n",
            "by 529\n",
            "Frederick 2\n",
            "A. 7\n",
            "Talbot 2\n",
            "This 185\n",
            "is 1828\n",
            "for 1049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def preprocess_word(word: str):\n",
        "    return re.sub(\"[^A-Za-z0-9]+\", \"\", word.lower())\n",
        "\n",
        "def preprocess_words(words: str):\n",
        "    return [preprocess_word(word) for word in words.split()]\n",
        "    \n",
        "    \n",
        "preprocess_words(\"The Project Gutenberg eBook of Millions from Waste, by Frederick A.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU0O48w9kr0Q",
        "outputId": "3903e8e3-acdb-4a6a-8ab5-b790147a7b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'project',\n",
              " 'gutenberg',\n",
              " 'ebook',\n",
              " 'of',\n",
              " 'millions',\n",
              " 'from',\n",
              " 'waste',\n",
              " 'by',\n",
              " 'frederick',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book = sc.textFile(\"/content/millions_from_waste.txt\")\n",
        "\n",
        "word_counts = book.flatMap(preprocess_words).countByValue()\n",
        "for i, (word, count) in enumerate(word_counts.items()):\n",
        "    if i == 15: break\n",
        "    print(word, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1miEzT1kvO8",
        "outputId": "201892cb-9e4d-40ce-f15b-8080e7c51543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 10099\n",
            "project 91\n",
            "gutenberg 31\n",
            "ebook 13\n",
            "of 5032\n",
            "millions 22\n",
            "from 974\n",
            "waste 435\n",
            "by 571\n",
            "frederick 4\n",
            "a 2090\n",
            "talbot 4\n",
            "this 1105\n",
            "is 1834\n",
            "for 1093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book = sc.textFile(\"/content/millions_from_waste.txt\")\n",
        "\n",
        "words = book.flatMap(preprocess_words)\n",
        "word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "word_counts.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8AXX5XTkyat",
        "outputId": "3cd7b2c8-95c4-4e41-a27c-25e4bdac3631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 10099),\n",
              " ('project', 91),\n",
              " ('gutenberg', 31),\n",
              " ('ebook', 13),\n",
              " ('of', 5032),\n",
              " ('millions', 22),\n",
              " ('from', 974),\n",
              " ('waste', 435),\n",
              " ('by', 571),\n",
              " ('frederick', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_sorted = word_counts.map(lambda x: (x[1], x[0]))\n",
        "word_counts_sorted.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDteWZD8k3VN",
        "outputId": "03f3ded6-6ba2-44b0-923f-7c208b1d3c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10099, 'the'),\n",
              " (91, 'project'),\n",
              " (31, 'gutenberg'),\n",
              " (13, 'ebook'),\n",
              " (5032, 'of'),\n",
              " (22, 'millions'),\n",
              " (974, 'from'),\n",
              " (435, 'waste'),\n",
              " (571, 'by'),\n",
              " (4, 'frederick')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey()\n",
        "word_counts_sorted.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDFzWta2k4mw",
        "outputId": "a114b6f1-cd39-475d-f0af-162d25b5050e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'april'),\n",
              " (1, '2022'),\n",
              " (1, '67837'),\n",
              " (1, 'english'),\n",
              " (1, 'deaurider'),\n",
              " (1, 'proofreading'),\n",
              " (1, 'team'),\n",
              " (1, 'httpswwwpgdpnet'),\n",
              " (1, 'images'),\n",
              " (1, 'generously')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
        "word_counts_sorted.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0tobNbik6HH",
        "outputId": "d4eea8fc-e56d-4cdb-b456-95283f86b285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10099, 'the'),\n",
              " (5032, 'of'),\n",
              " (4296, 'to'),\n",
              " (2490, 'and'),\n",
              " (2225, 'in'),\n",
              " (2090, 'a'),\n",
              " (1834, 'is'),\n",
              " (1405, 'be'),\n",
              " (1364, 'it'),\n",
              " (1207, 'as')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMSJ51fWk7Ut",
        "outputId": "53c165a9-6d2d-4c6b-c062-a88549d1c25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUCEBWW2k9sA",
        "outputId": "265906be-0c3b-4195-ecc7-c342de50847b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_word(word: str):\n",
        "    return re.sub(\"[^A-Za-z0-9]+\", \"\", word.lower())\n",
        "\n",
        "def preprocess_words(words: str):\n",
        "    preprocessed = [preprocess_word(word) for word in words.split()]\n",
        "    return [word for word in preprocessed if word not in stop_words and word != \"\"]\n",
        "    \n",
        "\n",
        "preprocess_words(\"The Project Gutenberg eBook of Millions from Waste, by Frederick A.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqLhZ6W-k-tl",
        "outputId": "343e3b04-655b-4256-f6f5-300f4fda6ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project', 'gutenberg', 'ebook', 'millions', 'waste', 'frederick']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book = sc.textFile(\"/content/millions_from_waste.txt\")\n",
        "\n",
        "words = book.flatMap(preprocess_words)\n",
        "word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "word_counts_sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
        "word_counts_sorted.collect()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6N-iyPylAbd",
        "outputId": "5c054bed-7661-4a20-9203-75734cb7ca07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(435, 'waste'),\n",
              " (346, 'one'),\n",
              " (337, 'upon'),\n",
              " (303, 'would'),\n",
              " (286, 'may'),\n",
              " (265, 'material'),\n",
              " (218, 'process'),\n",
              " (200, 'per'),\n",
              " (196, 'tons'),\n",
              " (181, 'fat')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zME9NeWJlB4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}